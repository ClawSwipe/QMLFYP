{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52CR7KmybQYr",
        "outputId": "02c8a31f-3759-42c0-abf6-411b6d49ecaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pennylane\n",
            "  Downloading PennyLane-0.38.0-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting pennylane-lightning\n",
            "  Downloading PennyLane_Lightning-0.38.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (26 kB)\n",
            "Collecting cotengra\n",
            "  Downloading cotengra-0.6.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting quimb\n",
            "  Downloading quimb-1.8.4-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pennylane) (3.3)\n",
            "Collecting rustworkx>=0.14.0 (from pennylane)\n",
            "  Downloading rustworkx-0.15.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.7.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.10.2)\n",
            "Collecting appdirs (from pennylane)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting autoray>=0.6.11 (from pennylane)\n",
            "  Downloading autoray-0.6.12-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from pennylane) (5.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pennylane) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pennylane) (4.12.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pennylane) (24.1)\n",
            "Collecting pennylane-lightning-gpu (from pennylane-lightning[gpu])\n",
            "  Downloading PennyLane_Lightning_GPU-0.38.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (26 kB)\n",
            "Collecting cytoolz>=0.8.0 (from quimb)\n",
            "  Downloading cytoolz-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: numba>=0.39 in /usr/local/lib/python3.10/dist-packages (from quimb) (0.60.0)\n",
            "Requirement already satisfied: psutil>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from quimb) (5.9.5)\n",
            "Requirement already satisfied: tqdm>=4 in /usr/local/lib/python3.10/dist-packages (from quimb) (4.66.5)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from cytoolz>=0.8.0->quimb) (0.12.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.39->quimb) (0.43.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2024.8.30)\n",
            "Downloading PennyLane-0.38.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PennyLane_Lightning-0.38.0-cp310-cp310-manylinux_2_28_x86_64.whl (15.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cotengra-0.6.2-py3-none-any.whl (177 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.8/177.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading quimb-1.8.4-py3-none-any.whl (541 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m541.6/541.6 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autoray-0.6.12-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cytoolz-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rustworkx-0.15.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading PennyLane_Lightning_GPU-0.38.0-cp310-cp310-manylinux_2_28_x86_64.whl (6.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: appdirs, rustworkx, cytoolz, autoray, cotengra, quimb, pennylane-lightning, pennylane, pennylane-lightning-gpu\n",
            "Successfully installed appdirs-1.4.4 autoray-0.6.12 cotengra-0.6.2 cytoolz-1.0.0 pennylane-0.38.0 pennylane-lightning-0.38.0 pennylane-lightning-gpu-0.38.0 quimb-1.8.4 rustworkx-0.15.1\n",
            "Looking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
            "Requirement already satisfied: jax[cuda12_pip] in /usr/local/lib/python3.10/dist-packages (0.4.33)\n",
            "Collecting jax[cuda12_pip]\n",
            "  Downloading jax-0.4.34-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.34,>=0.4.34 (from jax[cuda12_pip])\n",
            "  Downloading jaxlib-0.4.34-cp310-cp310-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax[cuda12_pip]) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from jax[cuda12_pip]) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax[cuda12_pip]) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax[cuda12_pip]) (1.13.1)\n",
            "Collecting jax-cuda12-plugin<=0.4.34,>=0.4.34 (from jax-cuda12-plugin[with_cuda]<=0.4.34,>=0.4.34; extra == \"cuda12-pip\"->jax[cuda12_pip])\n",
            "  Downloading jax_cuda12_plugin-0.4.34-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax-cuda12-pjrt==0.4.34 (from jax-cuda12-plugin<=0.4.34,>=0.4.34->jax-cuda12-plugin[with_cuda]<=0.4.34,>=0.4.34; extra == \"cuda12-pip\"->jax[cuda12_pip])\n",
            "  Downloading jax_cuda12_pjrt-0.4.34-py3-none-manylinux2014_x86_64.whl.metadata (349 bytes)\n",
            "Requirement already satisfied: nvidia-cublas-cu12>=12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.34,>=0.4.34; extra == \"cuda12-pip\"->jax[cuda12_pip]) (12.6.3.3)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12>=12.1.105 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.34,>=0.4.34; extra == \"cuda12-pip\"->jax[cuda12_pip]) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cuda-nvcc-cu12>=12.1.105 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.34,>=0.4.34; extra == \"cuda12-pip\"->jax[cuda12_pip]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12>=12.1.105 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.34,>=0.4.34; extra == \"cuda12-pip\"->jax[cuda12_pip]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12<10.0,>=9.1 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.34,>=0.4.34; extra == \"cuda12-pip\"->jax[cuda12_pip]) (9.4.0.58)\n",
            "Requirement already satisfied: nvidia-cufft-cu12>=11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.34,>=0.4.34; extra == \"cuda12-pip\"->jax[cuda12_pip]) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12>=11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.34,>=0.4.34; extra == \"cuda12-pip\"->jax[cuda12_pip]) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12>=12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.34,>=0.4.34; extra == \"cuda12-pip\"->jax[cuda12_pip]) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12>=2.18.1 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.34,>=0.4.34; extra == \"cuda12-pip\"->jax[cuda12_pip]) (2.23.4)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12>=12.1.105 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.34,>=0.4.34; extra == \"cuda12-pip\"->jax[cuda12_pip]) (12.6.77)\n",
            "Downloading jaxlib-0.4.34-cp310-cp310-manylinux2014_x86_64.whl (86.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.1/86.1 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax_cuda12_plugin-0.4.34-cp310-cp310-manylinux2014_x86_64.whl (14.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax_cuda12_pjrt-0.4.34-py3-none-manylinux2014_x86_64.whl (100.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.3/100.3 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax-0.4.34-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jax-cuda12-pjrt, jax-cuda12-plugin, jaxlib, jax\n",
            "  Attempting uninstall: jax-cuda12-pjrt\n",
            "    Found existing installation: jax-cuda12-pjrt 0.4.33\n",
            "    Uninstalling jax-cuda12-pjrt-0.4.33:\n",
            "      Successfully uninstalled jax-cuda12-pjrt-0.4.33\n",
            "  Attempting uninstall: jax-cuda12-plugin\n",
            "    Found existing installation: jax-cuda12-plugin 0.4.33\n",
            "    Uninstalling jax-cuda12-plugin-0.4.33:\n",
            "      Successfully uninstalled jax-cuda12-plugin-0.4.33\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.4.33\n",
            "    Uninstalling jaxlib-0.4.33:\n",
            "      Successfully uninstalled jaxlib-0.4.33\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.4.33\n",
            "    Uninstalling jax-0.4.33:\n",
            "      Successfully uninstalled jax-0.4.33\n",
            "Successfully installed jax-0.4.34 jax-cuda12-pjrt-0.4.34 jax-cuda12-plugin-0.4.34 jaxlib-0.4.34\n"
          ]
        }
      ],
      "source": [
        "# Install packages\n",
        "!pip install pennylane pennylane-lightning pennylane-lightning[gpu] cotengra quimb --upgrade\n",
        "!pip install -U \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
        "# !pip install --upgrade \"jax[cuda11_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import packages\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import numpy as np\n",
        "#np.set_printoptions(threshold=sys.maxsize)\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "import seaborn as sns\n",
        "import jax\n",
        "import time\n",
        "\n",
        "import functools\n",
        "\n",
        "from typing import List, Union, Tuple, Dict, Optional, Any\n",
        "from typing import Callable\n",
        "\n",
        "jax.config.update(\"jax_enable_x64\", True)\n",
        "#jax.config.update(\"jax_debug_nans\", True)\n",
        "import jax.numpy as jnp\n",
        "\n",
        "import optax  # optimization using jax\n",
        "\n",
        "import torch  # https://pytorch.org\n",
        "import torchvision  # https://pytorch.org\n",
        "#torch.set_printoptions(profile=\"full\")\n",
        "import pennylane as qml\n",
        "import pennylane.numpy as pnp\n",
        "\n",
        "import os, cv2, itertools # cv2 -- OpenCV\n",
        "import shutil\n",
        "import zipfile\n",
        "%matplotlib inline\n",
        "\n",
        "from jax.lib import xla_bridge\n",
        "\n",
        "def set_jax_platform():\n",
        "    # Check if TPU is available\n",
        "    try:\n",
        "        tpu_backend = xla_bridge.get_backend('tpu')\n",
        "        if tpu_backend and tpu_backend.device_count() > 0:\n",
        "            # Set platform to TPU\n",
        "            jax.config.update('jax_platform_name', 'tpu')\n",
        "            print(\"Set platform to TPU\")\n",
        "            return\n",
        "    except RuntimeError:\n",
        "        pass  # No TPU found, move on to check for GPU\n",
        "\n",
        "    # Check if GPU is available\n",
        "    try:\n",
        "      gpu_backend = xla_bridge.get_backend('gpu')\n",
        "      if gpu_backend and gpu_backend.device_count() > 0:\n",
        "          # Set platform to CUDA (GPU)\n",
        "          jax.config.update('jax_platform_name', 'gpu')\n",
        "          print(\"Set platform to GPU\")\n",
        "    except RuntimeError:\n",
        "          # Set platform to CPU\n",
        "          jax.config.update('jax_platform_name', 'cpu')\n",
        "          print(\"Set platform to CPU\")\n",
        "\n",
        "# Call the function to set the platform\n",
        "set_jax_platform()\n",
        "\n",
        "sns.set()\n",
        "\n",
        "seed = 1701\n",
        "rng = np.random.default_rng(seed=seed)\n",
        "prng = pnp.random.default_rng(seed=seed)\n",
        "jrng_key = jax.random.PRNGKey(seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YC92azd1bgkU",
        "outputId": "79db7adc-fe2d-4a6a-d34e-e518be4a322d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-b00b07bf67a6>:40: DeprecationWarning: jax.lib.xla_bridge.get_backend is deprecated; use jax.extend.backend.get_backend.\n",
            "  tpu_backend = xla_bridge.get_backend('tpu')\n",
            "<ipython-input-2-b00b07bf67a6>:51: DeprecationWarning: jax.lib.xla_bridge.get_backend is deprecated; use jax.extend.backend.get_backend.\n",
            "  gpu_backend = xla_bridge.get_backend('gpu')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Set platform to GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the Dataset\n",
        "\n",
        "For the rescaled image matrix $M$, the \"Hermitian version\" of it can be calculated as:\n",
        "\n",
        "$$\n",
        "A = \\frac{M+M^T}{2}\n",
        "$$"
      ],
      "metadata": {
        "id": "LCo8fdobbiTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Pad(2),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Lambda(lambda x: torch.squeeze(x)),\n",
        "    #torchvision.transforms.Lambda(lambda x: x / torch.trace(x)),\n",
        "    torchvision.transforms.Lambda(lambda x: (x+torch.transpose(x, 0, 1))/2)\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(\n",
        "    \"MNIST\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=preprocess,\n",
        ")\n",
        "test_dataset = torchvision.datasets.MNIST(\n",
        "    \"MNIST\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=preprocess,\n",
        ")\n",
        "dummy_trainloader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=64, shuffle=True\n",
        ")\n",
        "dummy_testloader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=64, shuffle=True\n",
        ")\n",
        "\n",
        "dummy_x, dummy_y = next(iter(dummy_trainloader))\n",
        "dummy_x = dummy_x.numpy()\n",
        "dummy_y = dummy_y.numpy()\n",
        "print(dummy_x.shape)  # 64x32x32\n",
        "print(dummy_y.shape)  # 64\n",
        "print(dummy_y)\n",
        "print(dummy_x[0,16])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlSUCStebkoA",
        "outputId": "034c7edd-fe13-4146-a7aa-d160babcc5d4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to MNIST/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 16280584.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST/MNIST/raw/train-images-idx3-ubyte.gz to MNIST/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to MNIST/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 494036.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST/MNIST/raw/train-labels-idx1-ubyte.gz to MNIST/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 4452523.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3031110.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST/MNIST/raw\n",
            "\n",
            "(64, 32, 32)\n",
            "(64,)\n",
            "[1 4 9 8 7 1 6 1 1 7 0 3 6 6 8 5 9 3 4 6 5 5 7 2 8 1 2 9 1 3 9 0 0 8 2 0 6\n",
            " 4 4 0 1 5 5 2 0 0 6 6 9 0 2 2 3 2 1 0 2 7 7 1 8 4 3 4]\n",
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.19215687 0.29215688 0.29215688 0.29215688\n",
            " 0.4019608  0.49607843 0.57058823 0.927451   0.99607843 0.79019606\n",
            " 0.49607843 0.49607843 0.49607843 0.49607843 0.49607843 0.49607843\n",
            " 0.49607843 0.49607843 0.         0.         0.         0.\n",
            " 0.         0.        ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Time-Evolve the Image Hermitian"
      ],
      "metadata": {
        "id": "TR5l9DZ5bubn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def img_hermitian_evolve(\n",
        "    img:jnp.ndarray,\n",
        "    t:float\n",
        ")->jnp.ndarray:\n",
        "  assert img.shape[-1]==32 and img.shape[-2] == 32, f\"The shape of the image must be 32 by 32, got {img.shape[-2]} by {img.shape[-1]}\"\n",
        "  return jax.scipy.linalg.expm(img*( -0.5j*t))\n",
        "\n",
        "print(\n",
        "    img_hermitian_evolve(\n",
        "        dummy_x[0],\n",
        "        10\n",
        "        )[16]\n",
        "    )\n",
        "\n",
        "\n",
        "print(\n",
        "    jnp.einsum(\n",
        "        \"ij,jk->ik\",\n",
        "        jnp.transpose(jnp.conjugate(img_hermitian_evolve(\n",
        "        dummy_x[0],\n",
        "        10\n",
        "        ))),\n",
        "        img_hermitian_evolve(\n",
        "        dummy_x[0],\n",
        "        10\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\n",
        "    jnp.einsum(\n",
        "        \"ij,jk->ik\",\n",
        "        img_hermitian_evolve(\n",
        "        dummy_x[0],\n",
        "        10\n",
        "        ),\n",
        "        jnp.transpose(jnp.conjugate(img_hermitian_evolve(\n",
        "        dummy_x[0],\n",
        "        10\n",
        "        )))\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYbF2gRNbxKF",
        "outputId": "e9dedcdb-9452-42ef-e0ed-833a63a396db"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.        +0.j          0.        +0.j          0.        +0.j\n",
            "  0.        +0.j          0.        +0.j          0.        +0.j\n",
            " -0.08425658+0.27362376j -0.11163641+0.34952053j -0.05980267+0.01368771j\n",
            " -0.05330864-0.12267236j -0.02982646-0.14010537j -0.03196526-0.08992453j\n",
            " -0.00083861-0.23477529j -0.05506559-0.16507737j  0.35857216+0.12227695j\n",
            "  0.34855375+0.09667016j  0.15120251+0.17321242j -0.03118993-0.0414421j\n",
            "  0.18919455+0.00185021j  0.22574805+0.17663069j -0.00412213-0.01116842j\n",
            " -0.09731125-0.15229319j  0.15256386-0.14876023j  0.09566949+0.20120546j\n",
            "  0.09373346+0.22009434j  0.10792629+0.04608551j  0.        +0.j\n",
            "  0.        +0.j          0.        +0.j          0.        +0.j\n",
            "  0.        +0.j          0.        +0.j        ]\n",
            "[[1.+0.j 0.+0.j 0.+0.j ... 0.+0.j 0.+0.j 0.+0.j]\n",
            " [0.+0.j 1.+0.j 0.+0.j ... 0.+0.j 0.+0.j 0.+0.j]\n",
            " [0.+0.j 0.+0.j 1.+0.j ... 0.+0.j 0.+0.j 0.+0.j]\n",
            " ...\n",
            " [0.+0.j 0.+0.j 0.+0.j ... 1.+0.j 0.+0.j 0.+0.j]\n",
            " [0.+0.j 0.+0.j 0.+0.j ... 0.+0.j 1.+0.j 0.+0.j]\n",
            " [0.+0.j 0.+0.j 0.+0.j ... 0.+0.j 0.+0.j 1.+0.j]]\n",
            "[[1.+0.j 0.+0.j 0.+0.j ... 0.+0.j 0.+0.j 0.+0.j]\n",
            " [0.+0.j 1.+0.j 0.+0.j ... 0.+0.j 0.+0.j 0.+0.j]\n",
            " [0.+0.j 0.+0.j 1.+0.j ... 0.+0.j 0.+0.j 0.+0.j]\n",
            " ...\n",
            " [0.+0.j 0.+0.j 0.+0.j ... 1.+0.j 0.+0.j 0.+0.j]\n",
            " [0.+0.j 0.+0.j 0.+0.j ... 0.+0.j 1.+0.j 0.+0.j]\n",
            " [0.+0.j 0.+0.j 0.+0.j ... 0.+0.j 0.+0.j 1.+0.j]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some Utilities"
      ],
      "metadata": {
        "id": "enL2qIVtcNLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ket = {\n",
        "    '0':jnp.array([1,0]),\n",
        "    '1':jnp.array([0,1]),\n",
        "    '+':(jnp.array([1,0]) + jnp.array([0,1]))/jnp.sqrt(2),\n",
        "    '-':(jnp.array([1,0]) - jnp.array([0,1]))/jnp.sqrt(2)\n",
        "}\n",
        "\n",
        "pauli = {\n",
        "    'I':jnp.array([[1,0],[0,1]]),\n",
        "    'X':jnp.array([[0,1],[1,0]]),\n",
        "    'Y':jnp.array([[0, -1j],[1j, 0]]),\n",
        "    'Z':jnp.array([[1,0],[0,-1]])\n",
        "}\n",
        "\n",
        "def tensor_product(*args):\n",
        "  input_list = [a for a in args]\n",
        "  return functools.reduce(jnp.kron, input_list)\n",
        "\n",
        "def multi_qubit_identity(n_qubits:int)->jnp.ndarray:\n",
        "  assert n_qubits>0\n",
        "  if n_qubits == 1:\n",
        "    return pauli['I']\n",
        "  else:\n",
        "    return tensor_product(*[pauli['I'] for _ in range(n_qubits)])\n",
        "\n",
        "pauli_words_su4 = {}\n",
        "for key1 in pauli.keys():\n",
        "  for key2 in pauli.keys():\n",
        "    if not (key1==key2 and key1=='I' and key2=='I'):\n",
        "      pauli_words_su4[key1+key2] = tensor_product(pauli[key1], pauli[key2])\n",
        "\n",
        "pauli_words_su8 = {}\n",
        "for key1 in pauli.keys():\n",
        "  for key2 in pauli.keys():\n",
        "    for key3 in pauli.keys():\n",
        "      if not key1+key2+key3 == 'III':\n",
        "        pauli_words_su8[key1+key2+key3] = tensor_product(pauli[key1], pauli[key2], pauli[key3])\n",
        "\n",
        "pauli_words_su16 = {}\n",
        "for key1 in pauli.keys():\n",
        "  for key2 in pauli.keys():\n",
        "    for key3 in pauli.keys():\n",
        "      for key4 in pauli.keys():\n",
        "        if not key1+key2+key3+key4 == 'IIII':\n",
        "          pauli_words_su16[key1+key2+key3+key4] = tensor_product(\n",
        "              pauli[key1],\n",
        "              pauli[key2],\n",
        "              pauli[key3],\n",
        "              pauli[key4]\n",
        "          )\n",
        "\n",
        "pauli_words_su32 = {}\n",
        "for key1 in pauli.keys():\n",
        "  for key2 in pauli.keys():\n",
        "    for key3 in pauli.keys():\n",
        "      for key4 in pauli.keys():\n",
        "        for key5 in pauli.keys():\n",
        "          if not key1+key2+key3+key4+key5 == 'IIIII':\n",
        "            pauli_words_su32[key1+key2+key3+key4+key5] = tensor_product(\n",
        "                pauli[key1],\n",
        "                pauli[key2],\n",
        "                pauli[key3],\n",
        "                pauli[key4],\n",
        "                pauli[key5]\n",
        "            )\n",
        "\n",
        "observables_10_cls_5q = [0]*10\n",
        "for i in ['0', '1']:\n",
        "  for j in ['0', '1']:\n",
        "    for k in ['0', '1']:\n",
        "      for l in ['0', '1']:\n",
        "        idx = int(i+j+k+l, 2)\n",
        "        if idx <10:\n",
        "          basis_state = tensor_product(*[ket[i], ket[j], ket[k], ket[l]])\n",
        "          four_qubit_obs = jnp.outer(basis_state, basis_state)\n",
        "          observables_10_cls_5q[idx] = tensor_product(four_qubit_obs, multi_qubit_identity(1))\n",
        "\n",
        "observables_8_cls_5q = [0]*8\n",
        "for i in ['0', '1']:\n",
        "  for j in ['0', '1']:\n",
        "    for k in ['0', '1']:\n",
        "      for l in ['0', '1']:\n",
        "        idx = int(i+j+k+l, 2)\n",
        "        if idx <8:\n",
        "          basis_state = tensor_product(*[ket[i], ket[j], ket[k], ket[l]])\n",
        "          four_qubit_obs = jnp.outer(basis_state, basis_state)\n",
        "          observables_8_cls_5q[idx] = tensor_product(four_qubit_obs, multi_qubit_identity(1))\n"
      ],
      "metadata": {
        "id": "xVOKRfoacK60"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def su32_op(\n",
        "    params:jnp.ndarray\n",
        "):\n",
        "  generator = jnp.einsum(\"i, ijk - >jk\", params, jnp.asarray(list(pauli_words_su32.values())))\n",
        "  return jax.scipy.linalg.expm(1j*generator)\n",
        "\n",
        "test_params = jax.random.normal(shape=[4**5-1], key=jrng_key)\n",
        "\n",
        "print(\n",
        "    jnp.einsum(\n",
        "        \"ij,jk->ik\",\n",
        "        jnp.transpose(jnp.conjugate(su32_op(test_params))),\n",
        "        su32_op(test_params)\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\n",
        "    jnp.einsum(\n",
        "        \"ij,jk->ik\",\n",
        "        su32_op(test_params),\n",
        "        jnp.transpose(jnp.conjugate(su32_op(test_params)))\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfMfQrSncpW1",
        "outputId": "43f79b97-16f5-4583-8e9d-61e284fc48a8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.00000000e+00-2.81802478e-18j -2.08166817e-17-6.93889390e-17j\n",
            "   1.87350135e-16+4.02455846e-16j ...  6.78276879e-16-5.07189776e-16j\n",
            "  -7.11236625e-16+6.24500451e-17j -2.53269627e-16-5.55111512e-16j]\n",
            " [-2.08166817e-17+4.85722573e-17j  1.00000000e+00+1.79597214e-18j\n",
            "   3.78169718e-16-5.82867088e-16j ...  8.04911693e-16-1.17267307e-15j\n",
            "   4.85722573e-17-4.51089090e-16j -2.35922393e-16-2.22044605e-16j]\n",
            " [ 1.87350135e-16-3.95516953e-16j  3.78169718e-16+5.55111512e-16j\n",
            "   1.00000000e+00-1.01336686e-18j ...  6.38378239e-16+8.18789481e-16j\n",
            "  -2.74086309e-16-5.20417043e-16j  4.85722573e-17-4.23272528e-16j]\n",
            " ...\n",
            " [ 6.81746326e-16+5.07189776e-16j  7.91033905e-16+1.17267307e-15j\n",
            "   6.93889390e-16-8.18789481e-16j ...  1.00000000e+00-1.40196419e-18j\n",
            "  -1.75207071e-16+8.53483950e-16j  3.46944695e-16-6.38378239e-16j]\n",
            " [-7.04297731e-16-6.24500451e-17j  4.16333634e-17+4.51089090e-16j\n",
            "  -2.63677968e-16+5.20417043e-16j ... -1.75207071e-16-8.60422844e-16j\n",
            "   1.00000000e+00+4.01611805e-18j -1.38777878e-16+5.13478149e-16j]\n",
            " [-2.60208521e-16+5.55111512e-16j -2.35922393e-16+2.22044605e-16j\n",
            "   6.72205347e-17+4.23272528e-16j ...  3.46944695e-16+6.34908792e-16j\n",
            "  -1.38777878e-16-4.99600361e-16j  1.00000000e+00-6.72126142e-18j]]\n",
            "[[ 1.00000000e+00-6.11733426e-18j -2.77555756e-16+1.28369537e-16j\n",
            "   4.89192020e-16+5.20417043e-16j ...  6.17561557e-16+8.32667268e-17j\n",
            "  -7.45931095e-16+4.44089210e-16j  2.15105711e-16+3.03576608e-16j]\n",
            " [-2.77555756e-16-1.28369537e-16j  1.00000000e+00+2.93457884e-18j\n",
            "  -4.16333634e-17-4.02455846e-16j ...  2.18575158e-16-7.97972799e-16j\n",
            "  -6.31439345e-16-5.65519853e-16j -7.84095011e-16+3.92047506e-16j]\n",
            " [ 4.89192020e-16-5.13478149e-16j -4.16333634e-17+4.16333634e-16j\n",
            "   1.00000000e+00+4.19581117e-18j ...  4.71844785e-16+4.02455846e-16j\n",
            "   6.93889390e-16-3.53883589e-16j -2.22044605e-16-4.76181594e-16j]\n",
            " ...\n",
            " [ 6.24500451e-16-8.32667268e-17j  2.22044605e-16+7.97972799e-16j\n",
            "   4.64905892e-16-4.02455846e-16j ...  1.00000000e+00-9.18932078e-19j\n",
            "  -2.49800181e-16+4.78783679e-16j  2.91433544e-16-3.05311332e-16j]\n",
            " [-7.56339436e-16-4.44089210e-16j -6.45317133e-16+5.65519853e-16j\n",
            "   6.81746326e-16+3.53883589e-16j ... -2.49800181e-16-5.06539255e-16j\n",
            "   1.00000000e+00-1.23589763e-18j -8.46545056e-16+8.85576334e-16j]\n",
            " [ 1.94289029e-16-3.03576608e-16j -7.75421394e-16-3.92047506e-16j\n",
            "  -2.35922393e-16+4.76181594e-16j ...  2.91433544e-16+3.05311332e-16j\n",
            "  -8.46545056e-16-8.76902717e-16j  1.00000000e+00-9.00364469e-19j]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def measure_sv(\n",
        "    state:jnp.ndarray,\n",
        "    observable:jnp.ndarray\n",
        "    ):\n",
        "  \"\"\"\n",
        "  Measure a statevector with a Hermitian observable.\n",
        "  Note: No checking Hermitianicity of the observable or whether the observable\n",
        "  has all real eigenvalues or not\n",
        "  \"\"\"\n",
        "  expectation_value = jnp.dot(jnp.conj(state.T), jnp.dot(observable, state))\n",
        "  return jnp.real(expectation_value)\n",
        "\n",
        "def measure_dm(\n",
        "    rho:jnp.ndarray,\n",
        "    observable:jnp.ndarray\n",
        "):\n",
        "  \"\"\"\n",
        "  Measure a density matrix with a Hermitian observable.\n",
        "  Note: No checking Hermitianicity of the observable or whether the observable\n",
        "  has all real eigenvalues or not.\n",
        "  \"\"\"\n",
        "  product = jnp.dot(rho, observable)\n",
        "\n",
        "  # Calculate the trace, which is the sum of diagonal elements\n",
        "  trace = jnp.trace(product)\n",
        "\n",
        "  # The expectation value should be real for physical observables\n",
        "  return jnp.real(trace)\n",
        "\n",
        "vmap_measure_sv = jax.vmap(measure_sv, in_axes=(None, 0), out_axes=0)\n",
        "vmap_measure_dm = jax.vmap(measure_dm, in_axes=(None, 0), out_axes=0)\n",
        "\n",
        "def bitstring_to_state(bitstring:str):\n",
        "  \"\"\"\n",
        "  Convert a bit string, like '0101001' or '+-+-101'\n",
        "  to a statevector. Each character in the bitstring must be among\n",
        "  0, 1, + and -\n",
        "  \"\"\"\n",
        "  assert len(bitstring)>0\n",
        "  for c in bitstring:\n",
        "    assert c in ['0', '1', '+', '-']\n",
        "  single_qubit_states = [ket[c] for c in bitstring]\n",
        "  return tensor_product(*single_qubit_states)"
      ],
      "metadata": {
        "id": "cJ8epBAvc_77"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The QNN\n",
        "\n",
        "With data re-uploading\n",
        "\n",
        "$$\n",
        "|{\\varphi(\\theta,t)}\\rangle = \\Pi_n (\\mathrm{ParameterisedLayers}(\\theta_n) e^{-\\frac{it_n}{2}M} )|+\\rangle^{\\otimes 5}\n",
        "$$"
      ],
      "metadata": {
        "id": "BMtaAqSXdDHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def qnn_hamevo(\n",
        "    params:jnp.ndarray,\n",
        "    t:jnp.ndarray,\n",
        "    img:jnp.ndarray\n",
        ")->jnp.ndarray:\n",
        "  \"\"\"\n",
        "  A QNN that takes (M+M^T)/2\n",
        "  as input, where M is the (rescaled) original image,\n",
        "  as well as a trainable parameter t,\n",
        "  and parameters for trainable layers\n",
        "  and output an array of 2 elements representing classification logits\n",
        "  \"\"\"\n",
        "  single_op_params = 4**5-1\n",
        "\n",
        "  n_outer_layers = len(t)\n",
        "  n_inner_layers = (len(params)//single_op_params)//n_outer_layers\n",
        "  state = tensor_product(ket['+'], ket['+'], ket['+'], ket['+'], ket['+'])\n",
        "  for i in range(n_outer_layers):\n",
        "    state = jnp.dot(\n",
        "      img_hermitian_evolve(img, t[i]),\n",
        "      state\n",
        "      )\n",
        "    inner_layer_params = params[i*(single_op_params*n_inner_layers):(i+1)*(single_op_params*n_inner_layers)]\n",
        "    for j in range(n_inner_layers):\n",
        "      state = jnp.dot(\n",
        "          #brickwall_su4_5q_single_layer(inner_layer_params[j*single_op_params:(j+1)*single_op_params]),\n",
        "          su32_op(inner_layer_params[j*single_op_params:(j+1)*single_op_params]),\n",
        "          state\n",
        "      )\n",
        "  return vmap_measure_sv(state, jnp.asarray(observables_8_cls_5q))\n",
        "\n",
        "\n",
        "\n",
        "print(\n",
        "    qnn_hamevo(\n",
        "        jax.random.normal(shape=[( 4**5-1)*15], key=jrng_key),\n",
        "        jax.random.normal(shape=[15], key=jrng_key),\n",
        "        dummy_x[0]\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdyCKp2pdE8j",
        "outputId": "0b183b20-f826-4d0a-de5e-63ab3cf808ee"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0662222  0.01141476 0.0228869  0.07490359 0.16326536 0.04935118\n",
            " 0.01021365 0.02042996]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "eFjfEttbdNEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def compute_out(weight,t, features, labels):\n",
        "    \"\"\"Computes the output of the corresponding label in the qcnn\"\"\"\n",
        "    out = lambda weight,t, feature, label: qnn_hamevo(weight,t, feature)\n",
        "    return jax.vmap(out, in_axes=(None,None,  0, 0), out_axes=0)(\n",
        "        weight,t, features, labels\n",
        "    )\n",
        "\n",
        "\n",
        "def compute_accuracy(weight,t, features, labels):\n",
        "    \"\"\"Computes the accuracy over the provided features and labels\"\"\"\n",
        "    out = compute_out(weight,t, features, labels)\n",
        "    pred = jnp.argmax(out, axis = 1)\n",
        "    return jnp.sum(jnp.array(pred == labels).astype(int)) / len(out)\n",
        "\n",
        "\n",
        "def compute_cost(weight,t, features, labels):\n",
        "    \"\"\"Computes the cost over the provided features and labels\"\"\"\n",
        "    logits = compute_out(weight,t, features, labels)\n",
        "    return jnp.nanmean(optax.softmax_cross_entropy_with_integer_labels(logits, labels))\n",
        "\n",
        "\n",
        "value_and_grad = jax.jit(jax.value_and_grad(compute_cost, argnums=[0,1]))"
      ],
      "metadata": {
        "id": "6Pc1I3fqdOX_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_OUTER_LAYERS = 10\n",
        "N_INNER_LAYERS = 1\n",
        "N_LAYERS = N_OUTER_LAYERS*N_INNER_LAYERS\n",
        "SINGLE_OP_PARAMS  = 4**5-1\n",
        "\n",
        "# def init_weights():\n",
        "#     return jax.random.normal(shape=[SINGLE_OP_PARAMS*N_LAYERS], key=jrng_key),jax.random.normal(shape=[N_OUTER_LAYERS], key=jrng_key)\n",
        "\n",
        "def init_weights(alpha=0.5, beta=2.0):\n",
        "    # Initialize weights with a Beta distribution skewed towards 0\n",
        "    weights = jax.random.beta(jrng_key, alpha, beta, shape=[SINGLE_OP_PARAMS*N_LAYERS])\n",
        "    biases = jax.random.beta(jrng_key, alpha, beta, shape=[N_OUTER_LAYERS])\n",
        "    return weights, biases"
      ],
      "metadata": {
        "id": "aI13rv6DdRJO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select data\n",
        "labels = [0,1,2,3,4,5,6,7,8,9]\n",
        "indices_train = [idx for idx, target in enumerate(train_dataset.targets) if target in labels]\n",
        "indices_test = [idx for idx, target in enumerate(test_dataset.targets) if target in labels]\n",
        "\n",
        "N_TRAIN = len(indices_train)\n",
        "N_TEST = len(indices_test)\n",
        "\n",
        "print(\n",
        "    f\"Training with: {N_TRAIN}; Testing with: {N_TEST}\"\n",
        ")\n",
        "\n",
        "def train_vqc(batchsize:int, n_epochs:int, seed:int=1701):\n",
        "  start = time.time()\n",
        "  pnp.random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  # load data\n",
        "  labels = [0,1,2,3,4,5,6,7,8,9]\n",
        "  indices_train = [idx for idx, target in enumerate(train_dataset.targets) if target in labels]\n",
        "  indices_test = [idx for idx, target in enumerate(test_dataset.targets) if target in labels]\n",
        "  trainloader = torch.utils.data.DataLoader(\n",
        "  torch.utils.data.Subset(train_dataset, indices_train), batch_size=batchsize, shuffle=True\n",
        ")\n",
        "  testloader = torch.utils.data.DataLoader(\n",
        "  torch.utils.data.Subset(test_dataset, indices_test), batch_size=batchsize, shuffle=True\n",
        ")\n",
        "  # Exponential decay of the learning rate.\n",
        "  scheduler = optax.exponential_decay(\n",
        "    init_value=0.01,\n",
        "    transition_steps=n_epochs,\n",
        "    decay_rate=0.99)\n",
        "\n",
        "  # Combining gradient transforms using `optax.chain`.\n",
        "  gradient_transform = optax.chain(\n",
        "    optax.clip(1.0),\n",
        "    optax.scale_by_adam(),  # Use the updates from adam.\n",
        "    optax.scale_by_schedule(scheduler),  # Use the learning rate from the scheduler.\n",
        "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
        "    optax.scale(-1.0)\n",
        "  )\n",
        "  # init weights and optimizer\n",
        "  # weights, weights_last = init_weights() # for normal\n",
        "  weights, weights_last = init_weights(0.5,2.0) # for beta\n",
        "\n",
        "  opt_state = gradient_transform.init((weights, weights_last))\n",
        "  #data containers\n",
        "  train_cost_epochs, test_cost_epochs, train_acc_epochs, test_acc_epochs = [], [], [], []\n",
        "  for step in range(n_epochs):\n",
        "        train_cost_batches = []\n",
        "        train_acc_batches = []\n",
        "        test_cost_batches = []\n",
        "        test_acc_batches = []\n",
        "        epoch_start = time.time()\n",
        "        print(f\"Training at Epoch {step+1}/{n_epochs}, Train batches {len(trainloader)}, Test batches {len(testloader)}......\")\n",
        "        for batch, (x_train, y_train) in enumerate(trainloader):\n",
        "          batch_start = time.time()\n",
        "          # Training step with (adam) optimizer\n",
        "          x_train, y_train = jnp.asarray(x_train.numpy()), jnp.asarray(y_train.numpy())\n",
        "          train_cost, grad_circuit = value_and_grad(weights, weights_last, x_train, y_train)\n",
        "          updates, opt_state = gradient_transform.update(grad_circuit, opt_state)\n",
        "          weights, weights_last = optax.apply_updates((weights, weights_last), updates)\n",
        "          train_acc = compute_accuracy(weights, weights_last, x_train, y_train)\n",
        "          train_cost_batches.append(train_cost)\n",
        "          train_acc_batches.append(train_acc)\n",
        "          if len(trainloader)<= 5 or (batch+1)%5==0:\n",
        "            print(f\"Training at Epoch {step+1}/{n_epochs}, Batch {batch+1}, Cost {train_cost}, Acc {train_acc}. Time {time.time()-batch_start}\")\n",
        "\n",
        "\n",
        "        train_cost_epochs.append(np.mean(train_cost_batches))\n",
        "        train_acc_epochs.append(np.mean(train_acc_batches))\n",
        "\n",
        "\n",
        "        # load test data\n",
        "        for batch, (x_test, y_test) in enumerate(testloader):\n",
        "          batch_start = time.time()\n",
        "          x_test, y_test = jnp.asarray(x_test.numpy()), jnp.asarray(y_test.numpy())\n",
        "          # compute accuracy and cost on testing data\n",
        "          test_out = compute_out(weights, weights_last, x_test, y_test)\n",
        "          test_pred = jnp.argmax(test_out, axis=1)\n",
        "          test_acc = jnp.sum(jnp.array(test_pred == y_test).astype(int)) / len(test_out)\n",
        "          test_cost = jnp.nanmean(optax.softmax_cross_entropy_with_integer_labels(test_out, y_test))\n",
        "          test_cost_batches.append(test_cost)\n",
        "          test_acc_batches.append(test_acc)\n",
        "          if len(testloader)<= 5 or (batch+1)%5==0:\n",
        "            print(f\"Testing at Epoch {step+1}/{n_epochs}, Batch {batch+1}, Cost {test_cost}, Acc {test_acc}. Time {time.time()-batch_start}\")\n",
        "        test_acc_epochs.append(np.mean(test_acc_batches))\n",
        "        test_cost = np.mean(test_cost_batches)\n",
        "        test_cost_epochs.append(test_cost)\n",
        "        print(\"......\")\n",
        "        print(f\"Epoch {step+1}/{n_epochs}, Train: Cost {np.mean(train_cost_batches)}, Acc {np.mean(train_acc_batches)}\")\n",
        "        print(f\"Epoch {step+1}/{n_epochs}, Test: Cost {test_cost}, Acc {test_acc}. Time {time.time()-epoch_start}\")\n",
        "        print(\"=-=\"*10)\n",
        "\n",
        "  return dict(\n",
        "        n_train=[N_TRAIN] * n_epochs,\n",
        "        step=np.arange(1, n_epochs + 1, dtype=int).tolist(),\n",
        "        train_cost=[c.astype(float) for c in train_cost_epochs],\n",
        "        train_acc=[c.astype(float) for c in train_acc_epochs],\n",
        "        test_cost=[c.astype(float) for c in test_cost_epochs],\n",
        "        test_acc=[c.astype(float) for c in test_acc_epochs],\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLyRg7EydVgN",
        "outputId": "67da2e68-2bb1-4b1a-ad56-d04ea237da4c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with: 60000; Testing with: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 25\n",
        "n_reps = 1\n",
        "batch_size = 500\n",
        "\n",
        "train_sizes = [N_TRAIN]\n",
        "\n",
        "def run_iterations():\n",
        "    results_df = pd.DataFrame(\n",
        "        columns=[\"train_acc\", \"train_cost\", \"test_acc\", \"test_cost\", \"step\", \"n_train\"]\n",
        "    )\n",
        "\n",
        "    for _ in range(n_reps):\n",
        "        results = train_vqc(n_epochs=n_epochs, batchsize=batch_size)\n",
        "        results_df = pd.concat(\n",
        "            [results_df, pd.DataFrame.from_dict(results)], axis=0, ignore_index=True\n",
        "        )\n",
        "\n",
        "    return results_df\n",
        "\n",
        "results_df = run_iterations()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTZnVAUldvDs",
        "outputId": "63182e9b-97d9-4431-fa80-a279da4d2ada"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training at Epoch 1/25, Train batches 120, Test batches 20......\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/jax/_src/lax/lax.py:3227: ComplexWarning: Casting complex values to real discards the imaginary part\n",
            "  x_bar = _convert_element_type(x_bar, x.aval.dtype, x.aval.weak_type)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training at Epoch 1/25, Batch 5, Cost 2.029289246793329, Acc 0.256. Time 1.3549363613128662\n",
            "Training at Epoch 1/25, Batch 10, Cost 1.9300579078145048, Acc 0.504. Time 1.3565199375152588\n",
            "Training at Epoch 1/25, Batch 15, Cost 1.8678731660285615, Acc 0.542. Time 1.3752751350402832\n",
            "Training at Epoch 1/25, Batch 20, Cost 1.837280234102852, Acc 0.538. Time 1.3685142993927002\n",
            "Training at Epoch 1/25, Batch 25, Cost 1.8181887759200175, Acc 0.63. Time 1.359377384185791\n",
            "Training at Epoch 1/25, Batch 30, Cost 1.8066651056053744, Acc 0.6. Time 1.3574020862579346\n",
            "Training at Epoch 1/25, Batch 35, Cost 1.7774442794548204, Acc 0.616. Time 1.338569164276123\n",
            "Training at Epoch 1/25, Batch 40, Cost 1.7706127315284064, Acc 0.626. Time 1.3459606170654297\n",
            "Training at Epoch 1/25, Batch 45, Cost 1.758234925242113, Acc 0.618. Time 1.3286020755767822\n",
            "Training at Epoch 1/25, Batch 50, Cost 1.7307015070712422, Acc 0.618. Time 1.3562514781951904\n",
            "Training at Epoch 1/25, Batch 55, Cost 1.7273619270992593, Acc 0.638. Time 1.3280830383300781\n",
            "Training at Epoch 1/25, Batch 60, Cost 1.7099437144198335, Acc 0.642. Time 1.3332269191741943\n",
            "Training at Epoch 1/25, Batch 65, Cost 1.712197137402116, Acc 0.652. Time 1.3325698375701904\n",
            "Training at Epoch 1/25, Batch 70, Cost 1.713979527233696, Acc 0.652. Time 1.3440771102905273\n",
            "Training at Epoch 1/25, Batch 75, Cost 1.705285090830397, Acc 0.642. Time 1.3477303981781006\n",
            "Training at Epoch 1/25, Batch 80, Cost 1.6966786977437567, Acc 0.654. Time 1.3448846340179443\n",
            "Training at Epoch 1/25, Batch 85, Cost 1.698444488122529, Acc 0.662. Time 1.3572890758514404\n",
            "Training at Epoch 1/25, Batch 90, Cost 1.6933911375759227, Acc 0.67. Time 1.3414018154144287\n",
            "Training at Epoch 1/25, Batch 95, Cost 1.6883333171505952, Acc 0.66. Time 1.351168155670166\n",
            "Training at Epoch 1/25, Batch 100, Cost 1.6775506738345574, Acc 0.648. Time 1.3392565250396729\n",
            "Training at Epoch 1/25, Batch 105, Cost 1.689275486571401, Acc 0.674. Time 1.3349425792694092\n",
            "Training at Epoch 1/25, Batch 110, Cost 1.6928722310716469, Acc 0.656. Time 1.3388540744781494\n",
            "Training at Epoch 1/25, Batch 115, Cost 1.6797599293378758, Acc 0.664. Time 1.3391218185424805\n",
            "Training at Epoch 1/25, Batch 120, Cost 1.6677587954219233, Acc 0.692. Time 1.3594603538513184\n",
            "Testing at Epoch 1/25, Batch 5, Cost 1.6665253968799305, Acc 0.656. Time 0.3414437770843506\n",
            "Testing at Epoch 1/25, Batch 10, Cost 1.66372500230493, Acc 0.684. Time 0.34059691429138184\n",
            "Testing at Epoch 1/25, Batch 15, Cost 1.689740746402727, Acc 0.646. Time 0.3385171890258789\n",
            "Testing at Epoch 1/25, Batch 20, Cost 1.656171777083811, Acc 0.698. Time 0.3392798900604248\n",
            "......\n",
            "Epoch 1/25, Train: Cost 1.758174370389908, Acc 0.61315\n",
            "Epoch 1/25, Test: Cost 1.6662390978223496, Acc 0.698. Time 233.37818312644958\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 2/25, Train batches 120, Test batches 20......\n",
            "Training at Epoch 2/25, Batch 5, Cost 1.659323284543968, Acc 0.682. Time 1.3421602249145508\n",
            "Training at Epoch 2/25, Batch 10, Cost 1.6604003034296988, Acc 0.682. Time 1.3440017700195312\n",
            "Training at Epoch 2/25, Batch 15, Cost 1.6620374128960727, Acc 0.714. Time 1.340399980545044\n",
            "Training at Epoch 2/25, Batch 20, Cost 1.669758777525806, Acc 0.654. Time 1.3418998718261719\n",
            "Training at Epoch 2/25, Batch 25, Cost 1.6677949529946534, Acc 0.688. Time 1.3384878635406494\n",
            "Training at Epoch 2/25, Batch 30, Cost 1.6529108579325282, Acc 0.686. Time 1.3567564487457275\n",
            "Training at Epoch 2/25, Batch 35, Cost 1.6494938530672774, Acc 0.712. Time 1.3379795551300049\n",
            "Training at Epoch 2/25, Batch 40, Cost 1.668261167485151, Acc 0.698. Time 1.354433536529541\n",
            "Training at Epoch 2/25, Batch 45, Cost 1.6587933308841771, Acc 0.704. Time 1.34041166305542\n",
            "Training at Epoch 2/25, Batch 50, Cost 1.6677874592010076, Acc 0.674. Time 1.3423206806182861\n",
            "Training at Epoch 2/25, Batch 55, Cost 1.6661781336150832, Acc 0.688. Time 1.3392417430877686\n",
            "Training at Epoch 2/25, Batch 60, Cost 1.6326628850168297, Acc 0.728. Time 1.337754726409912\n",
            "Training at Epoch 2/25, Batch 65, Cost 1.6687358242255288, Acc 0.696. Time 1.3503122329711914\n",
            "Training at Epoch 2/25, Batch 70, Cost 1.631948740351362, Acc 0.706. Time 1.3498289585113525\n",
            "Training at Epoch 2/25, Batch 75, Cost 1.64977225469598, Acc 0.7. Time 1.3545286655426025\n",
            "Training at Epoch 2/25, Batch 80, Cost 1.6543607886861003, Acc 0.704. Time 1.3414957523345947\n",
            "Training at Epoch 2/25, Batch 85, Cost 1.6439816096129916, Acc 0.698. Time 1.3382680416107178\n",
            "Training at Epoch 2/25, Batch 90, Cost 1.64792087413709, Acc 0.72. Time 1.3399806022644043\n",
            "Training at Epoch 2/25, Batch 95, Cost 1.6449049325899634, Acc 0.706. Time 1.34376859664917\n",
            "Training at Epoch 2/25, Batch 100, Cost 1.65488798936841, Acc 0.68. Time 1.3522233963012695\n",
            "Training at Epoch 2/25, Batch 105, Cost 1.6374804083688874, Acc 0.676. Time 1.3435697555541992\n",
            "Training at Epoch 2/25, Batch 110, Cost 1.65374435880271, Acc 0.682. Time 1.3622334003448486\n",
            "Training at Epoch 2/25, Batch 115, Cost 1.6314188198639163, Acc 0.714. Time 1.3401541709899902\n",
            "Training at Epoch 2/25, Batch 120, Cost 1.6156126604694554, Acc 0.724. Time 1.3395659923553467\n",
            "Testing at Epoch 2/25, Batch 5, Cost 1.6406246547166015, Acc 0.648. Time 0.33939671516418457\n",
            "Testing at Epoch 2/25, Batch 10, Cost 1.6333177467993159, Acc 0.69. Time 0.34041309356689453\n",
            "Testing at Epoch 2/25, Batch 15, Cost 1.621209852523111, Acc 0.712. Time 0.3409717082977295\n",
            "Testing at Epoch 2/25, Batch 20, Cost 1.6327371382768459, Acc 0.696. Time 0.3402588367462158\n",
            "......\n",
            "Epoch 2/25, Train: Cost 1.6478543786481155, Acc 0.6955333333333333\n",
            "Epoch 2/25, Test: Cost 1.628269066791096, Acc 0.696. Time 181.28624963760376\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 3/25, Train batches 120, Test batches 20......\n",
            "Training at Epoch 3/25, Batch 5, Cost 1.63719222644691, Acc 0.694. Time 1.342587947845459\n",
            "Training at Epoch 3/25, Batch 10, Cost 1.614330792322777, Acc 0.708. Time 1.3572165966033936\n",
            "Training at Epoch 3/25, Batch 15, Cost 1.6189196121408613, Acc 0.728. Time 1.3396565914154053\n",
            "Training at Epoch 3/25, Batch 20, Cost 1.6350372413725105, Acc 0.72. Time 1.354261875152588\n",
            "Training at Epoch 3/25, Batch 25, Cost 1.6263139102623059, Acc 0.7. Time 1.3422043323516846\n",
            "Training at Epoch 3/25, Batch 30, Cost 1.642052724253215, Acc 0.696. Time 1.3448033332824707\n",
            "Training at Epoch 3/25, Batch 35, Cost 1.6292926069023217, Acc 0.708. Time 1.341951847076416\n",
            "Training at Epoch 3/25, Batch 40, Cost 1.629221673543676, Acc 0.712. Time 1.3403818607330322\n",
            "Training at Epoch 3/25, Batch 45, Cost 1.6246587829182624, Acc 0.724. Time 1.355323076248169\n",
            "Training at Epoch 3/25, Batch 50, Cost 1.621021687530546, Acc 0.728. Time 1.3431994915008545\n",
            "Training at Epoch 3/25, Batch 55, Cost 1.6210784645921281, Acc 0.714. Time 1.3563740253448486\n",
            "Training at Epoch 3/25, Batch 60, Cost 1.6111606706563402, Acc 0.746. Time 1.3512086868286133\n",
            "Training at Epoch 3/25, Batch 65, Cost 1.6268238310534817, Acc 0.69. Time 1.3425803184509277\n",
            "Training at Epoch 3/25, Batch 70, Cost 1.6235210448299935, Acc 0.712. Time 1.3423466682434082\n",
            "Training at Epoch 3/25, Batch 75, Cost 1.6332197078974227, Acc 0.702. Time 1.3434760570526123\n",
            "Training at Epoch 3/25, Batch 80, Cost 1.6364072903914602, Acc 0.716. Time 1.3475406169891357\n",
            "Training at Epoch 3/25, Batch 85, Cost 1.639027795055337, Acc 0.678. Time 1.351048469543457\n",
            "Training at Epoch 3/25, Batch 90, Cost 1.6206190413139758, Acc 0.712. Time 1.3604164123535156\n",
            "Training at Epoch 3/25, Batch 95, Cost 1.633528318265546, Acc 0.698. Time 1.3462753295898438\n",
            "Training at Epoch 3/25, Batch 100, Cost 1.6047824264214052, Acc 0.722. Time 1.3433940410614014\n",
            "Training at Epoch 3/25, Batch 105, Cost 1.6191049821735788, Acc 0.682. Time 1.3413403034210205\n",
            "Training at Epoch 3/25, Batch 110, Cost 1.596969655396001, Acc 0.692. Time 1.3425946235656738\n",
            "Training at Epoch 3/25, Batch 115, Cost 1.619457685788425, Acc 0.702. Time 1.3415348529815674\n",
            "Training at Epoch 3/25, Batch 120, Cost 1.622167429850482, Acc 0.728. Time 1.3444762229919434\n",
            "Testing at Epoch 3/25, Batch 5, Cost 1.631048362314632, Acc 0.69. Time 0.3409392833709717\n",
            "Testing at Epoch 3/25, Batch 10, Cost 1.630527190306432, Acc 0.706. Time 0.3414440155029297\n",
            "Testing at Epoch 3/25, Batch 15, Cost 1.6080730674163204, Acc 0.74. Time 0.34305715560913086\n",
            "Testing at Epoch 3/25, Batch 20, Cost 1.603450978326536, Acc 0.69. Time 0.3402516841888428\n",
            "......\n",
            "Epoch 3/25, Train: Cost 1.6247461923118547, Acc 0.7059833333333333\n",
            "Epoch 3/25, Test: Cost 1.610666111318184, Acc 0.69. Time 181.8361873626709\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 4/25, Train batches 120, Test batches 20......\n",
            "Training at Epoch 4/25, Batch 5, Cost 1.6086598812419644, Acc 0.704. Time 1.3424842357635498\n",
            "Training at Epoch 4/25, Batch 10, Cost 1.6312435744003435, Acc 0.696. Time 1.343745470046997\n",
            "Training at Epoch 4/25, Batch 15, Cost 1.6317354387742709, Acc 0.646. Time 1.3399450778961182\n",
            "Training at Epoch 4/25, Batch 20, Cost 1.609984174971569, Acc 0.712. Time 1.342115879058838\n",
            "Training at Epoch 4/25, Batch 25, Cost 1.6040097000772087, Acc 0.712. Time 1.3488624095916748\n",
            "Training at Epoch 4/25, Batch 30, Cost 1.6036128652393735, Acc 0.696. Time 1.3443927764892578\n",
            "Training at Epoch 4/25, Batch 35, Cost 1.610344690302325, Acc 0.718. Time 1.3593502044677734\n",
            "Training at Epoch 4/25, Batch 40, Cost 1.6131086725281027, Acc 0.696. Time 1.340723991394043\n",
            "Training at Epoch 4/25, Batch 45, Cost 1.6170324644373755, Acc 0.704. Time 1.3403937816619873\n",
            "Training at Epoch 4/25, Batch 50, Cost 1.6011336128429163, Acc 0.716. Time 1.3414454460144043\n",
            "Training at Epoch 4/25, Batch 55, Cost 1.6187111452889735, Acc 0.7. Time 1.3386952877044678\n",
            "Training at Epoch 4/25, Batch 60, Cost 1.6129482212778203, Acc 0.698. Time 1.3507587909698486\n",
            "Training at Epoch 4/25, Batch 65, Cost 1.6194283796628732, Acc 0.71. Time 1.346672534942627\n",
            "Training at Epoch 4/25, Batch 70, Cost 1.6061287901084493, Acc 0.712. Time 1.3572900295257568\n",
            "Training at Epoch 4/25, Batch 75, Cost 1.5972992978630445, Acc 0.722. Time 1.3374061584472656\n",
            "Training at Epoch 4/25, Batch 80, Cost 1.6144278132845897, Acc 0.7. Time 1.344496250152588\n",
            "Training at Epoch 4/25, Batch 85, Cost 1.6160740415091657, Acc 0.67. Time 1.3439717292785645\n",
            "Training at Epoch 4/25, Batch 90, Cost 1.621730451759119, Acc 0.714. Time 1.3421645164489746\n",
            "Training at Epoch 4/25, Batch 95, Cost 1.617794655949662, Acc 0.724. Time 1.3408987522125244\n",
            "Training at Epoch 4/25, Batch 100, Cost 1.6233564967603822, Acc 0.708. Time 1.343355655670166\n",
            "Training at Epoch 4/25, Batch 105, Cost 1.6181207175257821, Acc 0.7. Time 1.3571460247039795\n",
            "Training at Epoch 4/25, Batch 110, Cost 1.6128576322408874, Acc 0.724. Time 1.3363251686096191\n",
            "Training at Epoch 4/25, Batch 115, Cost 1.6066711044692068, Acc 0.712. Time 1.3567016124725342\n",
            "Training at Epoch 4/25, Batch 120, Cost 1.613289425024914, Acc 0.694. Time 1.3412487506866455\n",
            "Testing at Epoch 4/25, Batch 5, Cost 1.6114981094763337, Acc 0.7. Time 0.3403949737548828\n",
            "Testing at Epoch 4/25, Batch 10, Cost 1.5993470384268482, Acc 0.708. Time 0.34836459159851074\n",
            "Testing at Epoch 4/25, Batch 15, Cost 1.610006192752698, Acc 0.698. Time 0.3385000228881836\n",
            "Testing at Epoch 4/25, Batch 20, Cost 1.6264561658669963, Acc 0.706. Time 0.3381955623626709\n",
            "......\n",
            "Epoch 4/25, Train: Cost 1.613246154622204, Acc 0.7091999999999998\n",
            "Epoch 4/25, Test: Cost 1.605276943248778, Acc 0.706. Time 181.4427206516266\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 5/25, Train batches 120, Test batches 20......\n",
            "Training at Epoch 5/25, Batch 5, Cost 1.6245871375716006, Acc 0.72. Time 1.3401789665222168\n",
            "Training at Epoch 5/25, Batch 10, Cost 1.6032697391314266, Acc 0.706. Time 1.3415837287902832\n",
            "Training at Epoch 5/25, Batch 15, Cost 1.5976467277249573, Acc 0.698. Time 1.354801893234253\n",
            "Training at Epoch 5/25, Batch 20, Cost 1.6017468160067916, Acc 0.722. Time 1.3467071056365967\n",
            "Training at Epoch 5/25, Batch 25, Cost 1.616512360161169, Acc 0.706. Time 1.3437252044677734\n",
            "Training at Epoch 5/25, Batch 30, Cost 1.5851109060636306, Acc 0.724. Time 1.355973720550537\n",
            "Training at Epoch 5/25, Batch 35, Cost 1.6116054878459887, Acc 0.714. Time 1.3427283763885498\n",
            "Training at Epoch 5/25, Batch 40, Cost 1.6197143685564503, Acc 0.68. Time 1.3388309478759766\n",
            "Training at Epoch 5/25, Batch 45, Cost 1.6145875353226073, Acc 0.678. Time 1.3376526832580566\n",
            "Training at Epoch 5/25, Batch 50, Cost 1.608912183673232, Acc 0.704. Time 1.3539190292358398\n",
            "Training at Epoch 5/25, Batch 55, Cost 1.6185959322103756, Acc 0.728. Time 1.3384425640106201\n",
            "Training at Epoch 5/25, Batch 60, Cost 1.6064505935631312, Acc 0.732. Time 1.353421688079834\n",
            "Training at Epoch 5/25, Batch 65, Cost 1.592216042369832, Acc 0.71. Time 1.3534634113311768\n",
            "Training at Epoch 5/25, Batch 70, Cost 1.6117512847346276, Acc 0.702. Time 1.3527684211730957\n",
            "Training at Epoch 5/25, Batch 75, Cost 1.6024913592946342, Acc 0.742. Time 1.3531162738800049\n",
            "Training at Epoch 5/25, Batch 80, Cost 1.5951077710059622, Acc 0.736. Time 1.3565964698791504\n",
            "Training at Epoch 5/25, Batch 85, Cost 1.605792222754233, Acc 0.696. Time 1.373366117477417\n",
            "Training at Epoch 5/25, Batch 90, Cost 1.6102449161712107, Acc 0.734. Time 1.354079008102417\n",
            "Training at Epoch 5/25, Batch 95, Cost 1.591163041714825, Acc 0.712. Time 1.3700528144836426\n",
            "Training at Epoch 5/25, Batch 100, Cost 1.614276620058176, Acc 0.702. Time 1.3569891452789307\n",
            "Training at Epoch 5/25, Batch 105, Cost 1.604933276230764, Acc 0.714. Time 1.351827621459961\n",
            "Training at Epoch 5/25, Batch 110, Cost 1.5903916550401263, Acc 0.718. Time 1.3572571277618408\n",
            "Training at Epoch 5/25, Batch 115, Cost 1.5894292363302667, Acc 0.734. Time 1.3535139560699463\n",
            "Training at Epoch 5/25, Batch 120, Cost 1.6099341116282975, Acc 0.668. Time 1.3695361614227295\n",
            "Testing at Epoch 5/25, Batch 5, Cost 1.5917335514288165, Acc 0.728. Time 0.342179536819458\n",
            "Testing at Epoch 5/25, Batch 10, Cost 1.5916212531329479, Acc 0.734. Time 0.3425254821777344\n",
            "Testing at Epoch 5/25, Batch 15, Cost 1.6101023422362994, Acc 0.708. Time 0.3427760601043701\n",
            "Testing at Epoch 5/25, Batch 20, Cost 1.5959623315265168, Acc 0.738. Time 0.34203076362609863\n",
            "......\n",
            "Epoch 5/25, Train: Cost 1.6050963868610266, Acc 0.71195\n",
            "Epoch 5/25, Test: Cost 1.601678625946083, Acc 0.738. Time 182.04693150520325\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 6/25, Train batches 120, Test batches 20......\n",
            "Training at Epoch 6/25, Batch 5, Cost 1.5863118506639968, Acc 0.74. Time 1.365415096282959\n",
            "Training at Epoch 6/25, Batch 10, Cost 1.6131084745001079, Acc 0.68. Time 1.351958990097046\n",
            "Training at Epoch 6/25, Batch 15, Cost 1.5904328736134405, Acc 0.732. Time 1.3527097702026367\n",
            "Training at Epoch 6/25, Batch 20, Cost 1.6114835817825885, Acc 0.726. Time 1.3488061428070068\n",
            "Training at Epoch 6/25, Batch 25, Cost 1.5942854682791179, Acc 0.73. Time 1.3486673831939697\n",
            "Training at Epoch 6/25, Batch 30, Cost 1.5910700376952862, Acc 0.724. Time 1.36183762550354\n",
            "Training at Epoch 6/25, Batch 35, Cost 1.6074343386847803, Acc 0.696. Time 1.3468644618988037\n",
            "Training at Epoch 6/25, Batch 40, Cost 1.6084172025380532, Acc 0.71. Time 1.3521947860717773\n",
            "Training at Epoch 6/25, Batch 45, Cost 1.585474709061604, Acc 0.734. Time 1.348447561264038\n",
            "Training at Epoch 6/25, Batch 50, Cost 1.5825615226167593, Acc 0.708. Time 1.3476276397705078\n",
            "Training at Epoch 6/25, Batch 55, Cost 1.606450456245174, Acc 0.684. Time 1.3505587577819824\n",
            "Training at Epoch 6/25, Batch 60, Cost 1.606674022626341, Acc 0.748. Time 1.3521544933319092\n",
            "Training at Epoch 6/25, Batch 65, Cost 1.6239143047794933, Acc 0.706. Time 1.3622410297393799\n",
            "Training at Epoch 6/25, Batch 70, Cost 1.6084667316157804, Acc 0.71. Time 1.3496475219726562\n",
            "Training at Epoch 6/25, Batch 75, Cost 1.6060411884557582, Acc 0.716. Time 1.363161563873291\n",
            "Training at Epoch 6/25, Batch 80, Cost 1.590656735788012, Acc 0.728. Time 1.3487300872802734\n",
            "Training at Epoch 6/25, Batch 85, Cost 1.5833564232062582, Acc 0.704. Time 1.3477535247802734\n",
            "Training at Epoch 6/25, Batch 90, Cost 1.6035371704955994, Acc 0.676. Time 1.348736047744751\n",
            "Training at Epoch 6/25, Batch 95, Cost 1.5930371182085268, Acc 0.704. Time 1.3467087745666504\n",
            "Training at Epoch 6/25, Batch 100, Cost 1.5752470812621127, Acc 0.718. Time 1.361684799194336\n",
            "Training at Epoch 6/25, Batch 105, Cost 1.6046825090325594, Acc 0.694. Time 1.3509879112243652\n",
            "Training at Epoch 6/25, Batch 110, Cost 1.5958156374238632, Acc 0.722. Time 1.3668806552886963\n",
            "Training at Epoch 6/25, Batch 115, Cost 1.5913151833980879, Acc 0.68. Time 1.3498599529266357\n",
            "Training at Epoch 6/25, Batch 120, Cost 1.5947646273065335, Acc 0.704. Time 1.3501341342926025\n",
            "Testing at Epoch 6/25, Batch 5, Cost 1.5840390137334381, Acc 0.72. Time 0.33812499046325684\n",
            "Testing at Epoch 6/25, Batch 10, Cost 1.589678426120109, Acc 0.71. Time 0.34081578254699707\n",
            "Testing at Epoch 6/25, Batch 15, Cost 1.584118456672378, Acc 0.75. Time 0.33854222297668457\n",
            "Testing at Epoch 6/25, Batch 20, Cost 1.6113226739072006, Acc 0.708. Time 0.34140729904174805\n",
            "......\n",
            "Epoch 6/25, Train: Cost 1.5979067682103798, Acc 0.7141999999999998\n",
            "Epoch 6/25, Test: Cost 1.5879984430387875, Acc 0.708. Time 182.1527397632599\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 7/25, Train batches 120, Test batches 20......\n",
            "Training at Epoch 7/25, Batch 5, Cost 1.5839475237323002, Acc 0.716. Time 1.3517117500305176\n",
            "Training at Epoch 7/25, Batch 10, Cost 1.6028335540170757, Acc 0.692. Time 1.3649623394012451\n",
            "Training at Epoch 7/25, Batch 15, Cost 1.5904188739067888, Acc 0.73. Time 1.3367564678192139\n",
            "Training at Epoch 7/25, Batch 20, Cost 1.5803798837287382, Acc 0.744. Time 1.3420534133911133\n",
            "Training at Epoch 7/25, Batch 25, Cost 1.6028033762411937, Acc 0.702. Time 1.3440234661102295\n",
            "Training at Epoch 7/25, Batch 30, Cost 1.611489103356852, Acc 0.708. Time 1.3420474529266357\n",
            "Training at Epoch 7/25, Batch 35, Cost 1.5820216337099746, Acc 0.706. Time 1.3387811183929443\n",
            "Training at Epoch 7/25, Batch 40, Cost 1.584121962138056, Acc 0.74. Time 1.354402780532837\n",
            "Training at Epoch 7/25, Batch 45, Cost 1.5871774013132973, Acc 0.714. Time 1.353830099105835\n",
            "Training at Epoch 7/25, Batch 50, Cost 1.5848668031670265, Acc 0.728. Time 1.3543431758880615\n",
            "Training at Epoch 7/25, Batch 55, Cost 1.5661414686533035, Acc 0.752. Time 1.3676841259002686\n",
            "Training at Epoch 7/25, Batch 60, Cost 1.5979952277875589, Acc 0.706. Time 1.35426664352417\n",
            "Training at Epoch 7/25, Batch 65, Cost 1.6134688877499155, Acc 0.706. Time 1.354177713394165\n",
            "Training at Epoch 7/25, Batch 70, Cost 1.589776167184278, Acc 0.704. Time 1.3582992553710938\n",
            "Training at Epoch 7/25, Batch 75, Cost 1.5949336856365834, Acc 0.716. Time 1.3548057079315186\n",
            "Training at Epoch 7/25, Batch 80, Cost 1.5856060061228792, Acc 0.71. Time 1.3565788269042969\n",
            "Training at Epoch 7/25, Batch 85, Cost 1.5992708699480034, Acc 0.732. Time 1.352933645248413\n",
            "Training at Epoch 7/25, Batch 90, Cost 1.583307745939161, Acc 0.706. Time 1.3672752380371094\n",
            "Training at Epoch 7/25, Batch 95, Cost 1.5813990760590342, Acc 0.708. Time 1.3537273406982422\n",
            "Training at Epoch 7/25, Batch 100, Cost 1.5863738608625304, Acc 0.704. Time 1.3556921482086182\n",
            "Training at Epoch 7/25, Batch 105, Cost 1.6179824369953102, Acc 0.724. Time 1.3525817394256592\n",
            "Training at Epoch 7/25, Batch 110, Cost 1.5924986013262208, Acc 0.704. Time 1.3549094200134277\n",
            "Training at Epoch 7/25, Batch 115, Cost 1.596671961356628, Acc 0.694. Time 1.3629190921783447\n",
            "Training at Epoch 7/25, Batch 120, Cost 1.5885687915452107, Acc 0.714. Time 1.355008840560913\n",
            "Testing at Epoch 7/25, Batch 5, Cost 1.591703090938901, Acc 0.73. Time 0.3407740592956543\n",
            "Testing at Epoch 7/25, Batch 10, Cost 1.5914287376937628, Acc 0.708. Time 0.3426837921142578\n",
            "Testing at Epoch 7/25, Batch 15, Cost 1.578426342142127, Acc 0.706. Time 0.3465752601623535\n",
            "Testing at Epoch 7/25, Batch 20, Cost 1.6092803622091587, Acc 0.692. Time 0.34285569190979004\n",
            "......\n",
            "Epoch 7/25, Train: Cost 1.5918503162088011, Acc 0.7164833333333332\n",
            "Epoch 7/25, Test: Cost 1.585479602935088, Acc 0.692. Time 182.70843815803528\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 8/25, Train batches 120, Test batches 20......\n",
            "Training at Epoch 8/25, Batch 5, Cost 1.5915440730206107, Acc 0.73. Time 1.3540570735931396\n",
            "Training at Epoch 8/25, Batch 10, Cost 1.5833674067964678, Acc 0.754. Time 1.351531982421875\n",
            "Training at Epoch 8/25, Batch 15, Cost 1.5907699063581189, Acc 0.692. Time 1.3528993129730225\n",
            "Training at Epoch 8/25, Batch 20, Cost 1.6166900118416996, Acc 0.688. Time 1.3536088466644287\n",
            "Training at Epoch 8/25, Batch 25, Cost 1.605860137458501, Acc 0.726. Time 1.3751859664916992\n",
            "Training at Epoch 8/25, Batch 30, Cost 1.590869338523658, Acc 0.69. Time 1.3532562255859375\n",
            "Training at Epoch 8/25, Batch 35, Cost 1.587800983169836, Acc 0.746. Time 1.3494603633880615\n",
            "Training at Epoch 8/25, Batch 40, Cost 1.5841114704563435, Acc 0.702. Time 1.3453612327575684\n",
            "Training at Epoch 8/25, Batch 45, Cost 1.58701993024922, Acc 0.692. Time 1.360020399093628\n",
            "Training at Epoch 8/25, Batch 50, Cost 1.6057142147561627, Acc 0.692. Time 1.3543052673339844\n",
            "Training at Epoch 8/25, Batch 55, Cost 1.6068876712527425, Acc 0.73. Time 1.3525550365447998\n",
            "Training at Epoch 8/25, Batch 60, Cost 1.5849435134979089, Acc 0.754. Time 1.3619239330291748\n",
            "Training at Epoch 8/25, Batch 65, Cost 1.5962673652486636, Acc 0.736. Time 1.3507781028747559\n",
            "Training at Epoch 8/25, Batch 70, Cost 1.5908620734549506, Acc 0.708. Time 1.3703410625457764\n",
            "Training at Epoch 8/25, Batch 75, Cost 1.578986176010579, Acc 0.74. Time 1.3533737659454346\n",
            "Training at Epoch 8/25, Batch 80, Cost 1.5932371669316727, Acc 0.734. Time 1.3608734607696533\n",
            "Training at Epoch 8/25, Batch 85, Cost 1.591006577838578, Acc 0.7. Time 1.351546049118042\n",
            "Training at Epoch 8/25, Batch 90, Cost 1.5882076077305232, Acc 0.706. Time 1.3562538623809814\n",
            "Training at Epoch 8/25, Batch 95, Cost 1.5948980525083145, Acc 0.688. Time 1.3700714111328125\n",
            "Training at Epoch 8/25, Batch 100, Cost 1.5758426140021031, Acc 0.706. Time 1.3547916412353516\n",
            "Training at Epoch 8/25, Batch 105, Cost 1.5998025223315029, Acc 0.71. Time 1.3686983585357666\n",
            "Training at Epoch 8/25, Batch 110, Cost 1.5802182154383022, Acc 0.762. Time 1.356523036956787\n",
            "Training at Epoch 8/25, Batch 115, Cost 1.5935357583395948, Acc 0.7. Time 1.3524298667907715\n",
            "Training at Epoch 8/25, Batch 120, Cost 1.5816951788847102, Acc 0.716. Time 1.3550760746002197\n",
            "Testing at Epoch 8/25, Batch 5, Cost 1.5817309175415293, Acc 0.732. Time 0.34693217277526855\n",
            "Testing at Epoch 8/25, Batch 10, Cost 1.5861561888346234, Acc 0.694. Time 0.34064197540283203\n",
            "Testing at Epoch 8/25, Batch 15, Cost 1.5979468538947978, Acc 0.682. Time 0.34136414527893066\n",
            "Testing at Epoch 8/25, Batch 20, Cost 1.578066674237829, Acc 0.72. Time 0.34247779846191406\n",
            "......\n",
            "Epoch 8/25, Train: Cost 1.5886427422161835, Acc 0.7179166666666668\n",
            "Epoch 8/25, Test: Cost 1.580371069161019, Acc 0.72. Time 182.8903431892395\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 9/25, Train batches 120, Test batches 20......\n",
            "Training at Epoch 9/25, Batch 5, Cost 1.595093494945225, Acc 0.72. Time 1.3600950241088867\n",
            "Training at Epoch 9/25, Batch 10, Cost 1.587117451312383, Acc 0.728. Time 1.3570902347564697\n",
            "Training at Epoch 9/25, Batch 15, Cost 1.567967553475207, Acc 0.716. Time 1.3552653789520264\n",
            "Training at Epoch 9/25, Batch 20, Cost 1.5895103937418584, Acc 0.71. Time 1.3576023578643799\n",
            "Training at Epoch 9/25, Batch 25, Cost 1.5769749204850458, Acc 0.722. Time 1.3559072017669678\n",
            "Training at Epoch 9/25, Batch 30, Cost 1.5705005747492518, Acc 0.768. Time 1.3637456893920898\n",
            "Training at Epoch 9/25, Batch 35, Cost 1.579792643371663, Acc 0.708. Time 1.3568036556243896\n",
            "Training at Epoch 9/25, Batch 40, Cost 1.586283421794292, Acc 0.752. Time 1.3658058643341064\n",
            "Training at Epoch 9/25, Batch 45, Cost 1.576726918300526, Acc 0.714. Time 1.3575191497802734\n",
            "Training at Epoch 9/25, Batch 50, Cost 1.5796180399171926, Acc 0.704. Time 1.35481858253479\n",
            "Training at Epoch 9/25, Batch 55, Cost 1.580001487777187, Acc 0.738. Time 1.3548378944396973\n",
            "Training at Epoch 9/25, Batch 60, Cost 1.5761025905464898, Acc 0.74. Time 1.3591558933258057\n",
            "Training at Epoch 9/25, Batch 65, Cost 1.5864726148127792, Acc 0.73. Time 1.3557868003845215\n",
            "Training at Epoch 9/25, Batch 70, Cost 1.5868195766776492, Acc 0.712. Time 1.3547861576080322\n",
            "Training at Epoch 9/25, Batch 75, Cost 1.5977096226267973, Acc 0.7. Time 1.356863260269165\n",
            "Training at Epoch 9/25, Batch 80, Cost 1.5947978272463539, Acc 0.684. Time 1.354956865310669\n",
            "Training at Epoch 9/25, Batch 85, Cost 1.5812138679757681, Acc 0.742. Time 1.3684139251708984\n",
            "Training at Epoch 9/25, Batch 90, Cost 1.5805422006083916, Acc 0.712. Time 1.3597421646118164\n",
            "Training at Epoch 9/25, Batch 95, Cost 1.5610908259819685, Acc 0.722. Time 1.358830213546753\n",
            "Training at Epoch 9/25, Batch 100, Cost 1.578274266867878, Acc 0.718. Time 1.3537273406982422\n",
            "Training at Epoch 9/25, Batch 105, Cost 1.5674072519638622, Acc 0.712. Time 1.352898359298706\n",
            "Training at Epoch 9/25, Batch 110, Cost 1.58162996256899, Acc 0.724. Time 1.3556511402130127\n",
            "Training at Epoch 9/25, Batch 115, Cost 1.5716224271948764, Acc 0.73. Time 1.3549280166625977\n",
            "Training at Epoch 9/25, Batch 120, Cost 1.5720074326365157, Acc 0.742. Time 1.3554692268371582\n",
            "Testing at Epoch 9/25, Batch 5, Cost 1.5988891217977124, Acc 0.694. Time 0.33817434310913086\n",
            "Testing at Epoch 9/25, Batch 10, Cost 1.5856208507342282, Acc 0.708. Time 0.3381013870239258\n",
            "Testing at Epoch 9/25, Batch 15, Cost 1.5687816436059465, Acc 0.75. Time 0.340853214263916\n",
            "Testing at Epoch 9/25, Batch 20, Cost 1.577384179086071, Acc 0.752. Time 0.34105491638183594\n",
            "......\n",
            "Epoch 9/25, Train: Cost 1.5836434306773262, Acc 0.7173999999999999\n",
            "Epoch 9/25, Test: Cost 1.5779045503625793, Acc 0.752. Time 182.80246782302856\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 10/25, Train batches 120, Test batches 20......\n",
            "Training at Epoch 10/25, Batch 5, Cost 1.5614782635530289, Acc 0.726. Time 1.3528835773468018\n",
            "Training at Epoch 10/25, Batch 10, Cost 1.5821908113781713, Acc 0.728. Time 1.3571655750274658\n",
            "Training at Epoch 10/25, Batch 15, Cost 1.574996026313599, Acc 0.718. Time 1.3537864685058594\n",
            "Training at Epoch 10/25, Batch 20, Cost 1.5826037854727313, Acc 0.716. Time 1.3654241561889648\n",
            "Training at Epoch 10/25, Batch 25, Cost 1.5849258517987816, Acc 0.736. Time 1.3544213771820068\n",
            "Training at Epoch 10/25, Batch 30, Cost 1.5737186688332427, Acc 0.764. Time 1.3727083206176758\n",
            "Training at Epoch 10/25, Batch 35, Cost 1.5659256061073568, Acc 0.726. Time 1.363328218460083\n",
            "Training at Epoch 10/25, Batch 40, Cost 1.5764654782236558, Acc 0.714. Time 1.3556721210479736\n",
            "Training at Epoch 10/25, Batch 45, Cost 1.5791784220130614, Acc 0.75. Time 1.3538830280303955\n",
            "Training at Epoch 10/25, Batch 50, Cost 1.572048046996583, Acc 0.744. Time 1.354231357574463\n",
            "Training at Epoch 10/25, Batch 55, Cost 1.5532151581860576, Acc 0.716. Time 1.3603579998016357\n",
            "Training at Epoch 10/25, Batch 60, Cost 1.5761692699174135, Acc 0.736. Time 1.3523340225219727\n",
            "Training at Epoch 10/25, Batch 65, Cost 1.5801949916879385, Acc 0.736. Time 1.3681612014770508\n",
            "Training at Epoch 10/25, Batch 70, Cost 1.5752048024860605, Acc 0.732. Time 1.3555412292480469\n",
            "Training at Epoch 10/25, Batch 75, Cost 1.5661049301182146, Acc 0.726. Time 1.3597650527954102\n",
            "Training at Epoch 10/25, Batch 80, Cost 1.5803706552694077, Acc 0.702. Time 1.3555371761322021\n",
            "Training at Epoch 10/25, Batch 85, Cost 1.5723642697427898, Acc 0.728. Time 1.35567045211792\n",
            "Training at Epoch 10/25, Batch 90, Cost 1.5917364061228074, Acc 0.684. Time 1.3588719367980957\n",
            "Training at Epoch 10/25, Batch 95, Cost 1.583358313592962, Acc 0.734. Time 1.3591840267181396\n",
            "Training at Epoch 10/25, Batch 100, Cost 1.5737064234236315, Acc 0.722. Time 1.3670427799224854\n",
            "Training at Epoch 10/25, Batch 105, Cost 1.59818571512871, Acc 0.728. Time 1.3555691242218018\n",
            "Training at Epoch 10/25, Batch 110, Cost 1.5826092235065123, Acc 0.728. Time 1.369227409362793\n",
            "Training at Epoch 10/25, Batch 115, Cost 1.56783315452267, Acc 0.738. Time 1.3646268844604492\n",
            "Training at Epoch 10/25, Batch 120, Cost 1.5824532082282636, Acc 0.742. Time 1.3536343574523926\n",
            "Testing at Epoch 10/25, Batch 5, Cost 1.5847523691876029, Acc 0.728. Time 0.3416132926940918\n",
            "Testing at Epoch 10/25, Batch 10, Cost 1.5887553965456038, Acc 0.726. Time 0.3427093029022217\n",
            "Testing at Epoch 10/25, Batch 15, Cost 1.5588710681546971, Acc 0.712. Time 0.338423490524292\n",
            "Testing at Epoch 10/25, Batch 20, Cost 1.5707438551953568, Acc 0.71. Time 0.34516143798828125\n",
            "......\n",
            "Epoch 10/25, Train: Cost 1.5806519388562066, Acc 0.7192333333333333\n",
            "Epoch 10/25, Test: Cost 1.575997148414023, Acc 0.71. Time 182.87071537971497\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 11/25, Train batches 120, Test batches 20......\n",
            "Training at Epoch 11/25, Batch 5, Cost 1.5569057698509225, Acc 0.75. Time 1.352649211883545\n",
            "Training at Epoch 11/25, Batch 10, Cost 1.5832876746389166, Acc 0.69. Time 1.3702704906463623\n",
            "Training at Epoch 11/25, Batch 15, Cost 1.5651160602648175, Acc 0.744. Time 1.3544917106628418\n",
            "Training at Epoch 11/25, Batch 20, Cost 1.5735053503611132, Acc 0.732. Time 1.359076738357544\n",
            "Training at Epoch 11/25, Batch 25, Cost 1.569105288274116, Acc 0.716. Time 1.3548641204833984\n",
            "Training at Epoch 11/25, Batch 30, Cost 1.5827032636584624, Acc 0.732. Time 1.350306510925293\n",
            "Training at Epoch 11/25, Batch 35, Cost 1.5971185319118555, Acc 0.714. Time 1.363919973373413\n",
            "Training at Epoch 11/25, Batch 40, Cost 1.5809944789724013, Acc 0.718. Time 1.3455557823181152\n",
            "Training at Epoch 11/25, Batch 45, Cost 1.569986965871484, Acc 0.754. Time 1.3596818447113037\n",
            "Training at Epoch 11/25, Batch 50, Cost 1.5896565438571082, Acc 0.712. Time 1.3459231853485107\n",
            "Training at Epoch 11/25, Batch 55, Cost 1.563929150925107, Acc 0.732. Time 1.3480021953582764\n",
            "Training at Epoch 11/25, Batch 60, Cost 1.5601394793475174, Acc 0.738. Time 1.3488388061523438\n",
            "Training at Epoch 11/25, Batch 65, Cost 1.5761530999470452, Acc 0.742. Time 1.3500888347625732\n",
            "Training at Epoch 11/25, Batch 70, Cost 1.5688578196454794, Acc 0.772. Time 1.3512182235717773\n",
            "Training at Epoch 11/25, Batch 75, Cost 1.5717690414806862, Acc 0.706. Time 1.347888469696045\n",
            "Training at Epoch 11/25, Batch 80, Cost 1.5993083440161577, Acc 0.676. Time 1.3616080284118652\n",
            "Training at Epoch 11/25, Batch 85, Cost 1.601566113857, Acc 0.692. Time 1.3480644226074219\n",
            "Training at Epoch 11/25, Batch 90, Cost 1.563877930724964, Acc 0.744. Time 1.3470556735992432\n",
            "Training at Epoch 11/25, Batch 95, Cost 1.5882395694547025, Acc 0.69. Time 1.3452811241149902\n",
            "Training at Epoch 11/25, Batch 100, Cost 1.577761524880019, Acc 0.73. Time 1.3442134857177734\n",
            "Training at Epoch 11/25, Batch 105, Cost 1.5896583374155, Acc 0.702. Time 1.3439314365386963\n",
            "Training at Epoch 11/25, Batch 110, Cost 1.5754129452213046, Acc 0.73. Time 1.347806692123413\n",
            "Training at Epoch 11/25, Batch 115, Cost 1.5653621596085094, Acc 0.712. Time 1.3568685054779053\n",
            "Training at Epoch 11/25, Batch 120, Cost 1.583776170597165, Acc 0.732. Time 1.3475074768066406\n",
            "Testing at Epoch 11/25, Batch 5, Cost 1.579318056424752, Acc 0.736. Time 0.3398287296295166\n",
            "Testing at Epoch 11/25, Batch 10, Cost 1.574964901989854, Acc 0.732. Time 0.34006714820861816\n",
            "Testing at Epoch 11/25, Batch 15, Cost 1.5777442707958687, Acc 0.726. Time 0.3405942916870117\n",
            "Testing at Epoch 11/25, Batch 20, Cost 1.5926138168881376, Acc 0.692. Time 0.3413527011871338\n",
            "......\n",
            "Epoch 11/25, Train: Cost 1.5773762427077933, Acc 0.7206833333333333\n",
            "Epoch 11/25, Test: Cost 1.5703799226624184, Acc 0.692. Time 182.15654873847961\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 12/25, Train batches 120, Test batches 20......\n",
            "Training at Epoch 12/25, Batch 5, Cost 1.5607275437820958, Acc 0.72. Time 1.3517873287200928\n",
            "Training at Epoch 12/25, Batch 10, Cost 1.5819394399048006, Acc 0.696. Time 1.3567919731140137\n",
            "Training at Epoch 12/25, Batch 15, Cost 1.5846052920661855, Acc 0.684. Time 1.3731112480163574\n",
            "Training at Epoch 12/25, Batch 20, Cost 1.5741810184677965, Acc 0.71. Time 1.3510246276855469\n",
            "Training at Epoch 12/25, Batch 25, Cost 1.5683706013495964, Acc 0.754. Time 1.3576080799102783\n",
            "Training at Epoch 12/25, Batch 30, Cost 1.5816464257539928, Acc 0.746. Time 1.3437995910644531\n",
            "Training at Epoch 12/25, Batch 35, Cost 1.589510104959364, Acc 0.692. Time 1.3445212841033936\n",
            "Training at Epoch 12/25, Batch 40, Cost 1.5806476549767938, Acc 0.696. Time 1.3513338565826416\n",
            "Training at Epoch 12/25, Batch 45, Cost 1.5857144529581257, Acc 0.716. Time 1.3507702350616455\n",
            "Training at Epoch 12/25, Batch 50, Cost 1.572202524131755, Acc 0.7. Time 1.364211082458496\n",
            "Training at Epoch 12/25, Batch 55, Cost 1.5602958095215775, Acc 0.732. Time 1.3528351783752441\n",
            "Training at Epoch 12/25, Batch 60, Cost 1.5840514883296493, Acc 0.736. Time 1.371671199798584\n",
            "Training at Epoch 12/25, Batch 65, Cost 1.5650320087077663, Acc 0.752. Time 1.354062557220459\n",
            "Training at Epoch 12/25, Batch 70, Cost 1.5755602646332456, Acc 0.698. Time 1.3517727851867676\n",
            "Training at Epoch 12/25, Batch 75, Cost 1.5583214549365731, Acc 0.716. Time 1.3544490337371826\n",
            "Training at Epoch 12/25, Batch 80, Cost 1.5709078575625528, Acc 0.716. Time 1.3542497158050537\n",
            "Training at Epoch 12/25, Batch 85, Cost 1.560742836782126, Acc 0.738. Time 1.3640313148498535\n",
            "Training at Epoch 12/25, Batch 90, Cost 1.582030699848386, Acc 0.71. Time 1.352428674697876\n",
            "Training at Epoch 12/25, Batch 95, Cost 1.5463464220890684, Acc 0.75. Time 1.3640687465667725\n",
            "Training at Epoch 12/25, Batch 100, Cost 1.5662015106606637, Acc 0.726. Time 1.368271827697754\n",
            "Training at Epoch 12/25, Batch 105, Cost 1.5712960259917583, Acc 0.724. Time 1.353522777557373\n",
            "Training at Epoch 12/25, Batch 110, Cost 1.5521700989940888, Acc 0.76. Time 1.3534188270568848\n",
            "Training at Epoch 12/25, Batch 115, Cost 1.5685713889176471, Acc 0.71. Time 1.3540682792663574\n",
            "Training at Epoch 12/25, Batch 120, Cost 1.5672776817180585, Acc 0.72. Time 1.3705918788909912\n",
            "Testing at Epoch 12/25, Batch 5, Cost 1.5743878070740345, Acc 0.708. Time 0.34097933769226074\n",
            "Testing at Epoch 12/25, Batch 10, Cost 1.5539292645820708, Acc 0.736. Time 0.34098386764526367\n",
            "Testing at Epoch 12/25, Batch 15, Cost 1.5652787722624946, Acc 0.714. Time 0.34325551986694336\n",
            "Testing at Epoch 12/25, Batch 20, Cost 1.5643605947784769, Acc 0.73. Time 0.34003758430480957\n",
            "......\n",
            "Epoch 12/25, Train: Cost 1.574374834293837, Acc 0.7204833333333334\n",
            "Epoch 12/25, Test: Cost 1.5712978286558485, Acc 0.73. Time 182.56208896636963\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 13/25, Train batches 120, Test batches 20......\n",
            "Training at Epoch 13/25, Batch 5, Cost 1.574159005975504, Acc 0.712. Time 1.352457046508789\n",
            "Training at Epoch 13/25, Batch 10, Cost 1.5594571171494487, Acc 0.744. Time 1.3467092514038086\n",
            "Training at Epoch 13/25, Batch 15, Cost 1.5671760411632898, Acc 0.722. Time 1.3564996719360352\n",
            "Training at Epoch 13/25, Batch 20, Cost 1.5714038588494519, Acc 0.704. Time 1.3554065227508545\n",
            "Training at Epoch 13/25, Batch 25, Cost 1.5829934441812028, Acc 0.684. Time 1.3526151180267334\n",
            "Training at Epoch 13/25, Batch 30, Cost 1.5705065102085853, Acc 0.71. Time 1.3655967712402344\n",
            "Training at Epoch 13/25, Batch 35, Cost 1.5735488223902898, Acc 0.74. Time 1.3445384502410889\n",
            "Training at Epoch 13/25, Batch 40, Cost 1.5657163498766848, Acc 0.702. Time 1.351494550704956\n",
            "Training at Epoch 13/25, Batch 45, Cost 1.5607210675921457, Acc 0.742. Time 1.350344181060791\n",
            "Training at Epoch 13/25, Batch 50, Cost 1.5896222819363879, Acc 0.708. Time 1.3532474040985107\n",
            "Training at Epoch 13/25, Batch 55, Cost 1.582553661079178, Acc 0.706. Time 1.351844072341919\n",
            "Training at Epoch 13/25, Batch 60, Cost 1.5710022619139832, Acc 0.746. Time 1.3539683818817139\n",
            "Training at Epoch 13/25, Batch 65, Cost 1.5852561922654385, Acc 0.698. Time 1.371286392211914\n",
            "Training at Epoch 13/25, Batch 70, Cost 1.565934450109976, Acc 0.72. Time 1.354330062866211\n",
            "Training at Epoch 13/25, Batch 75, Cost 1.5748469523027113, Acc 0.718. Time 1.3552401065826416\n",
            "Training at Epoch 13/25, Batch 80, Cost 1.570993704379384, Acc 0.722. Time 1.352036476135254\n",
            "Training at Epoch 13/25, Batch 85, Cost 1.5864053607689284, Acc 0.726. Time 1.3509783744812012\n",
            "Training at Epoch 13/25, Batch 90, Cost 1.565608088795829, Acc 0.75. Time 1.3533411026000977\n",
            "Training at Epoch 13/25, Batch 95, Cost 1.5625468427999682, Acc 0.738. Time 1.3523542881011963\n",
            "Training at Epoch 13/25, Batch 100, Cost 1.5815163292410241, Acc 0.704. Time 1.369208812713623\n",
            "Training at Epoch 13/25, Batch 105, Cost 1.588707902398396, Acc 0.718. Time 1.350830316543579\n",
            "Training at Epoch 13/25, Batch 110, Cost 1.5988932681127899, Acc 0.672. Time 1.3599376678466797\n",
            "Training at Epoch 13/25, Batch 115, Cost 1.5759593937535223, Acc 0.708. Time 1.3527965545654297\n",
            "Training at Epoch 13/25, Batch 120, Cost 1.5815204628060713, Acc 0.77. Time 1.352064847946167\n",
            "Testing at Epoch 13/25, Batch 5, Cost 1.5746030867948184, Acc 0.72. Time 0.33957529067993164\n",
            "Testing at Epoch 13/25, Batch 10, Cost 1.5626409297851858, Acc 0.728. Time 0.34235453605651855\n",
            "Testing at Epoch 13/25, Batch 15, Cost 1.5813908848163951, Acc 0.738. Time 0.3428173065185547\n",
            "Testing at Epoch 13/25, Batch 20, Cost 1.554491193720613, Acc 0.69. Time 0.343733549118042\n",
            "......\n",
            "Epoch 13/25, Train: Cost 1.5728129777133897, Acc 0.7210333333333333\n",
            "Epoch 13/25, Test: Cost 1.5665365528713113, Acc 0.69. Time 182.39671325683594\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 14/25, Train batches 120, Test batches 20......\n",
            "Training at Epoch 14/25, Batch 5, Cost 1.5697742706891324, Acc 0.724. Time 1.346235990524292\n",
            "Training at Epoch 14/25, Batch 10, Cost 1.5590373442159868, Acc 0.718. Time 1.3645944595336914\n",
            "Training at Epoch 14/25, Batch 15, Cost 1.5700551147293829, Acc 0.724. Time 1.3457226753234863\n",
            "Training at Epoch 14/25, Batch 20, Cost 1.5726781603710989, Acc 0.754. Time 1.3515419960021973\n",
            "Training at Epoch 14/25, Batch 25, Cost 1.5526378116324109, Acc 0.778. Time 1.3481929302215576\n",
            "Training at Epoch 14/25, Batch 30, Cost 1.549651443640895, Acc 0.758. Time 1.3560314178466797\n",
            "Training at Epoch 14/25, Batch 35, Cost 1.5672795112561266, Acc 0.726. Time 1.3601558208465576\n",
            "Training at Epoch 14/25, Batch 40, Cost 1.5618633832576538, Acc 0.75. Time 1.3527467250823975\n",
            "Training at Epoch 14/25, Batch 45, Cost 1.5786307354793028, Acc 0.706. Time 1.3677685260772705\n",
            "Training at Epoch 14/25, Batch 50, Cost 1.5832424476010696, Acc 0.702. Time 1.3524487018585205\n",
            "Training at Epoch 14/25, Batch 55, Cost 1.553558954629894, Acc 0.738. Time 1.355931043624878\n",
            "Training at Epoch 14/25, Batch 60, Cost 1.578335918244034, Acc 0.732. Time 1.3513164520263672\n",
            "Training at Epoch 14/25, Batch 65, Cost 1.5647694055139607, Acc 0.72. Time 1.3550963401794434\n",
            "Training at Epoch 14/25, Batch 70, Cost 1.5726834545986366, Acc 0.736. Time 1.3662643432617188\n",
            "Training at Epoch 14/25, Batch 75, Cost 1.548274487372866, Acc 0.752. Time 1.35111665725708\n",
            "Training at Epoch 14/25, Batch 80, Cost 1.5531799338237544, Acc 0.74. Time 1.367048978805542\n",
            "Training at Epoch 14/25, Batch 85, Cost 1.569368187652696, Acc 0.772. Time 1.3549656867980957\n",
            "Training at Epoch 14/25, Batch 90, Cost 1.5758795561212524, Acc 0.674. Time 1.351668119430542\n",
            "Training at Epoch 14/25, Batch 95, Cost 1.5641474818757723, Acc 0.72. Time 1.3523387908935547\n",
            "Training at Epoch 14/25, Batch 100, Cost 1.5942295286961745, Acc 0.708. Time 1.3565728664398193\n",
            "Training at Epoch 14/25, Batch 105, Cost 1.5577011274907113, Acc 0.752. Time 1.3649511337280273\n",
            "Training at Epoch 14/25, Batch 110, Cost 1.5872786938366108, Acc 0.712. Time 1.3620326519012451\n",
            "Training at Epoch 14/25, Batch 115, Cost 1.5858031362132203, Acc 0.696. Time 1.3753573894500732\n",
            "Training at Epoch 14/25, Batch 120, Cost 1.5706036276068125, Acc 0.706. Time 1.3539259433746338\n",
            "Testing at Epoch 14/25, Batch 5, Cost 1.5800547145391506, Acc 0.69. Time 0.3398020267486572\n",
            "Testing at Epoch 14/25, Batch 10, Cost 1.56704670144259, Acc 0.698. Time 0.3525257110595703\n",
            "Testing at Epoch 14/25, Batch 15, Cost 1.535885352126986, Acc 0.742. Time 0.3474314212799072\n",
            "Testing at Epoch 14/25, Batch 20, Cost 1.5674840251248736, Acc 0.718. Time 0.34244203567504883\n",
            "......\n",
            "Epoch 14/25, Train: Cost 1.5707858465142375, Acc 0.7219833333333334\n",
            "Epoch 14/25, Test: Cost 1.5640490103258216, Acc 0.718. Time 182.8328902721405\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 15/25, Train batches 120, Test batches 20......\n",
            "Training at Epoch 15/25, Batch 5, Cost 1.5638666206431797, Acc 0.702. Time 1.3371009826660156\n",
            "Training at Epoch 15/25, Batch 10, Cost 1.5685868628208535, Acc 0.716. Time 1.3402702808380127\n",
            "Training at Epoch 15/25, Batch 15, Cost 1.5766545729585264, Acc 0.696. Time 1.3503971099853516\n",
            "Training at Epoch 15/25, Batch 20, Cost 1.5645016686373416, Acc 0.718. Time 1.338820219039917\n",
            "Training at Epoch 15/25, Batch 25, Cost 1.5836502952921407, Acc 0.744. Time 1.3602478504180908\n",
            "Training at Epoch 15/25, Batch 30, Cost 1.5816605721611396, Acc 0.724. Time 1.3376743793487549\n",
            "Training at Epoch 15/25, Batch 35, Cost 1.569922896119076, Acc 0.736. Time 1.3413708209991455\n",
            "Training at Epoch 15/25, Batch 40, Cost 1.5598317594249942, Acc 0.732. Time 1.3405184745788574\n",
            "Training at Epoch 15/25, Batch 45, Cost 1.5662167745249531, Acc 0.712. Time 1.3414020538330078\n",
            "Training at Epoch 15/25, Batch 50, Cost 1.5632307872999887, Acc 0.724. Time 1.3528738021850586\n",
            "Training at Epoch 15/25, Batch 55, Cost 1.5422543253325605, Acc 0.718. Time 1.3513593673706055\n",
            "Training at Epoch 15/25, Batch 60, Cost 1.5826921223792192, Acc 0.72. Time 1.35914945602417\n",
            "Training at Epoch 15/25, Batch 65, Cost 1.5643670394489495, Acc 0.718. Time 1.3444154262542725\n",
            "Training at Epoch 15/25, Batch 70, Cost 1.5626591364609295, Acc 0.756. Time 1.3565833568572998\n",
            "Training at Epoch 15/25, Batch 75, Cost 1.5642947814880988, Acc 0.724. Time 1.3392443656921387\n",
            "Training at Epoch 15/25, Batch 80, Cost 1.6081636197734943, Acc 0.686. Time 1.3379626274108887\n",
            "Training at Epoch 15/25, Batch 85, Cost 1.5537981571874366, Acc 0.728. Time 1.3410706520080566\n",
            "Training at Epoch 15/25, Batch 90, Cost 1.5645162809373825, Acc 0.72. Time 1.3406307697296143\n",
            "Training at Epoch 15/25, Batch 95, Cost 1.5654326352843073, Acc 0.716. Time 1.3515915870666504\n",
            "Training at Epoch 15/25, Batch 100, Cost 1.5573358948943485, Acc 0.732. Time 1.335329532623291\n",
            "Training at Epoch 15/25, Batch 105, Cost 1.5832962002370303, Acc 0.702. Time 1.3434631824493408\n",
            "Training at Epoch 15/25, Batch 110, Cost 1.5780954486586896, Acc 0.69. Time 1.3417885303497314\n",
            "Training at Epoch 15/25, Batch 115, Cost 1.5740481955356256, Acc 0.73. Time 1.3406901359558105\n",
            "Training at Epoch 15/25, Batch 120, Cost 1.5670106818444955, Acc 0.75. Time 1.3394701480865479\n",
            "Testing at Epoch 15/25, Batch 5, Cost 1.5869370877781117, Acc 0.706. Time 0.34211158752441406\n",
            "Testing at Epoch 15/25, Batch 10, Cost 1.5535602382603861, Acc 0.734. Time 0.3392913341522217\n",
            "Testing at Epoch 15/25, Batch 15, Cost 1.5613418577678804, Acc 0.726. Time 0.33814096450805664\n",
            "Testing at Epoch 15/25, Batch 20, Cost 1.5783882544847065, Acc 0.67. Time 0.33669328689575195\n",
            "......\n",
            "Epoch 15/25, Train: Cost 1.56889768283567, Acc 0.7224166666666666\n",
            "Epoch 15/25, Test: Cost 1.5636277221286232, Acc 0.67. Time 181.20694088935852\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 16/25, Train batches 120, Test batches 20......\n",
            "Training at Epoch 16/25, Batch 5, Cost 1.559468277519253, Acc 0.728. Time 1.366976022720337\n",
            "Training at Epoch 16/25, Batch 10, Cost 1.5700057585841216, Acc 0.732. Time 1.3552577495574951\n",
            "Training at Epoch 16/25, Batch 15, Cost 1.5716632410734321, Acc 0.75. Time 1.35347580909729\n",
            "Training at Epoch 16/25, Batch 20, Cost 1.5868305431514758, Acc 0.738. Time 1.3536913394927979\n",
            "Training at Epoch 16/25, Batch 25, Cost 1.5736372907642557, Acc 0.704. Time 1.3553767204284668\n",
            "Training at Epoch 16/25, Batch 30, Cost 1.5763058196945163, Acc 0.724. Time 1.3560917377471924\n",
            "Training at Epoch 16/25, Batch 35, Cost 1.5665470789613936, Acc 0.73. Time 1.3538775444030762\n",
            "Training at Epoch 16/25, Batch 40, Cost 1.5685236611052524, Acc 0.746. Time 1.3656117916107178\n",
            "Training at Epoch 16/25, Batch 45, Cost 1.5742990624094624, Acc 0.734. Time 1.3546264171600342\n",
            "Training at Epoch 16/25, Batch 50, Cost 1.5842834209178287, Acc 0.722. Time 1.354968786239624\n",
            "Training at Epoch 16/25, Batch 55, Cost 1.55850452723696, Acc 0.748. Time 1.3513693809509277\n",
            "Training at Epoch 16/25, Batch 60, Cost 1.5658413934869284, Acc 0.732. Time 1.3533446788787842\n",
            "Training at Epoch 16/25, Batch 65, Cost 1.5654258143795652, Acc 0.746. Time 1.3701441287994385\n",
            "Training at Epoch 16/25, Batch 70, Cost 1.5806989850140791, Acc 0.748. Time 1.350252389907837\n",
            "Training at Epoch 16/25, Batch 75, Cost 1.5650517107258641, Acc 0.72. Time 1.3667972087860107\n",
            "Training at Epoch 16/25, Batch 80, Cost 1.5731138304564125, Acc 0.718. Time 1.3547627925872803\n",
            "Training at Epoch 16/25, Batch 85, Cost 1.567062205806799, Acc 0.72. Time 1.3520150184631348\n",
            "Training at Epoch 16/25, Batch 90, Cost 1.5848182696916844, Acc 0.712. Time 1.3520159721374512\n",
            "Training at Epoch 16/25, Batch 95, Cost 1.5722043865161763, Acc 0.694. Time 1.3512918949127197\n",
            "Training at Epoch 16/25, Batch 100, Cost 1.576371233387623, Acc 0.728. Time 1.3648467063903809\n",
            "Training at Epoch 16/25, Batch 105, Cost 1.5705453755179573, Acc 0.698. Time 1.3507273197174072\n",
            "Training at Epoch 16/25, Batch 110, Cost 1.5560586553069926, Acc 0.748. Time 1.3649976253509521\n",
            "Training at Epoch 16/25, Batch 115, Cost 1.5724195902779996, Acc 0.734. Time 1.3522891998291016\n",
            "Training at Epoch 16/25, Batch 120, Cost 1.5625251037426775, Acc 0.742. Time 1.352632761001587\n",
            "Testing at Epoch 16/25, Batch 5, Cost 1.542366304381754, Acc 0.74. Time 0.34207868576049805\n",
            "Testing at Epoch 16/25, Batch 10, Cost 1.5650316018711818, Acc 0.694. Time 0.3412361145019531\n",
            "Testing at Epoch 16/25, Batch 15, Cost 1.546641272626138, Acc 0.754. Time 0.3421640396118164\n",
            "Testing at Epoch 16/25, Batch 20, Cost 1.5565660649383757, Acc 0.74. Time 0.34285950660705566\n",
            "......\n",
            "Epoch 16/25, Train: Cost 1.566734385376192, Acc 0.72255\n",
            "Epoch 16/25, Test: Cost 1.559220214480389, Acc 0.74. Time 182.39492392539978\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 17/25, Train batches 120, Test batches 20......\n",
            "Training at Epoch 17/25, Batch 5, Cost 1.55368264646719, Acc 0.73. Time 1.353663444519043\n",
            "Training at Epoch 17/25, Batch 10, Cost 1.5689331322804667, Acc 0.69. Time 1.3718922138214111\n",
            "Training at Epoch 17/25, Batch 15, Cost 1.563922101331691, Acc 0.718. Time 1.3511779308319092\n",
            "Training at Epoch 17/25, Batch 20, Cost 1.563274486092559, Acc 0.7. Time 1.3736169338226318\n",
            "Training at Epoch 17/25, Batch 25, Cost 1.5661523109072195, Acc 0.672. Time 1.351670503616333\n",
            "Training at Epoch 17/25, Batch 30, Cost 1.56811053107739, Acc 0.732. Time 1.362226963043213\n",
            "Training at Epoch 17/25, Batch 35, Cost 1.5450417537494137, Acc 0.734. Time 1.3528294563293457\n",
            "Training at Epoch 17/25, Batch 40, Cost 1.5664376194967031, Acc 0.738. Time 1.3385992050170898\n",
            "Training at Epoch 17/25, Batch 45, Cost 1.5635598569837548, Acc 0.73. Time 1.348003625869751\n",
            "Training at Epoch 17/25, Batch 50, Cost 1.570361306354649, Acc 0.708. Time 1.3487305641174316\n",
            "Training at Epoch 17/25, Batch 55, Cost 1.5786905546850312, Acc 0.746. Time 1.362797737121582\n",
            "Training at Epoch 17/25, Batch 60, Cost 1.5660355728883824, Acc 0.7. Time 1.3478832244873047\n",
            "Training at Epoch 17/25, Batch 65, Cost 1.582460997001485, Acc 0.712. Time 1.3608012199401855\n",
            "Training at Epoch 17/25, Batch 70, Cost 1.5691612473611527, Acc 0.72. Time 1.3499720096588135\n",
            "Training at Epoch 17/25, Batch 75, Cost 1.547869253001412, Acc 0.728. Time 1.3466391563415527\n",
            "Training at Epoch 17/25, Batch 80, Cost 1.5617930178763195, Acc 0.74. Time 1.3489298820495605\n",
            "Training at Epoch 17/25, Batch 85, Cost 1.5491002982718638, Acc 0.72. Time 1.3467400074005127\n",
            "Training at Epoch 17/25, Batch 90, Cost 1.5800328156102694, Acc 0.732. Time 1.3602185249328613\n",
            "Training at Epoch 17/25, Batch 95, Cost 1.5571146365718727, Acc 0.76. Time 1.3485937118530273\n",
            "Training at Epoch 17/25, Batch 100, Cost 1.560786744167791, Acc 0.746. Time 1.36423921585083\n",
            "Training at Epoch 17/25, Batch 105, Cost 1.5695633962003792, Acc 0.72. Time 1.3461246490478516\n",
            "Training at Epoch 17/25, Batch 110, Cost 1.5581565236135702, Acc 0.762. Time 1.3506321907043457\n",
            "Training at Epoch 17/25, Batch 115, Cost 1.5556879025923902, Acc 0.742. Time 1.3496522903442383\n",
            "Training at Epoch 17/25, Batch 120, Cost 1.5658688676394952, Acc 0.744. Time 1.3490629196166992\n",
            "Testing at Epoch 17/25, Batch 5, Cost 1.5719803082096155, Acc 0.722. Time 0.3419005870819092\n",
            "Testing at Epoch 17/25, Batch 10, Cost 1.5601983364561862, Acc 0.714. Time 0.3392951488494873\n",
            "Testing at Epoch 17/25, Batch 15, Cost 1.552450267956743, Acc 0.738. Time 0.34174323081970215\n",
            "Testing at Epoch 17/25, Batch 20, Cost 1.5570637668355998, Acc 0.728. Time 0.3452022075653076\n",
            "......\n",
            "Epoch 17/25, Train: Cost 1.5656761854660235, Acc 0.7223000000000002\n",
            "Epoch 17/25, Test: Cost 1.5612183111539997, Acc 0.728. Time 182.92565989494324\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 18/25, Train batches 120, Test batches 20......\n",
            "Training at Epoch 18/25, Batch 5, Cost 1.570622452418748, Acc 0.698. Time 1.3565163612365723\n",
            "Training at Epoch 18/25, Batch 10, Cost 1.5722394784925173, Acc 0.736. Time 1.3545334339141846\n",
            "Training at Epoch 18/25, Batch 15, Cost 1.5767589933367485, Acc 0.71. Time 1.3521904945373535\n",
            "Training at Epoch 18/25, Batch 20, Cost 1.5659666425470185, Acc 0.684. Time 1.3706271648406982\n",
            "Training at Epoch 18/25, Batch 25, Cost 1.5732371200319937, Acc 0.696. Time 1.3553166389465332\n",
            "Training at Epoch 18/25, Batch 30, Cost 1.5604464676489431, Acc 0.738. Time 1.3534910678863525\n",
            "Training at Epoch 18/25, Batch 35, Cost 1.5714002842919956, Acc 0.706. Time 1.3634355068206787\n",
            "Training at Epoch 18/25, Batch 40, Cost 1.5930183409565315, Acc 0.686. Time 1.3423199653625488\n",
            "Training at Epoch 18/25, Batch 45, Cost 1.5705872335473896, Acc 0.72. Time 1.345510721206665\n",
            "Training at Epoch 18/25, Batch 50, Cost 1.5737736098855193, Acc 0.734. Time 1.3398497104644775\n",
            "Training at Epoch 18/25, Batch 55, Cost 1.5513132609622395, Acc 0.732. Time 1.3397939205169678\n",
            "Training at Epoch 18/25, Batch 60, Cost 1.581113298805191, Acc 0.7. Time 1.3388845920562744\n",
            "Training at Epoch 18/25, Batch 65, Cost 1.5606509789021437, Acc 0.684. Time 1.3372728824615479\n",
            "Training at Epoch 18/25, Batch 70, Cost 1.559487147423901, Acc 0.71. Time 1.3517117500305176\n",
            "Training at Epoch 18/25, Batch 75, Cost 1.562964457940024, Acc 0.724. Time 1.3410570621490479\n",
            "Training at Epoch 18/25, Batch 80, Cost 1.5725486873901535, Acc 0.724. Time 1.3570361137390137\n",
            "Training at Epoch 18/25, Batch 85, Cost 1.5587291854743481, Acc 0.718. Time 1.3489577770233154\n",
            "Training at Epoch 18/25, Batch 90, Cost 1.5825557347748855, Acc 0.672. Time 1.3388526439666748\n",
            "Training at Epoch 18/25, Batch 95, Cost 1.553392197729374, Acc 0.712. Time 1.3524181842803955\n",
            "Training at Epoch 18/25, Batch 100, Cost 1.5634046951788154, Acc 0.748. Time 1.345942497253418\n",
            "Training at Epoch 18/25, Batch 105, Cost 1.5601642651191518, Acc 0.72. Time 1.3402888774871826\n",
            "Training at Epoch 18/25, Batch 110, Cost 1.5624303675009394, Acc 0.744. Time 1.3435695171356201\n",
            "Training at Epoch 18/25, Batch 115, Cost 1.5643958317996862, Acc 0.74. Time 1.3588180541992188\n",
            "Training at Epoch 18/25, Batch 120, Cost 1.5914497818960196, Acc 0.708. Time 1.3397307395935059\n",
            "Testing at Epoch 18/25, Batch 5, Cost 1.5546531688383742, Acc 0.73. Time 0.3385035991668701\n",
            "Testing at Epoch 18/25, Batch 10, Cost 1.5640375108691498, Acc 0.724. Time 0.3407444953918457\n",
            "Testing at Epoch 18/25, Batch 15, Cost 1.5644443707014002, Acc 0.724. Time 0.33762240409851074\n",
            "Testing at Epoch 18/25, Batch 20, Cost 1.574686579770514, Acc 0.686. Time 0.33783388137817383\n",
            "......\n",
            "Epoch 18/25, Train: Cost 1.5648056887927728, Acc 0.7228833333333334\n",
            "Epoch 18/25, Test: Cost 1.5566915863394244, Acc 0.686. Time 181.50317358970642\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 19/25, Train batches 120, Test batches 20......\n",
            "Training at Epoch 19/25, Batch 5, Cost 1.5641511526214071, Acc 0.724. Time 1.340904951095581\n",
            "Training at Epoch 19/25, Batch 10, Cost 1.577856448856831, Acc 0.72. Time 1.3416659832000732\n",
            "Training at Epoch 19/25, Batch 15, Cost 1.5509144807905635, Acc 0.732. Time 1.364264726638794\n",
            "Training at Epoch 19/25, Batch 20, Cost 1.5729998746920333, Acc 0.716. Time 1.3535716533660889\n",
            "Training at Epoch 19/25, Batch 25, Cost 1.5664765219674888, Acc 0.728. Time 1.3746116161346436\n",
            "Training at Epoch 19/25, Batch 30, Cost 1.5497540541437795, Acc 0.764. Time 1.3557264804840088\n",
            "Training at Epoch 19/25, Batch 35, Cost 1.5495602121136658, Acc 0.758. Time 1.3689920902252197\n",
            "Training at Epoch 19/25, Batch 40, Cost 1.5787098549166045, Acc 0.74. Time 1.349832534790039\n",
            "Training at Epoch 19/25, Batch 45, Cost 1.561169293117666, Acc 0.724. Time 1.3520259857177734\n",
            "Training at Epoch 19/25, Batch 50, Cost 1.5671603614614529, Acc 0.754. Time 1.3524353504180908\n",
            "Training at Epoch 19/25, Batch 55, Cost 1.5572242490199693, Acc 0.722. Time 1.3549907207489014\n",
            "Training at Epoch 19/25, Batch 60, Cost 1.5785703044483619, Acc 0.72. Time 1.3695528507232666\n",
            "Training at Epoch 19/25, Batch 65, Cost 1.5690777844583423, Acc 0.706. Time 1.3513128757476807\n",
            "Training at Epoch 19/25, Batch 70, Cost 1.5465986859512726, Acc 0.732. Time 1.3673696517944336\n",
            "Training at Epoch 19/25, Batch 75, Cost 1.5664921696242788, Acc 0.7. Time 1.3521344661712646\n",
            "Training at Epoch 19/25, Batch 80, Cost 1.586636634017796, Acc 0.682. Time 1.3548204898834229\n",
            "Training at Epoch 19/25, Batch 85, Cost 1.5645258614994055, Acc 0.696. Time 1.3517627716064453\n",
            "Training at Epoch 19/25, Batch 90, Cost 1.5752112239939469, Acc 0.726. Time 1.3514087200164795\n",
            "Training at Epoch 19/25, Batch 95, Cost 1.571325232090051, Acc 0.712. Time 1.3617217540740967\n",
            "Training at Epoch 19/25, Batch 100, Cost 1.5751485621743724, Acc 0.756. Time 1.3532984256744385\n",
            "Training at Epoch 19/25, Batch 105, Cost 1.5764759937934514, Acc 0.724. Time 1.3643128871917725\n",
            "Training at Epoch 19/25, Batch 110, Cost 1.5693592793605595, Acc 0.72. Time 1.3525867462158203\n",
            "Training at Epoch 19/25, Batch 115, Cost 1.5715952042050951, Acc 0.702. Time 1.3513176441192627\n",
            "Training at Epoch 19/25, Batch 120, Cost 1.5688088992727047, Acc 0.758. Time 1.3540096282958984\n",
            "Testing at Epoch 19/25, Batch 5, Cost 1.5490993908491246, Acc 0.722. Time 0.34537363052368164\n",
            "Testing at Epoch 19/25, Batch 10, Cost 1.5489437405285005, Acc 0.708. Time 0.3389556407928467\n",
            "Testing at Epoch 19/25, Batch 15, Cost 1.5910283380518697, Acc 0.704. Time 0.33994626998901367\n",
            "Testing at Epoch 19/25, Batch 20, Cost 1.5796206089709988, Acc 0.686. Time 0.34015679359436035\n",
            "......\n",
            "Epoch 19/25, Train: Cost 1.5632056978128772, Acc 0.7229833333333334\n",
            "Epoch 19/25, Test: Cost 1.5589279858104026, Acc 0.686. Time 182.39073014259338\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 20/25, Train batches 120, Test batches 20......\n",
            "Training at Epoch 20/25, Batch 5, Cost 1.5793738195917095, Acc 0.706. Time 1.3600194454193115\n",
            "Training at Epoch 20/25, Batch 10, Cost 1.559135446862403, Acc 0.724. Time 1.3484199047088623\n",
            "Training at Epoch 20/25, Batch 15, Cost 1.5451584956446214, Acc 0.738. Time 1.3429832458496094\n",
            "Training at Epoch 20/25, Batch 20, Cost 1.5697080191963741, Acc 0.736. Time 1.347998857498169\n",
            "Training at Epoch 20/25, Batch 25, Cost 1.5561839897311096, Acc 0.726. Time 1.3396179676055908\n",
            "Training at Epoch 20/25, Batch 30, Cost 1.5526523080002015, Acc 0.712. Time 1.3384251594543457\n",
            "Training at Epoch 20/25, Batch 35, Cost 1.5646838057912489, Acc 0.72. Time 1.3549726009368896\n",
            "Training at Epoch 20/25, Batch 40, Cost 1.574875462576699, Acc 0.696. Time 1.3529009819030762\n",
            "Training at Epoch 20/25, Batch 45, Cost 1.5662791494109038, Acc 0.722. Time 1.3401234149932861\n",
            "Training at Epoch 20/25, Batch 50, Cost 1.572859386242313, Acc 0.69. Time 1.3451862335205078\n",
            "Training at Epoch 20/25, Batch 55, Cost 1.568812914875527, Acc 0.728. Time 1.3422167301177979\n",
            "Training at Epoch 20/25, Batch 60, Cost 1.5480564705309021, Acc 0.74. Time 1.3373160362243652\n",
            "Training at Epoch 20/25, Batch 65, Cost 1.5527050540297007, Acc 0.744. Time 1.3422901630401611\n",
            "Training at Epoch 20/25, Batch 70, Cost 1.5493021590381355, Acc 0.758. Time 1.3406774997711182\n",
            "Training at Epoch 20/25, Batch 75, Cost 1.565973505602667, Acc 0.704. Time 1.3528344631195068\n",
            "Training at Epoch 20/25, Batch 80, Cost 1.5658728193127776, Acc 0.752. Time 1.3399734497070312\n",
            "Training at Epoch 20/25, Batch 85, Cost 1.5469178428624184, Acc 0.742. Time 1.3623003959655762\n",
            "Training at Epoch 20/25, Batch 90, Cost 1.5432173138908436, Acc 0.724. Time 1.341614007949829\n",
            "Training at Epoch 20/25, Batch 95, Cost 1.5555930762838204, Acc 0.722. Time 1.3420639038085938\n",
            "Training at Epoch 20/25, Batch 100, Cost 1.5493932897771316, Acc 0.728. Time 1.3358914852142334\n",
            "Training at Epoch 20/25, Batch 105, Cost 1.5420811717760678, Acc 0.77. Time 1.3404741287231445\n",
            "Training at Epoch 20/25, Batch 110, Cost 1.5575982382084377, Acc 0.732. Time 1.3424561023712158\n",
            "Training at Epoch 20/25, Batch 115, Cost 1.568733749657413, Acc 0.702. Time 1.338914394378662\n",
            "Training at Epoch 20/25, Batch 120, Cost 1.557297376989533, Acc 0.736. Time 1.3560993671417236\n",
            "Testing at Epoch 20/25, Batch 5, Cost 1.5703828376207718, Acc 0.708. Time 0.33912038803100586\n",
            "Testing at Epoch 20/25, Batch 10, Cost 1.5552734072628582, Acc 0.71. Time 0.3388078212738037\n",
            "Testing at Epoch 20/25, Batch 15, Cost 1.563508179006542, Acc 0.736. Time 0.3396618366241455\n",
            "Testing at Epoch 20/25, Batch 20, Cost 1.5539797821737473, Acc 0.726. Time 0.33721446990966797\n",
            "......\n",
            "Epoch 20/25, Train: Cost 1.5620575440666158, Acc 0.7239833333333333\n",
            "Epoch 20/25, Test: Cost 1.5585873821353338, Acc 0.726. Time 181.1173450946808\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 21/25, Train batches 120, Test batches 20......\n",
            "Training at Epoch 21/25, Batch 5, Cost 1.5706474569691327, Acc 0.718. Time 1.3519480228424072\n",
            "Training at Epoch 21/25, Batch 10, Cost 1.5605029515183109, Acc 0.746. Time 1.352856159210205\n",
            "Training at Epoch 21/25, Batch 15, Cost 1.5589665352491908, Acc 0.73. Time 1.3553414344787598\n",
            "Training at Epoch 21/25, Batch 20, Cost 1.5666325483564665, Acc 0.726. Time 1.3632848262786865\n",
            "Training at Epoch 21/25, Batch 25, Cost 1.5602581971996852, Acc 0.762. Time 1.354590892791748\n",
            "Training at Epoch 21/25, Batch 30, Cost 1.559919850485063, Acc 0.762. Time 1.3537263870239258\n",
            "Training at Epoch 21/25, Batch 35, Cost 1.5672171751812676, Acc 0.688. Time 1.354011058807373\n",
            "Training at Epoch 21/25, Batch 40, Cost 1.5658639183270233, Acc 0.74. Time 1.3694615364074707\n",
            "Training at Epoch 21/25, Batch 45, Cost 1.5805485242218418, Acc 0.718. Time 1.3519322872161865\n",
            "Training at Epoch 21/25, Batch 50, Cost 1.5706638343467079, Acc 0.68. Time 1.3529059886932373\n",
            "Training at Epoch 21/25, Batch 55, Cost 1.5722959104168317, Acc 0.684. Time 1.3511569499969482\n",
            "Training at Epoch 21/25, Batch 60, Cost 1.5630864587820694, Acc 0.71. Time 1.3621037006378174\n",
            "Training at Epoch 21/25, Batch 65, Cost 1.5505686014212539, Acc 0.734. Time 1.3556764125823975\n",
            "Training at Epoch 21/25, Batch 70, Cost 1.5526734743659285, Acc 0.748. Time 1.3530888557434082\n",
            "Training at Epoch 21/25, Batch 75, Cost 1.5650082104969687, Acc 0.708. Time 1.3623173236846924\n",
            "Training at Epoch 21/25, Batch 80, Cost 1.555104900156245, Acc 0.758. Time 1.3526184558868408\n",
            "Training at Epoch 21/25, Batch 85, Cost 1.5526632909418658, Acc 0.69. Time 1.3519773483276367\n",
            "Training at Epoch 21/25, Batch 90, Cost 1.5446963916567622, Acc 0.738. Time 1.351501703262329\n",
            "Training at Epoch 21/25, Batch 95, Cost 1.5567425651877158, Acc 0.706. Time 1.352172613143921\n",
            "Training at Epoch 21/25, Batch 100, Cost 1.5769482432657185, Acc 0.654. Time 1.3564765453338623\n",
            "Training at Epoch 21/25, Batch 105, Cost 1.5635773024262942, Acc 0.716. Time 1.3527233600616455\n",
            "Training at Epoch 21/25, Batch 110, Cost 1.5659465923150606, Acc 0.684. Time 1.3695459365844727\n",
            "Training at Epoch 21/25, Batch 115, Cost 1.5495742489248874, Acc 0.722. Time 1.3526501655578613\n",
            "Training at Epoch 21/25, Batch 120, Cost 1.5525798605625145, Acc 0.704. Time 1.3583424091339111\n",
            "Testing at Epoch 21/25, Batch 5, Cost 1.5468042744021466, Acc 0.708. Time 0.34174108505249023\n",
            "Testing at Epoch 21/25, Batch 10, Cost 1.5388019892678662, Acc 0.766. Time 0.34197568893432617\n",
            "Testing at Epoch 21/25, Batch 15, Cost 1.5472763331091246, Acc 0.73. Time 0.3394010066986084\n",
            "Testing at Epoch 21/25, Batch 20, Cost 1.5477358850116416, Acc 0.736. Time 0.3427548408508301\n",
            "......\n",
            "Epoch 21/25, Train: Cost 1.5607436910419379, Acc 0.7235666666666667\n",
            "Epoch 21/25, Test: Cost 1.5561333494191756, Acc 0.736. Time 182.47557258605957\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 22/25, Train batches 120, Test batches 20......\n",
            "Training at Epoch 22/25, Batch 5, Cost 1.5544594464965047, Acc 0.72. Time 1.3535308837890625\n",
            "Training at Epoch 22/25, Batch 10, Cost 1.5693319271181618, Acc 0.722. Time 1.3634960651397705\n",
            "Training at Epoch 22/25, Batch 15, Cost 1.5763037314658896, Acc 0.694. Time 1.3554198741912842\n",
            "Training at Epoch 22/25, Batch 20, Cost 1.5634732015203756, Acc 0.7. Time 1.3604962825775146\n",
            "Training at Epoch 22/25, Batch 25, Cost 1.5736915614609621, Acc 0.736. Time 1.3525724411010742\n",
            "Training at Epoch 22/25, Batch 30, Cost 1.5584347103093141, Acc 0.756. Time 1.3624393939971924\n",
            "Training at Epoch 22/25, Batch 35, Cost 1.5787092333235895, Acc 0.732. Time 1.3531320095062256\n",
            "Training at Epoch 22/25, Batch 40, Cost 1.5486849456162264, Acc 0.726. Time 1.353560209274292\n",
            "Training at Epoch 22/25, Batch 45, Cost 1.5491175239983166, Acc 0.742. Time 1.3653013706207275\n",
            "Training at Epoch 22/25, Batch 50, Cost 1.5397738932077603, Acc 0.716. Time 1.361504077911377\n",
            "Training at Epoch 22/25, Batch 55, Cost 1.5565489388042937, Acc 0.754. Time 1.3599858283996582\n",
            "Training at Epoch 22/25, Batch 60, Cost 1.534424944635391, Acc 0.75. Time 1.3516218662261963\n",
            "Training at Epoch 22/25, Batch 65, Cost 1.56875602905698, Acc 0.708. Time 1.3510322570800781\n",
            "Training at Epoch 22/25, Batch 70, Cost 1.5462762560336334, Acc 0.71. Time 1.349550724029541\n",
            "Training at Epoch 22/25, Batch 75, Cost 1.5746374412096014, Acc 0.702. Time 1.3517699241638184\n",
            "Training at Epoch 22/25, Batch 80, Cost 1.549319726532143, Acc 0.748. Time 1.3645098209381104\n",
            "Training at Epoch 22/25, Batch 85, Cost 1.5564722341942498, Acc 0.722. Time 1.3504137992858887\n",
            "Training at Epoch 22/25, Batch 90, Cost 1.5575681208362695, Acc 0.694. Time 1.3670845031738281\n",
            "Training at Epoch 22/25, Batch 95, Cost 1.5585864709858746, Acc 0.702. Time 1.3519129753112793\n",
            "Training at Epoch 22/25, Batch 100, Cost 1.5434323962997072, Acc 0.736. Time 1.352870225906372\n",
            "Training at Epoch 22/25, Batch 105, Cost 1.5734855299584096, Acc 0.736. Time 1.3496384620666504\n",
            "Training at Epoch 22/25, Batch 110, Cost 1.5442544908668054, Acc 0.734. Time 1.3528296947479248\n",
            "Training at Epoch 22/25, Batch 115, Cost 1.5627981391364, Acc 0.718. Time 1.35612154006958\n",
            "Training at Epoch 22/25, Batch 120, Cost 1.5662940136605934, Acc 0.694. Time 1.352863073348999\n",
            "Testing at Epoch 22/25, Batch 5, Cost 1.5721207139153093, Acc 0.72. Time 0.34073615074157715\n",
            "Testing at Epoch 22/25, Batch 10, Cost 1.5466472995304357, Acc 0.748. Time 0.34140825271606445\n",
            "Testing at Epoch 22/25, Batch 15, Cost 1.551649700022736, Acc 0.74. Time 0.3438720703125\n",
            "Testing at Epoch 22/25, Batch 20, Cost 1.5488142865609287, Acc 0.726. Time 0.3408925533294678\n",
            "......\n",
            "Epoch 22/25, Train: Cost 1.5594495321314592, Acc 0.7241166666666667\n",
            "Epoch 22/25, Test: Cost 1.5559025274144396, Acc 0.726. Time 182.66236877441406\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 23/25, Train batches 120, Test batches 20......\n",
            "Training at Epoch 23/25, Batch 5, Cost 1.5501168685807543, Acc 0.724. Time 1.3515350818634033\n",
            "Training at Epoch 23/25, Batch 10, Cost 1.5433127200892935, Acc 0.738. Time 1.3592400550842285\n",
            "Training at Epoch 23/25, Batch 15, Cost 1.5544971022334662, Acc 0.736. Time 1.3512816429138184\n",
            "Training at Epoch 23/25, Batch 20, Cost 1.5561646271652796, Acc 0.766. Time 1.3547310829162598\n",
            "Training at Epoch 23/25, Batch 25, Cost 1.567981609616076, Acc 0.714. Time 1.3630433082580566\n",
            "Training at Epoch 23/25, Batch 30, Cost 1.5758584062436993, Acc 0.704. Time 1.355604887008667\n",
            "Training at Epoch 23/25, Batch 35, Cost 1.5810428202614144, Acc 0.712. Time 1.355010986328125\n",
            "Training at Epoch 23/25, Batch 40, Cost 1.5413510024190948, Acc 0.72. Time 1.3529982566833496\n",
            "Training at Epoch 23/25, Batch 45, Cost 1.551087352889456, Acc 0.752. Time 1.3697521686553955\n",
            "Training at Epoch 23/25, Batch 50, Cost 1.569073681851307, Acc 0.724. Time 1.352367639541626\n",
            "Training at Epoch 23/25, Batch 55, Cost 1.537922714127587, Acc 0.7. Time 1.350196123123169\n",
            "Training at Epoch 23/25, Batch 60, Cost 1.545942488738078, Acc 0.718. Time 1.3506295680999756\n",
            "Training at Epoch 23/25, Batch 65, Cost 1.5674323444049751, Acc 0.746. Time 1.352400541305542\n",
            "Training at Epoch 23/25, Batch 70, Cost 1.5584582435255858, Acc 0.726. Time 1.3616917133331299\n",
            "Training at Epoch 23/25, Batch 75, Cost 1.5711706912960668, Acc 0.704. Time 1.3544986248016357\n",
            "Training at Epoch 23/25, Batch 80, Cost 1.5783066269044574, Acc 0.702. Time 1.369507074356079\n",
            "Training at Epoch 23/25, Batch 85, Cost 1.5601698752583377, Acc 0.734. Time 1.351414442062378\n",
            "Training at Epoch 23/25, Batch 90, Cost 1.544842385155861, Acc 0.694. Time 1.3518359661102295\n",
            "Training at Epoch 23/25, Batch 95, Cost 1.5571394739261197, Acc 0.736. Time 1.3538758754730225\n",
            "Training at Epoch 23/25, Batch 100, Cost 1.5536593772401714, Acc 0.696. Time 1.3536407947540283\n",
            "Training at Epoch 23/25, Batch 105, Cost 1.575750067149889, Acc 0.71. Time 1.3646633625030518\n",
            "Training at Epoch 23/25, Batch 110, Cost 1.5710234519830102, Acc 0.682. Time 1.3523776531219482\n",
            "Training at Epoch 23/25, Batch 115, Cost 1.545519526606166, Acc 0.72. Time 1.380356788635254\n",
            "Training at Epoch 23/25, Batch 120, Cost 1.5617704990196486, Acc 0.732. Time 1.3523404598236084\n",
            "Testing at Epoch 23/25, Batch 5, Cost 1.5501215078334802, Acc 0.73. Time 0.34179258346557617\n",
            "Testing at Epoch 23/25, Batch 10, Cost 1.5679264871933543, Acc 0.754. Time 0.34459877014160156\n",
            "Testing at Epoch 23/25, Batch 15, Cost 1.559482388570651, Acc 0.706. Time 0.342404842376709\n",
            "Testing at Epoch 23/25, Batch 20, Cost 1.5648291777454917, Acc 0.728. Time 0.34146642684936523\n",
            "......\n",
            "Epoch 23/25, Train: Cost 1.5584712372811818, Acc 0.7238\n",
            "Epoch 23/25, Test: Cost 1.555890940954581, Acc 0.728. Time 182.61421823501587\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 24/25, Train batches 120, Test batches 20......\n",
            "Training at Epoch 24/25, Batch 5, Cost 1.5374387035200339, Acc 0.748. Time 1.3548970222473145\n",
            "Training at Epoch 24/25, Batch 10, Cost 1.5401626891188809, Acc 0.702. Time 1.35465669631958\n",
            "Training at Epoch 24/25, Batch 15, Cost 1.5770044041362112, Acc 0.714. Time 1.3645458221435547\n",
            "Training at Epoch 24/25, Batch 20, Cost 1.5589161081816494, Acc 0.724. Time 1.3532183170318604\n",
            "Training at Epoch 24/25, Batch 25, Cost 1.5483856526047528, Acc 0.716. Time 1.3517515659332275\n",
            "Training at Epoch 24/25, Batch 30, Cost 1.546817177169691, Acc 0.738. Time 1.3547375202178955\n",
            "Training at Epoch 24/25, Batch 35, Cost 1.5473151421115283, Acc 0.724. Time 1.3517794609069824\n",
            "Training at Epoch 24/25, Batch 40, Cost 1.5467697132644873, Acc 0.708. Time 1.3541135787963867\n",
            "Training at Epoch 24/25, Batch 45, Cost 1.5452582363026792, Acc 0.718. Time 1.3637957572937012\n",
            "Training at Epoch 24/25, Batch 50, Cost 1.5523661534153004, Acc 0.738. Time 1.362468957901001\n",
            "Training at Epoch 24/25, Batch 55, Cost 1.5605132856672597, Acc 0.708. Time 1.3536796569824219\n",
            "Training at Epoch 24/25, Batch 60, Cost 1.5480521149596624, Acc 0.708. Time 1.3538823127746582\n",
            "Training at Epoch 24/25, Batch 65, Cost 1.5597755858157634, Acc 0.732. Time 1.3523876667022705\n",
            "Training at Epoch 24/25, Batch 70, Cost 1.5563313224440878, Acc 0.746. Time 1.3537087440490723\n",
            "Training at Epoch 24/25, Batch 75, Cost 1.545467789452488, Acc 0.766. Time 1.3526990413665771\n",
            "Training at Epoch 24/25, Batch 80, Cost 1.5703504500877736, Acc 0.726. Time 1.351607084274292\n",
            "Training at Epoch 24/25, Batch 85, Cost 1.5633871830411896, Acc 0.738. Time 1.3677279949188232\n",
            "Training at Epoch 24/25, Batch 90, Cost 1.556552949314752, Acc 0.72. Time 1.3517694473266602\n",
            "Training at Epoch 24/25, Batch 95, Cost 1.5616081308440057, Acc 0.742. Time 1.3529293537139893\n",
            "Training at Epoch 24/25, Batch 100, Cost 1.5668188748349416, Acc 0.734. Time 1.3492214679718018\n",
            "Training at Epoch 24/25, Batch 105, Cost 1.5502806250983021, Acc 0.75. Time 1.3516335487365723\n",
            "Training at Epoch 24/25, Batch 110, Cost 1.5666333911582078, Acc 0.734. Time 1.3550317287445068\n",
            "Training at Epoch 24/25, Batch 115, Cost 1.5429427217091454, Acc 0.746. Time 1.3534271717071533\n",
            "Training at Epoch 24/25, Batch 120, Cost 1.5675585566961805, Acc 0.722. Time 1.365365982055664\n",
            "Testing at Epoch 24/25, Batch 5, Cost 1.5448667674891872, Acc 0.726. Time 0.34569740295410156\n",
            "Testing at Epoch 24/25, Batch 10, Cost 1.5537467132499396, Acc 0.738. Time 0.34095287322998047\n",
            "Testing at Epoch 24/25, Batch 15, Cost 1.5520813787370447, Acc 0.734. Time 0.34030842781066895\n",
            "Testing at Epoch 24/25, Batch 20, Cost 1.549390780912503, Acc 0.72. Time 0.34082579612731934\n",
            "......\n",
            "Epoch 24/25, Train: Cost 1.558058552364193, Acc 0.72415\n",
            "Epoch 24/25, Test: Cost 1.5530848067570522, Acc 0.72. Time 182.49355912208557\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 25/25, Train batches 120, Test batches 20......\n",
            "Training at Epoch 25/25, Batch 5, Cost 1.5768038439168774, Acc 0.716. Time 1.3537168502807617\n",
            "Training at Epoch 25/25, Batch 10, Cost 1.5585393584402218, Acc 0.744. Time 1.3559720516204834\n",
            "Training at Epoch 25/25, Batch 15, Cost 1.5682569762256515, Acc 0.728. Time 1.3596806526184082\n",
            "Training at Epoch 25/25, Batch 20, Cost 1.5528598952794113, Acc 0.736. Time 1.3730945587158203\n",
            "Training at Epoch 25/25, Batch 25, Cost 1.5546963574886206, Acc 0.734. Time 1.35532808303833\n",
            "Training at Epoch 25/25, Batch 30, Cost 1.566477051336512, Acc 0.706. Time 1.3566720485687256\n",
            "Training at Epoch 25/25, Batch 35, Cost 1.5651671418501614, Acc 0.714. Time 1.3519840240478516\n",
            "Training at Epoch 25/25, Batch 40, Cost 1.5643291317147796, Acc 0.736. Time 1.3513269424438477\n",
            "Training at Epoch 25/25, Batch 45, Cost 1.556001280483324, Acc 0.686. Time 1.3566205501556396\n",
            "Training at Epoch 25/25, Batch 50, Cost 1.560421528786157, Acc 0.716. Time 1.3545472621917725\n",
            "Training at Epoch 25/25, Batch 55, Cost 1.5595837110900184, Acc 0.764. Time 1.3634569644927979\n",
            "Training at Epoch 25/25, Batch 60, Cost 1.5468733271444959, Acc 0.748. Time 1.3557977676391602\n",
            "Training at Epoch 25/25, Batch 65, Cost 1.5519308044053928, Acc 0.718. Time 1.352452039718628\n",
            "Training at Epoch 25/25, Batch 70, Cost 1.5677644448525445, Acc 0.722. Time 1.35386061668396\n",
            "Training at Epoch 25/25, Batch 75, Cost 1.5469519048472253, Acc 0.756. Time 1.3518011569976807\n",
            "Training at Epoch 25/25, Batch 80, Cost 1.5451886682850078, Acc 0.714. Time 1.3523633480072021\n",
            "Training at Epoch 25/25, Batch 85, Cost 1.5622564751476746, Acc 0.688. Time 1.351759910583496\n",
            "Training at Epoch 25/25, Batch 90, Cost 1.5456635503806284, Acc 0.718. Time 1.361006736755371\n",
            "Training at Epoch 25/25, Batch 95, Cost 1.5592034648456494, Acc 0.712. Time 1.3508954048156738\n",
            "Training at Epoch 25/25, Batch 100, Cost 1.555540311906227, Acc 0.714. Time 1.3521897792816162\n",
            "Training at Epoch 25/25, Batch 105, Cost 1.5714930338447015, Acc 0.71. Time 1.351607322692871\n",
            "Training at Epoch 25/25, Batch 110, Cost 1.5693139015394106, Acc 0.728. Time 1.3567085266113281\n",
            "Training at Epoch 25/25, Batch 115, Cost 1.5469368302900903, Acc 0.712. Time 1.3538739681243896\n",
            "Training at Epoch 25/25, Batch 120, Cost 1.5736291349301368, Acc 0.708. Time 1.358034610748291\n",
            "Testing at Epoch 25/25, Batch 5, Cost 1.568461234348578, Acc 0.708. Time 0.33966946601867676\n",
            "Testing at Epoch 25/25, Batch 10, Cost 1.5583268873410745, Acc 0.74. Time 0.3421196937561035\n",
            "Testing at Epoch 25/25, Batch 15, Cost 1.5572088698515312, Acc 0.704. Time 0.342437744140625\n",
            "Testing at Epoch 25/25, Batch 20, Cost 1.547067075711651, Acc 0.716. Time 0.3409128189086914\n",
            "......\n",
            "Epoch 25/25, Train: Cost 1.5567078401854246, Acc 0.72425\n",
            "Epoch 25/25, Test: Cost 1.553612062263762, Acc 0.716. Time 182.7016806602478\n",
            "=-==-==-==-==-==-==-==-==-==-=\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-cdab6e19e1eb>:14: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  results_df = pd.concat(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# aggregate dataframe\n",
        "df_agg = results_df.groupby([\"n_train\", \"step\"]).agg([\"mean\", \"std\"])\n",
        "df_agg = df_agg.reset_index()\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "colors = sns.color_palette()\n",
        "fig, axes = plt.subplots(ncols=2, figsize=(24, 7))\n",
        "\n",
        "generalization_errors = []\n",
        "\n",
        "# plot losses and accuracies\n",
        "for i, n_train in enumerate(train_sizes):\n",
        "    df = df_agg[df_agg.n_train == n_train]\n",
        "\n",
        "    dfs = [df.train_cost[\"mean\"], df.test_cost[\"mean\"], df.train_acc[\"mean\"], df.test_acc[\"mean\"]]\n",
        "    lines = [\"o-\", \"x--\", \"o-\", \"x--\"]\n",
        "    labels = [fr\"$N={n_train}$\", None, fr\"$N={n_train}$\", None]\n",
        "    axs = [0,0,1,1]\n",
        "\n",
        "    for k in range(4):\n",
        "        ax = axes[axs[k]]\n",
        "        ax.plot(df.step, dfs[k], lines[k], label=labels[k], markevery=10, color=colors[i], alpha=0.8)\n",
        "\n",
        "\n",
        "    # plot final loss difference\n",
        "    #dif = df[df.step == n_epochs].test_cost[\"mean\"] - df[df.step == n_epochs].train_cost[\"mean\"]\n",
        "    #generalization_errors.append(dif)\n",
        "\n",
        "# format loss plot\n",
        "ax = axes[0]\n",
        "ax.set_title('Train and Test Losses', fontsize=14)\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Loss')\n",
        "\n",
        "# format generalization error plot\n",
        "#ax = axes[1]\n",
        "#ax.plot(train_sizes, generalization_errors, \"o-\", label=r\"$gen(\\alpha)$\")\n",
        "#ax.set_xscale('log')\n",
        "#ax.set_xticks(train_sizes)\n",
        "#ax.set_xticklabels(train_sizes)\n",
        "#ax.set_title(r'Generalization Error $gen(\\alpha) = R(\\alpha) - \\hat{R}_N(\\alpha)$', fontsize=14)\n",
        "#ax.set_xlabel('Training Set Size')\n",
        "\n",
        "# format loss plot\n",
        "ax = axes[1]\n",
        "ax.set_title('Train and Test Accuracies', fontsize=14)\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Accuracy')\n",
        "ax.set_ylim(0.05, 1.05)\n",
        "\n",
        "legend_elements = [\n",
        "    mpl.lines.Line2D([0], [0], label=f'N={n}', color=colors[i]) for i, n in enumerate(train_sizes)\n",
        "    ] + [\n",
        "    mpl.lines.Line2D([0], [0], marker='o', ls='-', label='Train', color='Black'),\n",
        "    mpl.lines.Line2D([0], [0], marker='x', ls='--', label='Test', color='Black')\n",
        "    ]\n",
        "\n",
        "axes[0].legend(handles=legend_elements, ncol=3)\n",
        "axes[1].legend(handles=legend_elements, ncol=3)\n",
        "\n",
        "#axes[1].set_yscale('log', base=2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZZwqGAsnd4zT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "outputId": "9983683e-4760-4dd7-97c9-e92318484734"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2400x700 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_df\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "results_df.to_csv('beta_results_60k_25epoch.csv', index=False)\n",
        "\n",
        "# Download the CSV file\n",
        "files.download('beta_results_60k_25epoch.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "nUjJHqJtcgRq",
        "outputId": "d4006421-85be-463a-930c-ce3d05945554"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5679a49d-51c0-4df1-b180-9ce9df360e65\", \"beta_results_60k_25epoch.csv\", 1941)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "ldsXaCl-dwiJ",
        "outputId": "350f53f3-d519-425a-bf34-2eaf392d64a8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    train_acc  train_cost  test_acc  test_cost step n_train\n",
              "0    0.613150    1.758174    0.6773   1.666239    1   60000\n",
              "1    0.695533    1.647854    0.6955   1.628269    2   60000\n",
              "2    0.705983    1.624746    0.7055   1.610666    3   60000\n",
              "3    0.709200    1.613246    0.7104   1.605277    4   60000\n",
              "4    0.711950    1.605096    0.7146   1.601679    5   60000\n",
              "5    0.714200    1.597907    0.7163   1.587998    6   60000\n",
              "6    0.716483    1.591850    0.7153   1.585480    7   60000\n",
              "7    0.717917    1.588643    0.7193   1.580371    8   60000\n",
              "8    0.717400    1.583643    0.7193   1.577905    9   60000\n",
              "9    0.719233    1.580652    0.7193   1.575997   10   60000\n",
              "10   0.720683    1.577376    0.7221   1.570380   11   60000\n",
              "11   0.720483    1.574375    0.7183   1.571298   12   60000\n",
              "12   0.721033    1.572813    0.7221   1.566537   13   60000\n",
              "13   0.721983    1.570786    0.7236   1.564049   14   60000\n",
              "14   0.722417    1.568898    0.7227   1.563628   15   60000\n",
              "15   0.722550    1.566734    0.7259   1.559220   16   60000\n",
              "16   0.722300    1.565676    0.7232   1.561218   17   60000\n",
              "17   0.722883    1.564806    0.7267   1.556692   18   60000\n",
              "18   0.722983    1.563206    0.7255   1.558928   19   60000\n",
              "19   0.723983    1.562058    0.7259   1.558587   20   60000\n",
              "20   0.723567    1.560744    0.7276   1.556133   21   60000\n",
              "21   0.724117    1.559450    0.7258   1.555903   22   60000\n",
              "22   0.723800    1.558471    0.7242   1.555891   23   60000\n",
              "23   0.724150    1.558059    0.7260   1.553085   24   60000\n",
              "24   0.724250    1.556708    0.7266   1.553612   25   60000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6492fd0a-c280-422c-9de6-7d2f838587d1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_acc</th>\n",
              "      <th>train_cost</th>\n",
              "      <th>test_acc</th>\n",
              "      <th>test_cost</th>\n",
              "      <th>step</th>\n",
              "      <th>n_train</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.613150</td>\n",
              "      <td>1.758174</td>\n",
              "      <td>0.6773</td>\n",
              "      <td>1.666239</td>\n",
              "      <td>1</td>\n",
              "      <td>60000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.695533</td>\n",
              "      <td>1.647854</td>\n",
              "      <td>0.6955</td>\n",
              "      <td>1.628269</td>\n",
              "      <td>2</td>\n",
              "      <td>60000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.705983</td>\n",
              "      <td>1.624746</td>\n",
              "      <td>0.7055</td>\n",
              "      <td>1.610666</td>\n",
              "      <td>3</td>\n",
              "      <td>60000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.709200</td>\n",
              "      <td>1.613246</td>\n",
              "      <td>0.7104</td>\n",
              "      <td>1.605277</td>\n",
              "      <td>4</td>\n",
              "      <td>60000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.711950</td>\n",
              "      <td>1.605096</td>\n",
              "      <td>0.7146</td>\n",
              "      <td>1.601679</td>\n",
              "      <td>5</td>\n",
              "      <td>60000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.714200</td>\n",
              "      <td>1.597907</td>\n",
              "      <td>0.7163</td>\n",
              "      <td>1.587998</td>\n",
              "      <td>6</td>\n",
              "      <td>60000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.716483</td>\n",
              "      <td>1.591850</td>\n",
              "      <td>0.7153</td>\n",
              "      <td>1.585480</td>\n",
              "      <td>7</td>\n",
              "      <td>60000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.717917</td>\n",
              "      <td>1.588643</td>\n",
              "      <td>0.7193</td>\n",
              "      <td>1.580371</td>\n",
              "      <td>8</td>\n",
              "      <td>60000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.717400</td>\n",
              "      <td>1.583643</td>\n",
              "      <td>0.7193</td>\n",
              "      <td>1.577905</td>\n",
              "      <td>9</td>\n",
              "      <td>60000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.719233</td>\n",
              "      <td>1.580652</td>\n",
              "      <td>0.7193</td>\n",
              "      <td>1.575997</td>\n",
              "      <td>10</td>\n",
              "      <td>60000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.720683</td>\n",
              "      <td>1.577376</td>\n",
              "      <td>0.7221</td>\n",
              "      <td>1.570380</td>\n",
              "      <td>11</td>\n",
              "      <td>60000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.720483</td>\n",
              "      <td>1.574375</td>\n",
              "      <td>0.7183</td>\n",
              "      <td>1.571298</td>\n",
              "      <td>12</td>\n",
              "      <td>60000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.721033</td>\n",
              "      <td>1.572813</td>\n",
              "      <td>0.7221</td>\n",
              "      <td>1.566537</td>\n",
              "      <td>13</td>\n",
              "      <td>60000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.721983</td>\n",
              "      <td>1.570786</td>\n",
              "      <td>0.7236</td>\n",
              "      <td>1.564049</td>\n",
              "      <td>14</td>\n",
              "      <td>60000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.722417</td>\n",
              "      <td>1.568898</td>\n",
              "      <td>0.7227</td>\n",
              "      <td>1.563628</td>\n",
              "      <td>15</td>\n",
              "      <td>60000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.722550</td>\n",
              "      <td>1.566734</td>\n",
              "      <td>0.7259</td>\n",
              "      <td>1.559220</td>\n",
              "      <td>16</td>\n",
              "      <td>60000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.722300</td>\n",
              "      <td>1.565676</td>\n",
              "      <td>0.7232</td>\n",
              "      <td>1.561218</td>\n",
              "      <td>17</td>\n",
              "      <td>60000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.722883</td>\n",
              "      <td>1.564806</td>\n",
              "      <td>0.7267</td>\n",
              "      <td>1.556692</td>\n",
              "      <td>18</td>\n",
              "      <td>60000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.722983</td>\n",
              "      <td>1.563206</td>\n",
              "      <td>0.7255</td>\n",
              "      <td>1.558928</td>\n",
              "      <td>19</td>\n",
              "      <td>60000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.723983</td>\n",
              "      <td>1.562058</td>\n",
              "      <td>0.7259</td>\n",
              "      <td>1.558587</td>\n",
              "      <td>20</td>\n",
              "      <td>60000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.723567</td>\n",
              "      <td>1.560744</td>\n",
              "      <td>0.7276</td>\n",
              "      <td>1.556133</td>\n",
              "      <td>21</td>\n",
              "      <td>60000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.724117</td>\n",
              "      <td>1.559450</td>\n",
              "      <td>0.7258</td>\n",
              "      <td>1.555903</td>\n",
              "      <td>22</td>\n",
              "      <td>60000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.723800</td>\n",
              "      <td>1.558471</td>\n",
              "      <td>0.7242</td>\n",
              "      <td>1.555891</td>\n",
              "      <td>23</td>\n",
              "      <td>60000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.724150</td>\n",
              "      <td>1.558059</td>\n",
              "      <td>0.7260</td>\n",
              "      <td>1.553085</td>\n",
              "      <td>24</td>\n",
              "      <td>60000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.724250</td>\n",
              "      <td>1.556708</td>\n",
              "      <td>0.7266</td>\n",
              "      <td>1.553612</td>\n",
              "      <td>25</td>\n",
              "      <td>60000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6492fd0a-c280-422c-9de6-7d2f838587d1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6492fd0a-c280-422c-9de6-7d2f838587d1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6492fd0a-c280-422c-9de6-7d2f838587d1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e973ceac-39b6-4905-8065-02f98394c002\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e973ceac-39b6-4905-8065-02f98394c002')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e973ceac-39b6-4905-8065-02f98394c002 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_52bc5745-e5a8-4705-b298-201bddd57fe4\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_52bc5745-e5a8-4705-b298-201bddd57fe4 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "results_df",
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 25,\n  \"fields\": [\n    {\n      \"column\": \"train_acc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.022199631336611923,\n        \"min\": 0.61315,\n        \"max\": 0.72425,\n        \"num_unique_values\": 25,\n        \"samples\": [\n          0.7173999999999999,\n          0.7223000000000002,\n          0.61315\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_cost\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04245051408602689,\n        \"min\": 1.5567078401854246,\n        \"max\": 1.758174370389908,\n        \"num_unique_values\": 25,\n        \"samples\": [\n          1.5836434306773262,\n          1.5656761854660235,\n          1.758174370389908\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_acc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.011366397846283577,\n        \"min\": 0.6773,\n        \"max\": 0.7276,\n        \"num_unique_values\": 23,\n        \"samples\": [\n          0.7266999999999999,\n          0.7221,\n          0.6773\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_cost\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.027356344052727587,\n        \"min\": 1.5530848067570522,\n        \"max\": 1.6662390978223496,\n        \"num_unique_values\": 25,\n        \"samples\": [\n          1.5779045503625793,\n          1.5612183111539997,\n          1.6662390978223496\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"step\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 1,\n        \"max\": 25,\n        \"num_unique_values\": 25,\n        \"samples\": [\n          9,\n          17,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_train\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 60000,\n        \"max\": 60000,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          60000\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_folder = \"/content/drive/MyDrive/Research/HermtianEncodingImg/mnist\"\n",
        "results_df.to_csv(os.path.join(save_folder, \"mnist_HermImgReUpload_results.csv\"))\n",
        "df_agg.to_csv(os.path.join(save_folder, \"mnist_HermImgReUpload_results_agg.csv\"))\n",
        "# save the plot to file\n",
        "fig.savefig(os.path.join(save_folder, \"mnist_HermImgReUpload_results.png\"))"
      ],
      "metadata": {
        "id": "wr-ZnyKwd5dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kukubDx5CD26"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}