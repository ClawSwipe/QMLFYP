{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52CR7KmybQYr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f26175d6-f33b-444c-cca7-7ced4088e2f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pennylane\n",
            "  Downloading PennyLane-0.39.0-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting pennylane-lightning\n",
            "  Downloading PennyLane_Lightning-0.39.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (26 kB)\n",
            "Collecting cotengra\n",
            "  Downloading cotengra-0.6.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting quimb\n",
            "  Downloading quimb-1.8.4-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: numpy<2.1 in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pennylane) (3.4.2)\n",
            "Collecting rustworkx>=0.14.0 (from pennylane)\n",
            "  Downloading rustworkx-0.15.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.7.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.10.2)\n",
            "Collecting appdirs (from pennylane)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting autoray>=0.6.11 (from pennylane)\n",
            "  Downloading autoray-0.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from pennylane) (5.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pennylane) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pennylane) (4.12.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pennylane) (24.1)\n",
            "Collecting pennylane-lightning-gpu (from pennylane-lightning[gpu])\n",
            "  Downloading PennyLane_Lightning_GPU-0.39.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (26 kB)\n",
            "Collecting cytoolz>=0.8.0 (from quimb)\n",
            "  Downloading cytoolz-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: numba>=0.39 in /usr/local/lib/python3.10/dist-packages (from quimb) (0.60.0)\n",
            "Requirement already satisfied: psutil>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from quimb) (5.9.5)\n",
            "Requirement already satisfied: tqdm>=4 in /usr/local/lib/python3.10/dist-packages (from quimb) (4.66.6)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from cytoolz>=0.8.0->quimb) (0.12.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.39->quimb) (0.43.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2024.8.30)\n",
            "Downloading PennyLane-0.39.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PennyLane_Lightning-0.39.0-cp310-cp310-manylinux_2_28_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cotengra-0.6.2-py3-none-any.whl (177 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.8/177.8 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading quimb-1.8.4-py3-none-any.whl (541 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m541.6/541.6 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autoray-0.7.0-py3-none-any.whl (930 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m930.0/930.0 kB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cytoolz-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rustworkx-0.15.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading PennyLane_Lightning_GPU-0.39.0-cp310-cp310-manylinux_2_28_x86_64.whl (776 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.7/776.7 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: appdirs, rustworkx, cytoolz, autoray, cotengra, quimb, pennylane-lightning, pennylane, pennylane-lightning-gpu\n",
            "Successfully installed appdirs-1.4.4 autoray-0.7.0 cotengra-0.6.2 cytoolz-1.0.0 pennylane-0.39.0 pennylane-lightning-0.39.0 pennylane-lightning-gpu-0.39.0 quimb-1.8.4 rustworkx-0.15.1\n",
            "Looking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
            "Requirement already satisfied: jax[cuda12_pip] in /usr/local/lib/python3.10/dist-packages (0.4.33)\n",
            "Collecting jax[cuda12_pip]\n",
            "  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.35,>=0.4.34 (from jax[cuda12_pip])\n",
            "  Downloading jaxlib-0.4.35-cp310-cp310-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Requirement already satisfied: ml-dtypes>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from jax[cuda12_pip]) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from jax[cuda12_pip]) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax[cuda12_pip]) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax[cuda12_pip]) (1.13.1)\n",
            "  Downloading jaxlib-0.4.34-cp310-cp310-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax-cuda12-plugin<=0.4.35,>=0.4.34 (from jax-cuda12-plugin[with_cuda]<=0.4.35,>=0.4.34; extra == \"cuda12-pip\"->jax[cuda12_pip])\n",
            "  Downloading jax_cuda12_plugin-0.4.35-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax-cuda12-pjrt==0.4.35 (from jax-cuda12-plugin<=0.4.35,>=0.4.34->jax-cuda12-plugin[with_cuda]<=0.4.35,>=0.4.34; extra == \"cuda12-pip\"->jax[cuda12_pip])\n",
            "  Downloading jax_cuda12_pjrt-0.4.35-py3-none-manylinux2014_x86_64.whl.metadata (349 bytes)\n",
            "Requirement already satisfied: nvidia-cublas-cu12>=12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.35,>=0.4.34; extra == \"cuda12-pip\"->jax[cuda12_pip]) (12.6.3.3)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12>=12.1.105 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.35,>=0.4.34; extra == \"cuda12-pip\"->jax[cuda12_pip]) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cuda-nvcc-cu12>=12.1.105 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.35,>=0.4.34; extra == \"cuda12-pip\"->jax[cuda12_pip]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12>=12.1.105 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.35,>=0.4.34; extra == \"cuda12-pip\"->jax[cuda12_pip]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12<10.0,>=9.1 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.35,>=0.4.34; extra == \"cuda12-pip\"->jax[cuda12_pip]) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cufft-cu12>=11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.35,>=0.4.34; extra == \"cuda12-pip\"->jax[cuda12_pip]) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12>=11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.35,>=0.4.34; extra == \"cuda12-pip\"->jax[cuda12_pip]) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12>=12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.35,>=0.4.34; extra == \"cuda12-pip\"->jax[cuda12_pip]) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12>=2.18.1 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.35,>=0.4.34; extra == \"cuda12-pip\"->jax[cuda12_pip]) (2.23.4)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12>=12.1.105 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.35,>=0.4.34; extra == \"cuda12-pip\"->jax[cuda12_pip]) (12.6.77)\n",
            "Downloading jaxlib-0.4.34-cp310-cp310-manylinux2014_x86_64.whl (86.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.1/86.1 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax_cuda12_plugin-0.4.35-cp310-cp310-manylinux2014_x86_64.whl (15.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax_cuda12_pjrt-0.4.35-py3-none-manylinux2014_x86_64.whl (100.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.8/100.8 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax-0.4.35-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jax-cuda12-pjrt, jax-cuda12-plugin, jaxlib, jax\n",
            "  Attempting uninstall: jax-cuda12-pjrt\n",
            "    Found existing installation: jax-cuda12-pjrt 0.4.33\n",
            "    Uninstalling jax-cuda12-pjrt-0.4.33:\n",
            "      Successfully uninstalled jax-cuda12-pjrt-0.4.33\n",
            "  Attempting uninstall: jax-cuda12-plugin\n",
            "    Found existing installation: jax-cuda12-plugin 0.4.33\n",
            "    Uninstalling jax-cuda12-plugin-0.4.33:\n",
            "      Successfully uninstalled jax-cuda12-plugin-0.4.33\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.4.33\n",
            "    Uninstalling jaxlib-0.4.33:\n",
            "      Successfully uninstalled jaxlib-0.4.33\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.4.33\n",
            "    Uninstalling jax-0.4.33:\n",
            "      Successfully uninstalled jax-0.4.33\n",
            "Successfully installed jax-0.4.35 jax-cuda12-pjrt-0.4.35 jax-cuda12-plugin-0.4.35 jaxlib-0.4.34\n"
          ]
        }
      ],
      "source": [
        "# Install packages\n",
        "!pip install pennylane pennylane-lightning pennylane-lightning[gpu] cotengra quimb --upgrade\n",
        "!pip install -U \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
        "# !pip install --upgrade \"jax[cuda11_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YC92azd1bgkU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3d28fab-dfa1-461d-cae0-39a647929bbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-b00b07bf67a6>:40: DeprecationWarning: jax.lib.xla_bridge.get_backend is deprecated; use jax.extend.backend.get_backend.\n",
            "  tpu_backend = xla_bridge.get_backend('tpu')\n",
            "<ipython-input-2-b00b07bf67a6>:51: DeprecationWarning: jax.lib.xla_bridge.get_backend is deprecated; use jax.extend.backend.get_backend.\n",
            "  gpu_backend = xla_bridge.get_backend('gpu')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Set platform to GPU\n"
          ]
        }
      ],
      "source": [
        "# Import packages\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import numpy as np\n",
        "#np.set_printoptions(threshold=sys.maxsize)\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "import seaborn as sns\n",
        "import jax\n",
        "import time\n",
        "\n",
        "import functools\n",
        "\n",
        "from typing import List, Union, Tuple, Dict, Optional, Any\n",
        "from typing import Callable\n",
        "\n",
        "jax.config.update(\"jax_enable_x64\", True)\n",
        "#jax.config.update(\"jax_debug_nans\", True)\n",
        "import jax.numpy as jnp\n",
        "\n",
        "import optax  # optimization using jax\n",
        "\n",
        "import torch  # https://pytorch.org\n",
        "import torchvision  # https://pytorch.org\n",
        "#torch.set_printoptions(profile=\"full\")\n",
        "import pennylane as qml\n",
        "import pennylane.numpy as pnp\n",
        "\n",
        "import os, cv2, itertools # cv2 -- OpenCV\n",
        "import shutil\n",
        "import zipfile\n",
        "%matplotlib inline\n",
        "\n",
        "from jax.lib import xla_bridge\n",
        "\n",
        "def set_jax_platform():\n",
        "    # Check if TPU is available\n",
        "    try:\n",
        "        tpu_backend = xla_bridge.get_backend('tpu')\n",
        "        if tpu_backend and tpu_backend.device_count() > 0:\n",
        "            # Set platform to TPU\n",
        "            jax.config.update('jax_platform_name', 'tpu')\n",
        "            print(\"Set platform to TPU\")\n",
        "            return\n",
        "    except RuntimeError:\n",
        "        pass  # No TPU found, move on to check for GPU\n",
        "\n",
        "    # Check if GPU is available\n",
        "    try:\n",
        "      gpu_backend = xla_bridge.get_backend('gpu')\n",
        "      if gpu_backend and gpu_backend.device_count() > 0:\n",
        "          # Set platform to CUDA (GPU)\n",
        "          jax.config.update('jax_platform_name', 'gpu')\n",
        "          print(\"Set platform to GPU\")\n",
        "    except RuntimeError:\n",
        "          # Set platform to CPU\n",
        "          jax.config.update('jax_platform_name', 'cpu')\n",
        "          print(\"Set platform to CPU\")\n",
        "\n",
        "# Call the function to set the platform\n",
        "set_jax_platform()\n",
        "\n",
        "sns.set()\n",
        "\n",
        "seed = 1701\n",
        "rng = np.random.default_rng(seed=seed)\n",
        "prng = pnp.random.default_rng(seed=seed)\n",
        "jrng_key = jax.random.PRNGKey(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCo8fdobbiTB"
      },
      "source": [
        "# Prepare the Dataset\n",
        "\n",
        "For the rescaled image matrix $M$, the \"Hermitian version\" of it can be calculated as:\n",
        "\n",
        "$$\n",
        "A = \\frac{M+M^T}{2}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlSUCStebkoA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55b6aaf5-3a9f-446d-cd09-aca01e2a2461"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to FashioMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:00<00:00, 113MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting FashioMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz to FashioMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to FashioMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 5.88MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting FashioMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz to FashioMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to FashioMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 65.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting FashioMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to FashioMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to FashioMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 7.52MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting FashioMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to FashioMNIST/FashionMNIST/raw\n",
            "\n",
            "(64, 32, 32)\n",
            "(64,)\n",
            "[9 0 8 5 4 6 3 0 2 7 8 4 3 0 9 6 1 8 1 6 2 2 3 7 8 4 5 6 7 3 1 7 5 1 6 0 0\n",
            " 7 1 0 1 2 4 5 2 5 6 9 4 7 7 8 9 7 4 0 3 9 5 8 1 9 8 1]\n",
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.00784314 0.0882353  0.23921569 0.7490196  0.8352941  0.73921573\n",
            " 0.85294116 0.8392157  0.8784314  0.9137255  0.84901965 0.7352941\n",
            " 0.7509804  0.8392157  0.8        0.39215687 0.02941176 0.\n",
            " 0.         0.        ]\n"
          ]
        }
      ],
      "source": [
        "preprocess = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Pad(2),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Lambda(lambda x: torch.squeeze(x)),\n",
        "    #torchvision.transforms.Lambda(lambda x: x / torch.trace(x)),\n",
        "    torchvision.transforms.Lambda(lambda x: (x+torch.transpose(x, 0, 1))/2)\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.FashionMNIST(\n",
        "    \"FashioMNIST\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=preprocess,\n",
        ")\n",
        "test_dataset = torchvision.datasets.FashionMNIST(\n",
        "    \"FashioMNIST\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=preprocess,\n",
        ")\n",
        "dummy_trainloader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=64, shuffle=True\n",
        ")\n",
        "dummy_testloader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=64, shuffle=True\n",
        ")\n",
        "\n",
        "dummy_x, dummy_y = next(iter(dummy_trainloader))\n",
        "dummy_x = dummy_x.numpy()\n",
        "dummy_y = dummy_y.numpy()\n",
        "print(dummy_x.shape)  # 64x32x32\n",
        "print(dummy_y.shape)  # 64\n",
        "print(dummy_y)\n",
        "print(dummy_x[0,16])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR5l9DZ5bubn"
      },
      "source": [
        "# Time-Evolve the Image Hermitian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYbF2gRNbxKF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19eb8c99-cf1d-441e-b419-a89126c15fa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.00000000e+00+0.j          0.00000000e+00+0.j\n",
            " -5.89534007e-02+0.04101709j -6.95098191e-02+0.1484778j\n",
            "  8.60111415e-02+0.12288549j -1.01286255e-01+0.09358104j\n",
            " -1.90073550e-01-0.18942711j -9.22792405e-02-0.02789476j\n",
            " -1.26657680e-01-0.1437277j  -8.17655772e-02-0.18954791j\n",
            " -2.45259088e-02-0.10738815j -2.85231229e-02-0.05934005j\n",
            "  3.87904383e-02+0.03087516j  8.22723955e-02-0.10486024j\n",
            "  1.83286145e-01+0.08504909j  5.09984046e-02-0.09211471j\n",
            "  1.45972639e-01+0.22950402j -4.08930391e-01+0.2426286j\n",
            "  5.87662915e-03+0.05058653j  1.31081358e-01-0.15186265j\n",
            "  2.53468812e-01-0.10031033j  5.32645695e-02+0.19603465j\n",
            " -7.36299977e-02+0.19501266j  4.20510396e-03+0.0379052j\n",
            " -1.62196919e-01-0.01994889j -1.87066719e-01+0.07093962j\n",
            " -2.72042185e-01+0.03491033j -5.81320412e-02-0.11716465j\n",
            "  2.01218184e-02-0.09768091j  5.38383138e-05-0.06946835j\n",
            "  0.00000000e+00+0.j          0.00000000e+00+0.j        ]\n",
            "[[1.0000000e+00+0.0000000e+00j 0.0000000e+00+0.0000000e+00j\n",
            "  0.0000000e+00+0.0000000e+00j ... 0.0000000e+00+0.0000000e+00j\n",
            "  0.0000000e+00+0.0000000e+00j 0.0000000e+00+0.0000000e+00j]\n",
            " [0.0000000e+00+0.0000000e+00j 1.0000000e+00+0.0000000e+00j\n",
            "  0.0000000e+00+0.0000000e+00j ... 0.0000000e+00+0.0000000e+00j\n",
            "  0.0000000e+00+0.0000000e+00j 0.0000000e+00+0.0000000e+00j]\n",
            " [0.0000000e+00+0.0000000e+00j 0.0000000e+00+0.0000000e+00j\n",
            "  1.0000036e+00+1.9448686e-10j ... 1.6522912e-07-6.6029315e-08j\n",
            "  0.0000000e+00+0.0000000e+00j 0.0000000e+00+0.0000000e+00j]\n",
            " ...\n",
            " [0.0000000e+00+0.0000000e+00j 0.0000000e+00+0.0000000e+00j\n",
            "  1.6542543e-07+6.6029315e-08j ... 9.9999976e-01+6.5050854e-11j\n",
            "  0.0000000e+00+0.0000000e+00j 0.0000000e+00+0.0000000e+00j]\n",
            " [0.0000000e+00+0.0000000e+00j 0.0000000e+00+0.0000000e+00j\n",
            "  0.0000000e+00+0.0000000e+00j ... 0.0000000e+00+0.0000000e+00j\n",
            "  1.0000000e+00+0.0000000e+00j 0.0000000e+00+0.0000000e+00j]\n",
            " [0.0000000e+00+0.0000000e+00j 0.0000000e+00+0.0000000e+00j\n",
            "  0.0000000e+00+0.0000000e+00j ... 0.0000000e+00+0.0000000e+00j\n",
            "  0.0000000e+00+0.0000000e+00j 1.0000000e+00+0.0000000e+00j]]\n",
            "[[1.0000000e+00+0.0000000e+00j 0.0000000e+00+0.0000000e+00j\n",
            "  0.0000000e+00+0.0000000e+00j ... 0.0000000e+00+0.0000000e+00j\n",
            "  0.0000000e+00+0.0000000e+00j 0.0000000e+00+0.0000000e+00j]\n",
            " [0.0000000e+00+0.0000000e+00j 1.0000000e+00+0.0000000e+00j\n",
            "  0.0000000e+00+0.0000000e+00j ... 0.0000000e+00+0.0000000e+00j\n",
            "  0.0000000e+00+0.0000000e+00j 0.0000000e+00+0.0000000e+00j]\n",
            " [0.0000000e+00+0.0000000e+00j 0.0000000e+00+0.0000000e+00j\n",
            "  1.0000037e+00-2.3909108e-10j ... 2.8064298e-08-1.8446245e-08j\n",
            "  0.0000000e+00+0.0000000e+00j 0.0000000e+00+0.0000000e+00j]\n",
            " ...\n",
            " [0.0000000e+00+0.0000000e+00j 0.0000000e+00+0.0000000e+00j\n",
            "  2.8169644e-08+1.8446245e-08j ... 9.9999988e-01-6.5050854e-11j\n",
            "  0.0000000e+00+0.0000000e+00j 0.0000000e+00+0.0000000e+00j]\n",
            " [0.0000000e+00+0.0000000e+00j 0.0000000e+00+0.0000000e+00j\n",
            "  0.0000000e+00+0.0000000e+00j ... 0.0000000e+00+0.0000000e+00j\n",
            "  1.0000000e+00+0.0000000e+00j 0.0000000e+00+0.0000000e+00j]\n",
            " [0.0000000e+00+0.0000000e+00j 0.0000000e+00+0.0000000e+00j\n",
            "  0.0000000e+00+0.0000000e+00j ... 0.0000000e+00+0.0000000e+00j\n",
            "  0.0000000e+00+0.0000000e+00j 1.0000000e+00+0.0000000e+00j]]\n"
          ]
        }
      ],
      "source": [
        "def img_hermitian_evolve(\n",
        "    img:jnp.ndarray,\n",
        "    t:float\n",
        ")->jnp.ndarray:\n",
        "  assert img.shape[-1]==32 and img.shape[-2] == 32, f\"The shape of the image must be 32 by 32, got {img.shape[-2]} by {img.shape[-1]}\"\n",
        "  return jax.scipy.linalg.expm(img*( -0.5j*t))\n",
        "\n",
        "print(\n",
        "    img_hermitian_evolve(\n",
        "        dummy_x[0],\n",
        "        10\n",
        "        )[16]\n",
        "    )\n",
        "\n",
        "\n",
        "print(\n",
        "    jnp.einsum(\n",
        "        \"ij,jk->ik\",\n",
        "        jnp.transpose(jnp.conjugate(img_hermitian_evolve(\n",
        "        dummy_x[0],\n",
        "        10\n",
        "        ))),\n",
        "        img_hermitian_evolve(\n",
        "        dummy_x[0],\n",
        "        10\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\n",
        "    jnp.einsum(\n",
        "        \"ij,jk->ik\",\n",
        "        img_hermitian_evolve(\n",
        "        dummy_x[0],\n",
        "        10\n",
        "        ),\n",
        "        jnp.transpose(jnp.conjugate(img_hermitian_evolve(\n",
        "        dummy_x[0],\n",
        "        10\n",
        "        )))\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enL2qIVtcNLF"
      },
      "source": [
        "# Some Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVOKRfoacK60"
      },
      "outputs": [],
      "source": [
        "ket = {\n",
        "    '0':jnp.array([1,0]),\n",
        "    '1':jnp.array([0,1]),\n",
        "    '+':(jnp.array([1,0]) + jnp.array([0,1]))/jnp.sqrt(2),\n",
        "    '-':(jnp.array([1,0]) - jnp.array([0,1]))/jnp.sqrt(2)\n",
        "}\n",
        "\n",
        "pauli = {\n",
        "    'I':jnp.array([[1,0],[0,1]]),\n",
        "    'X':jnp.array([[0,1],[1,0]]),\n",
        "    'Y':jnp.array([[0, -1j],[1j, 0]]),\n",
        "    'Z':jnp.array([[1,0],[0,-1]])\n",
        "}\n",
        "\n",
        "def tensor_product(*args):\n",
        "  input_list = [a for a in args]\n",
        "  return functools.reduce(jnp.kron, input_list)\n",
        "\n",
        "def multi_qubit_identity(n_qubits:int)->jnp.ndarray:\n",
        "  assert n_qubits>0\n",
        "  if n_qubits == 1:\n",
        "    return pauli['I']\n",
        "  else:\n",
        "    return tensor_product(*[pauli['I'] for _ in range(n_qubits)])\n",
        "\n",
        "pauli_words_su4 = {}\n",
        "for key1 in pauli.keys():\n",
        "  for key2 in pauli.keys():\n",
        "    if not (key1==key2 and key1=='I' and key2=='I'):\n",
        "      pauli_words_su4[key1+key2] = tensor_product(pauli[key1], pauli[key2])\n",
        "\n",
        "pauli_words_su8 = {}\n",
        "for key1 in pauli.keys():\n",
        "  for key2 in pauli.keys():\n",
        "    for key3 in pauli.keys():\n",
        "      if not key1+key2+key3 == 'III':\n",
        "        pauli_words_su8[key1+key2+key3] = tensor_product(pauli[key1], pauli[key2], pauli[key3])\n",
        "\n",
        "pauli_words_su16 = {}\n",
        "for key1 in pauli.keys():\n",
        "  for key2 in pauli.keys():\n",
        "    for key3 in pauli.keys():\n",
        "      for key4 in pauli.keys():\n",
        "        if not key1+key2+key3+key4 == 'IIII':\n",
        "          pauli_words_su16[key1+key2+key3+key4] = tensor_product(\n",
        "              pauli[key1],\n",
        "              pauli[key2],\n",
        "              pauli[key3],\n",
        "              pauli[key4]\n",
        "          )\n",
        "\n",
        "pauli_words_su32 = {}\n",
        "for key1 in pauli.keys():\n",
        "  for key2 in pauli.keys():\n",
        "    for key3 in pauli.keys():\n",
        "      for key4 in pauli.keys():\n",
        "        for key5 in pauli.keys():\n",
        "          if not key1+key2+key3+key4+key5 == 'IIIII':\n",
        "            pauli_words_su32[key1+key2+key3+key4+key5] = tensor_product(\n",
        "                pauli[key1],\n",
        "                pauli[key2],\n",
        "                pauli[key3],\n",
        "                pauli[key4],\n",
        "                pauli[key5]\n",
        "            )\n",
        "\n",
        "observables_10_cls_5q = [0]*10\n",
        "for i in ['0', '1']:\n",
        "  for j in ['0', '1']:\n",
        "    for k in ['0', '1']:\n",
        "      for l in ['0', '1']:\n",
        "        idx = int(i+j+k+l, 2)\n",
        "        if idx <10:\n",
        "          basis_state = tensor_product(*[ket[i], ket[j], ket[k], ket[l]])\n",
        "          four_qubit_obs = jnp.outer(basis_state, basis_state)\n",
        "          observables_10_cls_5q[idx] = tensor_product(four_qubit_obs, multi_qubit_identity(1))\n",
        "\n",
        "observables_8_cls_5q = [0]*8\n",
        "for i in ['0', '1']:\n",
        "  for j in ['0', '1']:\n",
        "    for k in ['0', '1']:\n",
        "      for l in ['0', '1']:\n",
        "        idx = int(i+j+k+l, 2)\n",
        "        if idx <8:\n",
        "          basis_state = tensor_product(*[ket[i], ket[j], ket[k], ket[l]])\n",
        "          four_qubit_obs = jnp.outer(basis_state, basis_state)\n",
        "          observables_8_cls_5q[idx] = tensor_product(four_qubit_obs, multi_qubit_identity(1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfMfQrSncpW1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2644cff6-5dec-4716-81f3-99a78165ac5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.00000000e+00-2.81802478e-18j -2.08166817e-17-6.93889390e-17j\n",
            "   1.87350135e-16+4.02455846e-16j ...  6.78276879e-16-5.07189776e-16j\n",
            "  -7.11236625e-16+6.24500451e-17j -2.53269627e-16-5.55111512e-16j]\n",
            " [-2.08166817e-17+4.85722573e-17j  1.00000000e+00+1.79597214e-18j\n",
            "   3.78169718e-16-5.82867088e-16j ...  8.04911693e-16-1.17267307e-15j\n",
            "   4.85722573e-17-4.51089090e-16j -2.35922393e-16-2.22044605e-16j]\n",
            " [ 1.87350135e-16-3.95516953e-16j  3.78169718e-16+5.55111512e-16j\n",
            "   1.00000000e+00-1.01336686e-18j ...  6.38378239e-16+8.18789481e-16j\n",
            "  -2.74086309e-16-5.20417043e-16j  4.85722573e-17-4.23272528e-16j]\n",
            " ...\n",
            " [ 6.81746326e-16+5.07189776e-16j  7.91033905e-16+1.17267307e-15j\n",
            "   6.93889390e-16-8.18789481e-16j ...  1.00000000e+00-1.40196419e-18j\n",
            "  -1.75207071e-16+8.53483950e-16j  3.46944695e-16-6.38378239e-16j]\n",
            " [-7.04297731e-16-6.24500451e-17j  4.16333634e-17+4.51089090e-16j\n",
            "  -2.63677968e-16+5.20417043e-16j ... -1.75207071e-16-8.60422844e-16j\n",
            "   1.00000000e+00+4.01611805e-18j -1.38777878e-16+5.13478149e-16j]\n",
            " [-2.60208521e-16+5.55111512e-16j -2.35922393e-16+2.22044605e-16j\n",
            "   6.72205347e-17+4.23272528e-16j ...  3.46944695e-16+6.34908792e-16j\n",
            "  -1.38777878e-16-4.99600361e-16j  1.00000000e+00-6.72126142e-18j]]\n",
            "[[ 1.00000000e+00-6.11733426e-18j -2.77555756e-16+1.28369537e-16j\n",
            "   4.89192020e-16+5.20417043e-16j ...  6.17561557e-16+8.32667268e-17j\n",
            "  -7.45931095e-16+4.44089210e-16j  2.15105711e-16+3.03576608e-16j]\n",
            " [-2.77555756e-16-1.28369537e-16j  1.00000000e+00+2.93457884e-18j\n",
            "  -4.16333634e-17-4.02455846e-16j ...  2.18575158e-16-7.97972799e-16j\n",
            "  -6.31439345e-16-5.65519853e-16j -7.84095011e-16+3.92047506e-16j]\n",
            " [ 4.89192020e-16-5.13478149e-16j -4.16333634e-17+4.16333634e-16j\n",
            "   1.00000000e+00+4.19581117e-18j ...  4.71844785e-16+4.02455846e-16j\n",
            "   6.93889390e-16-3.53883589e-16j -2.22044605e-16-4.76181594e-16j]\n",
            " ...\n",
            " [ 6.24500451e-16-8.32667268e-17j  2.22044605e-16+7.97972799e-16j\n",
            "   4.64905892e-16-4.02455846e-16j ...  1.00000000e+00-9.18932078e-19j\n",
            "  -2.49800181e-16+4.78783679e-16j  2.91433544e-16-3.05311332e-16j]\n",
            " [-7.56339436e-16-4.44089210e-16j -6.45317133e-16+5.65519853e-16j\n",
            "   6.81746326e-16+3.53883589e-16j ... -2.49800181e-16-5.06539255e-16j\n",
            "   1.00000000e+00-1.23589763e-18j -8.46545056e-16+8.85576334e-16j]\n",
            " [ 1.94289029e-16-3.03576608e-16j -7.75421394e-16-3.92047506e-16j\n",
            "  -2.35922393e-16+4.76181594e-16j ...  2.91433544e-16+3.05311332e-16j\n",
            "  -8.46545056e-16-8.76902717e-16j  1.00000000e+00-9.00364469e-19j]]\n"
          ]
        }
      ],
      "source": [
        "def su32_op(\n",
        "    params:jnp.ndarray\n",
        "):\n",
        "  generator = jnp.einsum(\"i, ijk - >jk\", params, jnp.asarray(list(pauli_words_su32.values())))\n",
        "  return jax.scipy.linalg.expm(1j*generator)\n",
        "\n",
        "test_params = jax.random.normal(shape=[4**5-1], key=jrng_key)\n",
        "\n",
        "print(\n",
        "    jnp.einsum(\n",
        "        \"ij,jk->ik\",\n",
        "        jnp.transpose(jnp.conjugate(su32_op(test_params))),\n",
        "        su32_op(test_params)\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\n",
        "    jnp.einsum(\n",
        "        \"ij,jk->ik\",\n",
        "        su32_op(test_params),\n",
        "        jnp.transpose(jnp.conjugate(su32_op(test_params)))\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJ8epBAvc_77"
      },
      "outputs": [],
      "source": [
        "def measure_sv(\n",
        "    state:jnp.ndarray,\n",
        "    observable:jnp.ndarray\n",
        "    ):\n",
        "  \"\"\"\n",
        "  Measure a statevector with a Hermitian observable.\n",
        "  Note: No checking Hermitianicity of the observable or whether the observable\n",
        "  has all real eigenvalues or not\n",
        "  \"\"\"\n",
        "  expectation_value = jnp.dot(jnp.conj(state.T), jnp.dot(observable, state))\n",
        "  return jnp.real(expectation_value)\n",
        "\n",
        "def measure_dm(\n",
        "    rho:jnp.ndarray,\n",
        "    observable:jnp.ndarray\n",
        "):\n",
        "  \"\"\"\n",
        "  Measure a density matrix with a Hermitian observable.\n",
        "  Note: No checking Hermitianicity of the observable or whether the observable\n",
        "  has all real eigenvalues or not.\n",
        "  \"\"\"\n",
        "  product = jnp.dot(rho, observable)\n",
        "\n",
        "  # Calculate the trace, which is the sum of diagonal elements\n",
        "  trace = jnp.trace(product)\n",
        "\n",
        "  # The expectation value should be real for physical observables\n",
        "  return jnp.real(trace)\n",
        "\n",
        "vmap_measure_sv = jax.vmap(measure_sv, in_axes=(None, 0), out_axes=0)\n",
        "vmap_measure_dm = jax.vmap(measure_dm, in_axes=(None, 0), out_axes=0)\n",
        "\n",
        "def bitstring_to_state(bitstring:str):\n",
        "  \"\"\"\n",
        "  Convert a bit string, like '0101001' or '+-+-101'\n",
        "  to a statevector. Each character in the bitstring must be among\n",
        "  0, 1, + and -\n",
        "  \"\"\"\n",
        "  assert len(bitstring)>0\n",
        "  for c in bitstring:\n",
        "    assert c in ['0', '1', '+', '-']\n",
        "  single_qubit_states = [ket[c] for c in bitstring]\n",
        "  return tensor_product(*single_qubit_states)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMtaAqSXdDHH"
      },
      "source": [
        "# The QNN\n",
        "\n",
        "With data re-uploading\n",
        "\n",
        "$$\n",
        "|{\\varphi(\\theta,t)}\\rangle = \\Pi_n (\\mathrm{ParameterisedLayers}(\\theta_n) e^{-\\frac{it_n}{2}M} )|+\\rangle^{\\otimes 5}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdyCKp2pdE8j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03025d07-14af-4ce6-808e-b0970728f711"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.00440108 0.06853663 0.09392378 0.02776329 0.0212095  0.05979369\n",
            " 0.10347425 0.07256673]\n"
          ]
        }
      ],
      "source": [
        "def qnn_hamevo(\n",
        "    params:jnp.ndarray,\n",
        "    t:jnp.ndarray,\n",
        "    img:jnp.ndarray\n",
        ")->jnp.ndarray:\n",
        "  \"\"\"\n",
        "  A QNN that takes (M+M^T)/2\n",
        "  as input, where M is the (rescaled) original image,\n",
        "  as well as a trainable parameter t,\n",
        "  and parameters for trainable layers\n",
        "  and output an array of 2 elements representing classification logits\n",
        "  \"\"\"\n",
        "  single_op_params = 4**5-1\n",
        "\n",
        "  n_outer_layers = len(t)\n",
        "  n_inner_layers = (len(params)//single_op_params)//n_outer_layers\n",
        "  state = tensor_product(ket['+'], ket['+'], ket['+'], ket['+'], ket['+'])\n",
        "  for i in range(n_outer_layers):\n",
        "    state = jnp.dot(\n",
        "      img_hermitian_evolve(img, t[i]),\n",
        "      state\n",
        "      )\n",
        "    inner_layer_params = params[i*(single_op_params*n_inner_layers):(i+1)*(single_op_params*n_inner_layers)]\n",
        "    for j in range(n_inner_layers):\n",
        "      state = jnp.dot(\n",
        "          #brickwall_su4_5q_single_layer(inner_layer_params[j*single_op_params:(j+1)*single_op_params]),\n",
        "          su32_op(inner_layer_params[j*single_op_params:(j+1)*single_op_params]),\n",
        "          state\n",
        "      )\n",
        "  return vmap_measure_sv(state, jnp.asarray(observables_8_cls_5q))\n",
        "\n",
        "\n",
        "\n",
        "print(\n",
        "    qnn_hamevo(\n",
        "        jax.random.normal(shape=[( 4**5-1)*15], key=jrng_key),\n",
        "        jax.random.normal(shape=[15], key=jrng_key),\n",
        "        dummy_x[0]\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFjfEttbdNEn"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Pc1I3fqdOX_"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def compute_out(weight,t, features, labels):\n",
        "    \"\"\"Computes the output of the corresponding label in the qcnn\"\"\"\n",
        "    out = lambda weight,t, feature, label: qnn_hamevo(weight,t, feature)\n",
        "    return jax.vmap(out, in_axes=(None,None,  0, 0), out_axes=0)(\n",
        "        weight,t, features, labels\n",
        "    )\n",
        "\n",
        "\n",
        "def compute_accuracy(weight,t, features, labels):\n",
        "    \"\"\"Computes the accuracy over the provided features and labels\"\"\"\n",
        "    out = compute_out(weight,t, features, labels)\n",
        "    pred = jnp.argmax(out, axis = 1)\n",
        "    return jnp.sum(jnp.array(pred == labels).astype(int)) / len(out)\n",
        "\n",
        "\n",
        "def compute_cost(weight,t, features, labels):\n",
        "    \"\"\"Computes the cost over the provided features and labels\"\"\"\n",
        "    logits = compute_out(weight,t, features, labels)\n",
        "    return jnp.nanmean(optax.softmax_cross_entropy_with_integer_labels(logits, labels))\n",
        "\n",
        "\n",
        "value_and_grad = jax.jit(jax.value_and_grad(compute_cost, argnums=[0,1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aI13rv6DdRJO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import jax.numpy as jnp\n",
        "N_OUTER_LAYERS = 10\n",
        "N_INNER_LAYERS = 1\n",
        "N_LAYERS = N_OUTER_LAYERS*N_INNER_LAYERS\n",
        "SINGLE_OP_PARAMS  = 4**5-1\n",
        "\n",
        "# def init_weights():\n",
        "#     return jax.random.normal(shape=[SINGLE_OP_PARAMS*N_LAYERS], key=jrng_key),jax.random.normal(shape=[N_OUTER_LAYERS], key=jrng_key)\n",
        "\n",
        "def init_weights(alpha=0.5, beta=2.0):\n",
        "    df = pd.read_csv('weights_beta.csv')\n",
        "\n",
        "# Check if the DataFrame is not empty\n",
        "    if not df.empty:\n",
        "    # Access the last row directly\n",
        "       row = df.iloc[-1]  # Access the last row\n",
        "\n",
        "    # Extract the epoch, weights, and biases\n",
        "    epoch = row['epoch']\n",
        "    weights = jnp.array(ast.literal_eval(row['weights']))  # Convert to JAX array\n",
        "    biases = jnp.array(ast.literal_eval(row['biases']))    # Convert to JAX array\n",
        "    # Initialize weights with a Beta distribution skewed towards 0\n",
        "    # weights = jax.random.beta(jrng_key, alpha, beta, shape=[SINGLE_OP_PARAMS*N_LAYERS])\n",
        "    # biases = jax.random.beta(jrng_key, alpha, beta, shape=[N_OUTER_LAYERS])\n",
        "    return weights, biases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftq2tMNou7a6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def save_weights_to_csv(weights, biases, epoch, file_name='weights_beta.csv'):\n",
        "    \"\"\"Saves the weights and biases to a CSV file.\"\"\"\n",
        "    # Convert weights and biases to a flat list\n",
        "    weight_list = weights.flatten().tolist()\n",
        "    bias_list = biases.flatten().tolist()\n",
        "\n",
        "    # Create a dictionary to store the weights and biases with epoch\n",
        "    data = {'epoch': [epoch], 'weights': [weight_list], 'biases': [bias_list]}\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Check if the file exists before appending\n",
        "    if not os.path.isfile(file_name):\n",
        "        # If the file does not exist, create it with a header\n",
        "        df.to_csv(file_name, mode='w', header=True, index=False)\n",
        "    else:\n",
        "        # If the file exists, append the new data without a header\n",
        "        df.to_csv(file_name, mode='a', header=False, index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xU8hJrkM6OFO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c862064f-4cea-472d-fffe-fb3c9879d599"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with: 45600; Validation test: 2400; Testing with: 8000\n"
          ]
        }
      ],
      "source": [
        "# Select data\n",
        "labels = [0, 1, 2, 3, 4, 5, 6, 7]\n",
        "indices_train = [idx for idx, target in enumerate(train_dataset.targets) if target in labels]\n",
        "\n",
        "# Separate 5% of the training data for validation\n",
        "val_size = int(0.05 * len(indices_train))\n",
        "indices_val = indices_train[:val_size]  # First 5% for validation\n",
        "indices_train = indices_train[val_size:]  # Remaining 95% for training\n",
        "\n",
        "indices_test = [idx for idx, target in enumerate(test_dataset.targets) if target in labels]\n",
        "\n",
        "N_TRAIN = len(indices_train)\n",
        "N_VAL = len(indices_val)\n",
        "N_TEST = len(indices_test)\n",
        "\n",
        "print(\n",
        "    f\"Training with: {N_TRAIN}; Validation test: {N_VAL}; Testing with: {N_TEST}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLyRg7EydVgN"
      },
      "outputs": [],
      "source": [
        "def train_vqc(batchsize: int, n_epochs: int, seed: int = 1701):\n",
        "    start = time.time()\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Load data\n",
        "    labels = [0, 1, 2, 3, 4, 5, 6, 7]\n",
        "    indices_train = [idx for idx, target in enumerate(train_dataset.targets) if target in labels]\n",
        "    indices_test = [idx for idx, target in enumerate(test_dataset.targets) if target in labels]\n",
        "\n",
        "    N_TRAIN = len(indices_train)\n",
        "\n",
        "    # Calculate the number of validation samples (5%)\n",
        "    n_val = int(0.05 * N_TRAIN)  # 5% for validation\n",
        "    n_train = N_TRAIN - n_val  # Remaining for training\n",
        "\n",
        "    # Shuffle the training indices\n",
        "    np.random.shuffle(indices_train)\n",
        "\n",
        "    # Split the indices into training and validation\n",
        "    indices_val = indices_train[:n_val]  # First 5% for validation\n",
        "    indices_train_final = indices_train[n_val:]  # Remaining for training\n",
        "\n",
        "    # Create data loaders\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "        torch.utils.data.Subset(train_dataset, indices_train_final), batch_size=batchsize, shuffle=True\n",
        "    )\n",
        "    valloader = torch.utils.data.DataLoader(\n",
        "        torch.utils.data.Subset(train_dataset, indices_val), batch_size=batchsize, shuffle=False\n",
        "    )\n",
        "\n",
        "    # Exponential decay of the learning rate.\n",
        "    scheduler = optax.exponential_decay(\n",
        "        init_value=0.01,\n",
        "        transition_steps=n_epochs,\n",
        "        decay_rate=0.99\n",
        "    )\n",
        "\n",
        "    # Combining gradient transforms using `optax.chain`.\n",
        "    gradient_transform = optax.chain(\n",
        "        optax.clip(1.0),\n",
        "        optax.scale_by_adam(),\n",
        "        optax.scale_by_schedule(scheduler),\n",
        "        optax.scale(-1.0)\n",
        "    )\n",
        "\n",
        "    # Init weights and optimizer\n",
        "    weights, weights_last = init_weights()\n",
        "    opt_state = gradient_transform.init((weights, weights_last))\n",
        "\n",
        "    # Data containers\n",
        "    train_cost_epochs, val_cost_epochs, train_acc_epochs, val_acc_epochs = [], [], [], []\n",
        "\n",
        "    for step in range(n_epochs):\n",
        "        train_cost_batches = []\n",
        "        train_acc_batches = []\n",
        "        val_cost_batches = []\n",
        "        val_acc_batches = []\n",
        "\n",
        "        epoch_start = time.time()\n",
        "        print(f\"Training at Epoch {step + 1}/{n_epochs}, Train batches {len(trainloader)}, Val batches {len(valloader)}......\")\n",
        "\n",
        "        # Training loop\n",
        "        for batch, (x_train, y_train) in enumerate(trainloader):\n",
        "            batch_start = time.time()\n",
        "            x_train, y_train = jnp.asarray(x_train.numpy()), jnp.asarray(y_train.numpy())\n",
        "            train_cost, grad_circuit = value_and_grad(weights, weights_last, x_train, y_train)\n",
        "            updates, opt_state = gradient_transform.update(grad_circuit, opt_state)\n",
        "            weights, weights_last = optax.apply_updates((weights, weights_last), updates)\n",
        "            train_acc = compute_accuracy(weights, weights_last, x_train, y_train)\n",
        "            train_cost_batches.append(train_cost)\n",
        "            train_acc_batches.append(train_acc)\n",
        "\n",
        "            if len(trainloader) <= 5 or (batch + 1) % 5 == 0:\n",
        "                print(f\"Training at Epoch {step + 1}/{n_epochs}, Batch {batch + 1}, Cost {train_cost}, Acc {train_acc}. Time {time.time() - batch_start}\")\n",
        "\n",
        "        train_cost_epochs.append(np.mean(train_cost_batches))\n",
        "        train_acc_epochs.append(np.mean(train_acc_batches))\n",
        "\n",
        "        # Validation loop\n",
        "        for batch, (x_val, y_val) in enumerate(valloader):\n",
        "            x_val, y_val = jnp.asarray(x_val.numpy()), jnp.asarray(y_val.numpy())\n",
        "            val_out = compute_out(weights, weights_last, x_val, y_val)\n",
        "            val_pred = jnp.argmax(val_out, axis=1)\n",
        "            val_acc = jnp.sum(jnp.array(val_pred == y_val).astype(int)) / len(val_out)\n",
        "            val_cost = jnp.nanmean(optax.softmax_cross_entropy_with_integer_labels(val_out, y_val))\n",
        "            val_cost_batches.append(val_cost)\n",
        "            val_acc_batches.append(val_acc)\n",
        "\n",
        "        val_acc_epochs.append(np.mean(val_acc_batches))\n",
        "        val_cost_epochs.append(np.mean(val_cost_batches))\n",
        "\n",
        "        print(\"......\")\n",
        "        print(f\"Epoch {step + 1}/{n_epochs}, Train: Cost {np.mean(train_cost_batches)}, Acc {np.mean(train_acc_batches)}\")\n",
        "        print(f\"Epoch {step + 1}/{n_epochs}, Validation: Cost {np.mean(val_cost_batches)}, Acc {np.mean(val_acc_batches)}\")\n",
        "        print(\"=-=\" * 10)\n",
        "        save_weights_to_csv(weights, weights_last, step+1, file_name=\"weights.csv\")\n",
        "    return dict(\n",
        "        n_train=[N_TRAIN] * n_epochs,\n",
        "        step=np.arange(1, n_epochs + 1, dtype=int).tolist(),\n",
        "        train_cost=[c.astype(float) for c in train_cost_epochs],\n",
        "        train_acc=[c.astype(float) for c in train_acc_epochs],\n",
        "        val_cost=[c.astype(float) for c in val_cost_epochs],\n",
        "        val_acc=[c.astype(float) for c in val_acc_epochs],\n",
        "    )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTZnVAUldvDs",
        "outputId": "1f2a163c-a80c-43b2-a02d-d614fa58aa46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training at Epoch 1/50, Train batches 46, Val batches 3......\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/jax/_src/lax/lax.py:3373: ComplexWarning: Casting complex values to real discards the imaginary part\n",
            "  x_bar = _convert_element_type(x_bar, x.aval.dtype, x.aval.weak_type)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training at Epoch 1/50, Batch 5, Cost 2.00277904285205, Acc 0.589. Time 2.5157265663146973\n",
            "Training at Epoch 1/50, Batch 10, Cost 1.8410726847924042, Acc 0.72. Time 2.5398690700531006\n",
            "Training at Epoch 1/50, Batch 15, Cost 1.7726368326674282, Acc 0.725. Time 2.582477331161499\n",
            "Training at Epoch 1/50, Batch 20, Cost 1.7378079649296618, Acc 0.73. Time 2.5862505435943604\n",
            "Training at Epoch 1/50, Batch 25, Cost 1.708522965611464, Acc 0.729. Time 2.554138422012329\n",
            "Training at Epoch 1/50, Batch 30, Cost 1.6833010321633277, Acc 0.774. Time 2.5351033210754395\n",
            "Training at Epoch 1/50, Batch 35, Cost 1.673480824670964, Acc 0.745. Time 2.541529417037964\n",
            "Training at Epoch 1/50, Batch 40, Cost 1.6591233116307977, Acc 0.769. Time 2.437004327774048\n",
            "Training at Epoch 1/50, Batch 45, Cost 1.6511274314042612, Acc 0.781. Time 2.4917376041412354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/jax/_src/lax/lax.py:3373: ComplexWarning: Casting complex values to real discards the imaginary part\n",
            "  x_bar = _convert_element_type(x_bar, x.aval.dtype, x.aval.weak_type)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "......\n",
            "Epoch 1/50, Train: Cost 1.7548520389302376, Acc 0.6925\n",
            "Epoch 1/50, Validation: Cost 1.6457284895661173, Acc 0.7776666666666667\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 2/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 2/50, Batch 5, Cost 1.6434963802639768, Acc 0.772. Time 2.5180001258850098\n",
            "Training at Epoch 2/50, Batch 10, Cost 1.64474446948147, Acc 0.757. Time 2.6245713233947754\n",
            "Training at Epoch 2/50, Batch 15, Cost 1.6399485492826655, Acc 0.777. Time 2.5805742740631104\n",
            "Training at Epoch 2/50, Batch 20, Cost 1.6307438813351427, Acc 0.773. Time 2.5573649406433105\n",
            "Training at Epoch 2/50, Batch 25, Cost 1.633704947974244, Acc 0.771. Time 2.521503448486328\n",
            "Training at Epoch 2/50, Batch 30, Cost 1.6210841851475988, Acc 0.792. Time 2.524085521697998\n",
            "Training at Epoch 2/50, Batch 35, Cost 1.6274463899592675, Acc 0.786. Time 2.533053398132324\n",
            "Training at Epoch 2/50, Batch 40, Cost 1.6120031729499567, Acc 0.812. Time 2.541379928588867\n",
            "Training at Epoch 2/50, Batch 45, Cost 1.6153310596888875, Acc 0.794. Time 2.5592126846313477\n",
            "......\n",
            "Epoch 2/50, Train: Cost 1.6336634677317472, Acc 0.7757173913043478\n",
            "Epoch 2/50, Validation: Cost 1.6208271715619669, Acc 0.7836666666666666\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 3/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 3/50, Batch 5, Cost 1.6235430857819917, Acc 0.771. Time 2.553173542022705\n",
            "Training at Epoch 3/50, Batch 10, Cost 1.6060833842794273, Acc 0.808. Time 2.558011770248413\n",
            "Training at Epoch 3/50, Batch 15, Cost 1.6301981815315298, Acc 0.769. Time 2.5511128902435303\n",
            "Training at Epoch 3/50, Batch 20, Cost 1.6020781751333661, Acc 0.794. Time 2.540717363357544\n",
            "Training at Epoch 3/50, Batch 25, Cost 1.6011422032210125, Acc 0.803. Time 2.4534990787506104\n",
            "Training at Epoch 3/50, Batch 30, Cost 1.6126276271609294, Acc 0.798. Time 2.4817588329315186\n",
            "Training at Epoch 3/50, Batch 35, Cost 1.6161453252966869, Acc 0.781. Time 2.5662853717803955\n",
            "Training at Epoch 3/50, Batch 40, Cost 1.6368306157388661, Acc 0.747. Time 2.5674972534179688\n",
            "Training at Epoch 3/50, Batch 45, Cost 1.638845149210458, Acc 0.76. Time 2.5643091201782227\n",
            "......\n",
            "Epoch 3/50, Train: Cost 1.618877211163723, Acc 0.7826159420289854\n",
            "Epoch 3/50, Validation: Cost 1.6151455230347302, Acc 0.7883333333333334\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 4/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 4/50, Batch 5, Cost 1.6159590997769318, Acc 0.789. Time 2.520374298095703\n",
            "Training at Epoch 4/50, Batch 10, Cost 1.6206206767842715, Acc 0.78. Time 2.565732717514038\n",
            "Training at Epoch 4/50, Batch 15, Cost 1.6005799146032513, Acc 0.794. Time 2.5491487979888916\n",
            "Training at Epoch 4/50, Batch 20, Cost 1.6144527093399952, Acc 0.795. Time 2.5481410026550293\n",
            "Training at Epoch 4/50, Batch 25, Cost 1.6050202943479512, Acc 0.805. Time 2.543397903442383\n",
            "Training at Epoch 4/50, Batch 30, Cost 1.6071164734647956, Acc 0.786. Time 2.5518195629119873\n",
            "Training at Epoch 4/50, Batch 35, Cost 1.6074280238086913, Acc 0.794. Time 2.5453386306762695\n",
            "Training at Epoch 4/50, Batch 40, Cost 1.5928359627961468, Acc 0.802. Time 2.5485754013061523\n",
            "Training at Epoch 4/50, Batch 45, Cost 1.6081308236036542, Acc 0.79. Time 2.539008617401123\n",
            "......\n",
            "Epoch 4/50, Train: Cost 1.609545873199201, Acc 0.7877391304347825\n",
            "Epoch 4/50, Validation: Cost 1.6027969026326103, Acc 0.7963333333333334\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 5/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 5/50, Batch 5, Cost 1.6031475955303363, Acc 0.796. Time 2.552305221557617\n",
            "Training at Epoch 5/50, Batch 10, Cost 1.6105138036832687, Acc 0.785. Time 2.5437071323394775\n",
            "Training at Epoch 5/50, Batch 15, Cost 1.6261816844159989, Acc 0.763. Time 2.532059907913208\n",
            "Training at Epoch 5/50, Batch 20, Cost 1.6122471931238889, Acc 0.774. Time 2.5383079051971436\n",
            "Training at Epoch 5/50, Batch 25, Cost 1.6144313514179789, Acc 0.779. Time 2.4396088123321533\n",
            "Training at Epoch 5/50, Batch 30, Cost 1.6073688217414532, Acc 0.783. Time 2.495562791824341\n",
            "Training at Epoch 5/50, Batch 35, Cost 1.596829516705277, Acc 0.798. Time 2.552429437637329\n",
            "Training at Epoch 5/50, Batch 40, Cost 1.607014385859849, Acc 0.789. Time 2.560506582260132\n",
            "Training at Epoch 5/50, Batch 45, Cost 1.6001955978107254, Acc 0.785. Time 2.5560011863708496\n",
            "......\n",
            "Epoch 5/50, Train: Cost 1.6042768418537292, Acc 0.7905724637681157\n",
            "Epoch 5/50, Validation: Cost 1.5983751879750179, Acc 0.7968333333333334\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 6/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 6/50, Batch 5, Cost 1.586041282382661, Acc 0.803. Time 2.505352020263672\n",
            "Training at Epoch 6/50, Batch 10, Cost 1.605079641935367, Acc 0.789. Time 2.5698013305664062\n",
            "Training at Epoch 6/50, Batch 15, Cost 1.6185368090775691, Acc 0.774. Time 2.550736427307129\n",
            "Training at Epoch 6/50, Batch 20, Cost 1.5805355205147933, Acc 0.829. Time 2.4571738243103027\n",
            "Training at Epoch 6/50, Batch 25, Cost 1.602335381669779, Acc 0.792. Time 2.5503251552581787\n",
            "Training at Epoch 6/50, Batch 30, Cost 1.6182828615991016, Acc 0.77. Time 2.5469753742218018\n",
            "Training at Epoch 6/50, Batch 35, Cost 1.5955481145092327, Acc 0.798. Time 2.5402636528015137\n",
            "Training at Epoch 6/50, Batch 40, Cost 1.612522973858594, Acc 0.775. Time 2.540135622024536\n",
            "Training at Epoch 6/50, Batch 45, Cost 1.6008901398523585, Acc 0.792. Time 2.5312864780426025\n",
            "......\n",
            "Epoch 6/50, Train: Cost 1.6019176577256737, Acc 0.7926086956521741\n",
            "Epoch 6/50, Validation: Cost 1.5984182768262043, Acc 0.7968333333333334\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 7/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 7/50, Batch 5, Cost 1.5949428280244569, Acc 0.79. Time 2.530698537826538\n",
            "Training at Epoch 7/50, Batch 10, Cost 1.6030287996595036, Acc 0.781. Time 2.5485024452209473\n",
            "Training at Epoch 7/50, Batch 15, Cost 1.5919541639534054, Acc 0.807. Time 2.5436387062072754\n",
            "Training at Epoch 7/50, Batch 20, Cost 1.6039778162742384, Acc 0.778. Time 2.4386849403381348\n",
            "Training at Epoch 7/50, Batch 25, Cost 1.5892751374616043, Acc 0.801. Time 2.4467203617095947\n",
            "Training at Epoch 7/50, Batch 30, Cost 1.5960941532469202, Acc 0.798. Time 2.4635894298553467\n",
            "Training at Epoch 7/50, Batch 35, Cost 1.6177598937511644, Acc 0.76. Time 2.566894054412842\n",
            "Training at Epoch 7/50, Batch 40, Cost 1.607867000116867, Acc 0.793. Time 2.566405773162842\n",
            "Training at Epoch 7/50, Batch 45, Cost 1.5988285705734064, Acc 0.797. Time 2.5522189140319824\n",
            "......\n",
            "Epoch 7/50, Train: Cost 1.599195052343791, Acc 0.7937826086956521\n",
            "Epoch 7/50, Validation: Cost 1.6002565088447183, Acc 0.8028333333333334\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 8/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 8/50, Batch 5, Cost 1.5904593467176147, Acc 0.812. Time 2.4919872283935547\n",
            "Training at Epoch 8/50, Batch 10, Cost 1.59825413619474, Acc 0.786. Time 2.5505716800689697\n",
            "Training at Epoch 8/50, Batch 15, Cost 1.5960279343319626, Acc 0.802. Time 2.5529117584228516\n",
            "Training at Epoch 8/50, Batch 20, Cost 1.591249876848191, Acc 0.798. Time 2.53593111038208\n",
            "Training at Epoch 8/50, Batch 25, Cost 1.5999766051535858, Acc 0.788. Time 2.546353578567505\n",
            "Training at Epoch 8/50, Batch 30, Cost 1.5868341187065806, Acc 0.806. Time 2.4325714111328125\n",
            "Training at Epoch 8/50, Batch 35, Cost 1.608164530352331, Acc 0.786. Time 2.527089834213257\n",
            "Training at Epoch 8/50, Batch 40, Cost 1.5976293382741267, Acc 0.798. Time 2.5204672813415527\n",
            "Training at Epoch 8/50, Batch 45, Cost 1.6074313476280715, Acc 0.785. Time 2.5457842350006104\n",
            "......\n",
            "Epoch 8/50, Train: Cost 1.5988870178771661, Acc 0.7946376811594205\n",
            "Epoch 8/50, Validation: Cost 1.5947820512574007, Acc 0.7988333333333334\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 9/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 9/50, Batch 5, Cost 1.6064980303334917, Acc 0.783. Time 2.539421558380127\n",
            "Training at Epoch 9/50, Batch 10, Cost 1.601530301634048, Acc 0.793. Time 2.546389579772949\n",
            "Training at Epoch 9/50, Batch 15, Cost 1.5975076056550543, Acc 0.791. Time 2.5465099811553955\n",
            "Training at Epoch 9/50, Batch 20, Cost 1.5950908516390159, Acc 0.796. Time 2.459660768508911\n",
            "Training at Epoch 9/50, Batch 25, Cost 1.5856863445460365, Acc 0.821. Time 2.454627752304077\n",
            "Training at Epoch 9/50, Batch 30, Cost 1.5914449294150508, Acc 0.798. Time 2.5671513080596924\n",
            "Training at Epoch 9/50, Batch 35, Cost 1.599308109003411, Acc 0.776. Time 2.5646777153015137\n",
            "Training at Epoch 9/50, Batch 40, Cost 1.605663087311757, Acc 0.757. Time 2.5489139556884766\n",
            "Training at Epoch 9/50, Batch 45, Cost 1.5987906907813558, Acc 0.795. Time 2.558391571044922\n",
            "......\n",
            "Epoch 9/50, Train: Cost 1.5956920652256112, Acc 0.7949927536231887\n",
            "Epoch 9/50, Validation: Cost 1.5919969052114604, Acc 0.8003333333333335\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 10/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 10/50, Batch 5, Cost 1.5943570979331623, Acc 0.793. Time 2.4785144329071045\n",
            "Training at Epoch 10/50, Batch 10, Cost 1.6009596037742806, Acc 0.802. Time 2.560290575027466\n",
            "Training at Epoch 10/50, Batch 15, Cost 1.582307647445486, Acc 0.812. Time 2.558256149291992\n",
            "Training at Epoch 10/50, Batch 20, Cost 1.5908083589967625, Acc 0.81. Time 2.5322299003601074\n",
            "Training at Epoch 10/50, Batch 25, Cost 1.6016210243842945, Acc 0.788. Time 2.5551395416259766\n",
            "Training at Epoch 10/50, Batch 30, Cost 1.5884360526363561, Acc 0.796. Time 2.5512123107910156\n",
            "Training at Epoch 10/50, Batch 35, Cost 1.5968934530738914, Acc 0.789. Time 2.5531115531921387\n",
            "Training at Epoch 10/50, Batch 40, Cost 1.598135081515443, Acc 0.799. Time 2.451294183731079\n",
            "Training at Epoch 10/50, Batch 45, Cost 1.6008134320567884, Acc 0.773. Time 2.545499324798584\n",
            "......\n",
            "Epoch 10/50, Train: Cost 1.5957226818278196, Acc 0.7961884057971015\n",
            "Epoch 10/50, Validation: Cost 1.594810701229582, Acc 0.7996666666666666\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 11/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 11/50, Batch 5, Cost 1.6064795304377486, Acc 0.791. Time 2.5535659790039062\n",
            "Training at Epoch 11/50, Batch 10, Cost 1.594312227591966, Acc 0.795. Time 2.54931378364563\n",
            "Training at Epoch 11/50, Batch 15, Cost 1.6064504627850829, Acc 0.782. Time 2.5499253273010254\n",
            "Training at Epoch 11/50, Batch 20, Cost 1.6096117322682628, Acc 0.797. Time 2.552044153213501\n",
            "Training at Epoch 11/50, Batch 25, Cost 1.585139852026376, Acc 0.805. Time 2.4472432136535645\n",
            "Training at Epoch 11/50, Batch 30, Cost 1.5986689712263238, Acc 0.795. Time 2.504600763320923\n",
            "Training at Epoch 11/50, Batch 35, Cost 1.5911915081146533, Acc 0.819. Time 2.554218053817749\n",
            "Training at Epoch 11/50, Batch 40, Cost 1.598963073356928, Acc 0.797. Time 2.555161476135254\n",
            "Training at Epoch 11/50, Batch 45, Cost 1.5992804766969007, Acc 0.78. Time 2.547924518585205\n",
            "......\n",
            "Epoch 11/50, Train: Cost 1.595070288547127, Acc 0.7968478260869566\n",
            "Epoch 11/50, Validation: Cost 1.5916684769288143, Acc 0.7985000000000001\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 12/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 12/50, Batch 5, Cost 1.6010419743139597, Acc 0.786. Time 2.497445583343506\n",
            "Training at Epoch 12/50, Batch 10, Cost 1.5915954943262587, Acc 0.801. Time 2.533043622970581\n",
            "Training at Epoch 12/50, Batch 15, Cost 1.6051629089262966, Acc 0.781. Time 2.5600366592407227\n",
            "Training at Epoch 12/50, Batch 20, Cost 1.600139006271704, Acc 0.783. Time 2.558464288711548\n",
            "Training at Epoch 12/50, Batch 25, Cost 1.5888086897992284, Acc 0.804. Time 2.5547051429748535\n",
            "Training at Epoch 12/50, Batch 30, Cost 1.5886596083216582, Acc 0.803. Time 2.547896385192871\n",
            "Training at Epoch 12/50, Batch 35, Cost 1.5933882475759404, Acc 0.8. Time 2.5546112060546875\n",
            "Training at Epoch 12/50, Batch 40, Cost 1.6010588948661402, Acc 0.791. Time 2.548532485961914\n",
            "Training at Epoch 12/50, Batch 45, Cost 1.5970736487200838, Acc 0.792. Time 2.5461905002593994\n",
            "......\n",
            "Epoch 12/50, Train: Cost 1.593573313633874, Acc 0.7974057971014494\n",
            "Epoch 12/50, Validation: Cost 1.5914793054623593, Acc 0.8035\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 13/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 13/50, Batch 5, Cost 1.60405478594162, Acc 0.796. Time 2.545159339904785\n",
            "Training at Epoch 13/50, Batch 10, Cost 1.5888832028764162, Acc 0.805. Time 2.534226417541504\n",
            "Training at Epoch 13/50, Batch 15, Cost 1.5908753335619512, Acc 0.802. Time 2.4048047065734863\n",
            "Training at Epoch 13/50, Batch 20, Cost 1.6008831278315814, Acc 0.791. Time 2.4537336826324463\n",
            "Training at Epoch 13/50, Batch 25, Cost 1.5965189761143743, Acc 0.801. Time 2.498755693435669\n",
            "Training at Epoch 13/50, Batch 30, Cost 1.6051564919917423, Acc 0.785. Time 2.5607354640960693\n",
            "Training at Epoch 13/50, Batch 35, Cost 1.5880549608995427, Acc 0.8. Time 2.5616469383239746\n",
            "Training at Epoch 13/50, Batch 40, Cost 1.6051109983612104, Acc 0.78. Time 2.55027437210083\n",
            "Training at Epoch 13/50, Batch 45, Cost 1.5880565519519212, Acc 0.819. Time 2.5522589683532715\n",
            "......\n",
            "Epoch 13/50, Train: Cost 1.5931659927479729, Acc 0.7985797101449276\n",
            "Epoch 13/50, Validation: Cost 1.5905981423700901, Acc 0.7988333333333334\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 14/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 14/50, Batch 5, Cost 1.584979394949632, Acc 0.802. Time 2.5085482597351074\n",
            "Training at Epoch 14/50, Batch 10, Cost 1.6036344444479185, Acc 0.788. Time 2.548616409301758\n",
            "Training at Epoch 14/50, Batch 15, Cost 1.606637924135035, Acc 0.77. Time 2.5484955310821533\n",
            "Training at Epoch 14/50, Batch 20, Cost 1.5741101242595277, Acc 0.814. Time 2.5529775619506836\n",
            "Training at Epoch 14/50, Batch 25, Cost 1.5953338550777634, Acc 0.795. Time 2.5412120819091797\n",
            "Training at Epoch 14/50, Batch 30, Cost 1.5932779083538788, Acc 0.783. Time 2.545604944229126\n",
            "Training at Epoch 14/50, Batch 35, Cost 1.5884672107846745, Acc 0.806. Time 2.5509743690490723\n",
            "Training at Epoch 14/50, Batch 40, Cost 1.614864000745064, Acc 0.764. Time 2.5473358631134033\n",
            "Training at Epoch 14/50, Batch 45, Cost 1.586516797845285, Acc 0.802. Time 2.4665188789367676\n",
            "......\n",
            "Epoch 14/50, Train: Cost 1.591398690111976, Acc 0.7973985507246377\n",
            "Epoch 14/50, Validation: Cost 1.5895467196519977, Acc 0.801\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 15/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 15/50, Batch 5, Cost 1.5968441260073067, Acc 0.791. Time 2.53422474861145\n",
            "Training at Epoch 15/50, Batch 10, Cost 1.6003923551551293, Acc 0.775. Time 2.516417980194092\n",
            "Training at Epoch 15/50, Batch 15, Cost 1.6008197936417219, Acc 0.783. Time 2.4466450214385986\n",
            "Training at Epoch 15/50, Batch 20, Cost 1.5842701688385272, Acc 0.807. Time 2.488398313522339\n",
            "Training at Epoch 15/50, Batch 25, Cost 1.574975257753833, Acc 0.82. Time 2.564483165740967\n",
            "Training at Epoch 15/50, Batch 30, Cost 1.603274205836051, Acc 0.782. Time 2.5594112873077393\n",
            "Training at Epoch 15/50, Batch 35, Cost 1.6006839175973762, Acc 0.787. Time 2.5404915809631348\n",
            "Training at Epoch 15/50, Batch 40, Cost 1.5972298232824664, Acc 0.786. Time 2.5505402088165283\n",
            "Training at Epoch 15/50, Batch 45, Cost 1.5932174566516184, Acc 0.8. Time 2.549950122833252\n",
            "......\n",
            "Epoch 15/50, Train: Cost 1.59179806388789, Acc 0.7991884057971015\n",
            "Epoch 15/50, Validation: Cost 1.5910741759023461, Acc 0.8016666666666667\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 16/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 16/50, Batch 5, Cost 1.5835583736291536, Acc 0.81. Time 2.5459258556365967\n",
            "Training at Epoch 16/50, Batch 10, Cost 1.5913033208885323, Acc 0.79. Time 2.5491316318511963\n",
            "Training at Epoch 16/50, Batch 15, Cost 1.5726642862320017, Acc 0.829. Time 2.5330593585968018\n",
            "Training at Epoch 16/50, Batch 20, Cost 1.5836647696774244, Acc 0.8. Time 2.5490469932556152\n",
            "Training at Epoch 16/50, Batch 25, Cost 1.5968680331158096, Acc 0.792. Time 2.5468592643737793\n",
            "Training at Epoch 16/50, Batch 30, Cost 1.5950154113695936, Acc 0.797. Time 2.5542147159576416\n",
            "Training at Epoch 16/50, Batch 35, Cost 1.5960883410695668, Acc 0.802. Time 2.538755178451538\n",
            "Training at Epoch 16/50, Batch 40, Cost 1.579920610300939, Acc 0.818. Time 2.4622063636779785\n",
            "Training at Epoch 16/50, Batch 45, Cost 1.592110936999085, Acc 0.795. Time 2.504082679748535\n",
            "......\n",
            "Epoch 16/50, Train: Cost 1.5936662714202097, Acc 0.7983695652173913\n",
            "Epoch 16/50, Validation: Cost 1.591291492003985, Acc 0.8028333333333334\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 17/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 17/50, Batch 5, Cost 1.5912338708737783, Acc 0.803. Time 2.4990732669830322\n",
            "Training at Epoch 17/50, Batch 10, Cost 1.5931006781806778, Acc 0.799. Time 2.4276885986328125\n",
            "Training at Epoch 17/50, Batch 15, Cost 1.5879074676204283, Acc 0.802. Time 2.4946300983428955\n",
            "Training at Epoch 17/50, Batch 20, Cost 1.6033931729984006, Acc 0.777. Time 2.546672821044922\n",
            "Training at Epoch 17/50, Batch 25, Cost 1.602835615246351, Acc 0.787. Time 2.5551202297210693\n",
            "Training at Epoch 17/50, Batch 30, Cost 1.5982463321251748, Acc 0.796. Time 2.553117275238037\n",
            "Training at Epoch 17/50, Batch 35, Cost 1.5904611470555474, Acc 0.793. Time 2.5438127517700195\n",
            "Training at Epoch 17/50, Batch 40, Cost 1.589099279366798, Acc 0.803. Time 2.5485594272613525\n",
            "Training at Epoch 17/50, Batch 45, Cost 1.584418990758955, Acc 0.796. Time 2.5471673011779785\n",
            "......\n",
            "Epoch 17/50, Train: Cost 1.5915937607522432, Acc 0.7988333333333333\n",
            "Epoch 17/50, Validation: Cost 1.5912973574749723, Acc 0.8039999999999999\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 18/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 18/50, Batch 5, Cost 1.593587174906424, Acc 0.81. Time 2.550130605697632\n",
            "Training at Epoch 18/50, Batch 10, Cost 1.6055465780342215, Acc 0.77. Time 2.5463926792144775\n",
            "Training at Epoch 18/50, Batch 15, Cost 1.5898470688840705, Acc 0.801. Time 2.5502099990844727\n",
            "Training at Epoch 18/50, Batch 20, Cost 1.5961077172136307, Acc 0.794. Time 2.5513722896575928\n",
            "Training at Epoch 18/50, Batch 25, Cost 1.5956699671039518, Acc 0.799. Time 2.550532341003418\n",
            "Training at Epoch 18/50, Batch 30, Cost 1.6011733741605256, Acc 0.781. Time 2.451460361480713\n",
            "Training at Epoch 18/50, Batch 35, Cost 1.585699873592344, Acc 0.799. Time 2.44838547706604\n",
            "Training at Epoch 18/50, Batch 40, Cost 1.577208856868726, Acc 0.823. Time 2.5006659030914307\n",
            "Training at Epoch 18/50, Batch 45, Cost 1.5980810909149878, Acc 0.795. Time 2.559372663497925\n",
            "......\n",
            "Epoch 18/50, Train: Cost 1.5910840526766004, Acc 0.7983260869565217\n",
            "Epoch 18/50, Validation: Cost 1.593381835024929, Acc 0.8033333333333333\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 19/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 19/50, Batch 5, Cost 1.5881247682007351, Acc 0.794. Time 2.4561667442321777\n",
            "Training at Epoch 19/50, Batch 10, Cost 1.5800238608515278, Acc 0.816. Time 2.508558511734009\n",
            "Training at Epoch 19/50, Batch 15, Cost 1.5991498237791006, Acc 0.789. Time 2.5536911487579346\n",
            "Training at Epoch 19/50, Batch 20, Cost 1.595400760494329, Acc 0.801. Time 2.562727451324463\n",
            "Training at Epoch 19/50, Batch 25, Cost 1.587241052624678, Acc 0.81. Time 2.5661704540252686\n",
            "Training at Epoch 19/50, Batch 30, Cost 1.595985423876124, Acc 0.793. Time 2.536679744720459\n",
            "Training at Epoch 19/50, Batch 35, Cost 1.589043482074292, Acc 0.799. Time 2.539654493331909\n",
            "Training at Epoch 19/50, Batch 40, Cost 1.5814043488885385, Acc 0.812. Time 2.5440852642059326\n",
            "Training at Epoch 19/50, Batch 45, Cost 1.6077689952375072, Acc 0.769. Time 2.5459189414978027\n",
            "......\n",
            "Epoch 19/50, Train: Cost 1.5899537115799731, Acc 0.8001086956521739\n",
            "Epoch 19/50, Validation: Cost 1.5907698518206397, Acc 0.8026666666666668\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 20/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 20/50, Batch 5, Cost 1.570831487600264, Acc 0.813. Time 2.5465917587280273\n",
            "Training at Epoch 20/50, Batch 10, Cost 1.5974334233430563, Acc 0.799. Time 2.539731025695801\n",
            "Training at Epoch 20/50, Batch 15, Cost 1.592854936834039, Acc 0.801. Time 2.4495689868927\n",
            "Training at Epoch 20/50, Batch 20, Cost 1.5884870801070599, Acc 0.803. Time 2.425290107727051\n",
            "Training at Epoch 20/50, Batch 25, Cost 1.599604666974814, Acc 0.792. Time 2.4657931327819824\n",
            "Training at Epoch 20/50, Batch 30, Cost 1.5995079192688821, Acc 0.793. Time 2.5514748096466064\n",
            "Training at Epoch 20/50, Batch 35, Cost 1.5977567939506987, Acc 0.79. Time 2.560706615447998\n",
            "Training at Epoch 20/50, Batch 40, Cost 1.5923903558384591, Acc 0.801. Time 2.5430898666381836\n",
            "Training at Epoch 20/50, Batch 45, Cost 1.594609430376494, Acc 0.798. Time 2.5435190200805664\n",
            "......\n",
            "Epoch 20/50, Train: Cost 1.590920226736476, Acc 0.8002463768115945\n",
            "Epoch 20/50, Validation: Cost 1.590467592254737, Acc 0.8013333333333333\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 21/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 21/50, Batch 5, Cost 1.5863749488723655, Acc 0.806. Time 2.5425240993499756\n",
            "Training at Epoch 21/50, Batch 10, Cost 1.586729429112186, Acc 0.812. Time 2.545071840286255\n",
            "Training at Epoch 21/50, Batch 15, Cost 1.5918628376158896, Acc 0.801. Time 2.546787977218628\n",
            "Training at Epoch 21/50, Batch 20, Cost 1.5999550728625578, Acc 0.781. Time 2.5298211574554443\n",
            "Training at Epoch 21/50, Batch 25, Cost 1.5886865097221188, Acc 0.795. Time 2.5458076000213623\n",
            "Training at Epoch 21/50, Batch 30, Cost 1.5788342428426403, Acc 0.809. Time 2.546067237854004\n",
            "Training at Epoch 21/50, Batch 35, Cost 1.5653265171389252, Acc 0.824. Time 2.446955442428589\n",
            "Training at Epoch 21/50, Batch 40, Cost 1.5904089998608664, Acc 0.805. Time 2.4302263259887695\n",
            "Training at Epoch 21/50, Batch 45, Cost 1.5733702914672834, Acc 0.815. Time 2.471872091293335\n",
            "......\n",
            "Epoch 21/50, Train: Cost 1.589024016937587, Acc 0.7987826086956522\n",
            "Epoch 21/50, Validation: Cost 1.5867673691610547, Acc 0.8031666666666667\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 22/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 22/50, Batch 5, Cost 1.598592842957104, Acc 0.785. Time 2.432464838027954\n",
            "Training at Epoch 22/50, Batch 10, Cost 1.5905174969878557, Acc 0.792. Time 2.498167037963867\n",
            "Training at Epoch 22/50, Batch 15, Cost 1.5942250652251688, Acc 0.792. Time 2.5496909618377686\n",
            "Training at Epoch 22/50, Batch 20, Cost 1.5775184170877217, Acc 0.813. Time 2.5614778995513916\n",
            "Training at Epoch 22/50, Batch 25, Cost 1.5903582150272138, Acc 0.807. Time 2.5440478324890137\n",
            "Training at Epoch 22/50, Batch 30, Cost 1.583319010008602, Acc 0.817. Time 2.5301198959350586\n",
            "Training at Epoch 22/50, Batch 35, Cost 1.6011269038874179, Acc 0.783. Time 2.549856662750244\n",
            "Training at Epoch 22/50, Batch 40, Cost 1.5892947873821563, Acc 0.786. Time 2.5488650798797607\n",
            "Training at Epoch 22/50, Batch 45, Cost 1.5898712897659388, Acc 0.803. Time 2.5543510913848877\n",
            "......\n",
            "Epoch 22/50, Train: Cost 1.5887580753700756, Acc 0.800623188405797\n",
            "Epoch 22/50, Validation: Cost 1.5863587767280887, Acc 0.8011666666666667\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 23/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 23/50, Batch 5, Cost 1.5886007979195884, Acc 0.8. Time 2.5179505348205566\n",
            "Training at Epoch 23/50, Batch 10, Cost 1.587915496256623, Acc 0.79. Time 2.5474846363067627\n",
            "Training at Epoch 23/50, Batch 15, Cost 1.6099345311799518, Acc 0.784. Time 2.554708480834961\n",
            "Training at Epoch 23/50, Batch 20, Cost 1.5962435921526226, Acc 0.804. Time 2.5395567417144775\n",
            "Training at Epoch 23/50, Batch 25, Cost 1.5898381620404796, Acc 0.799. Time 2.5515027046203613\n",
            "Training at Epoch 23/50, Batch 30, Cost 1.5890702839405948, Acc 0.792. Time 2.4347944259643555\n",
            "Training at Epoch 23/50, Batch 35, Cost 1.5882633646142503, Acc 0.796. Time 2.5001935958862305\n",
            "Training at Epoch 23/50, Batch 40, Cost 1.593540231008779, Acc 0.794. Time 2.472548007965088\n",
            "Training at Epoch 23/50, Batch 45, Cost 1.5853189087761634, Acc 0.803. Time 2.5531177520751953\n",
            "......\n",
            "Epoch 23/50, Train: Cost 1.5879916419941076, Acc 0.8001014492753622\n",
            "Epoch 23/50, Validation: Cost 1.5879641198748908, Acc 0.8035\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 24/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 24/50, Batch 5, Cost 1.5913776479374104, Acc 0.793. Time 2.449153184890747\n",
            "Training at Epoch 24/50, Batch 10, Cost 1.5613666822543806, Acc 0.839. Time 2.555387496948242\n",
            "Training at Epoch 24/50, Batch 15, Cost 1.5956140376776151, Acc 0.788. Time 2.554098129272461\n",
            "Training at Epoch 24/50, Batch 20, Cost 1.5778724281533922, Acc 0.81. Time 2.553054094314575\n",
            "Training at Epoch 24/50, Batch 25, Cost 1.5869670642151772, Acc 0.8. Time 2.5314362049102783\n",
            "Training at Epoch 24/50, Batch 30, Cost 1.6062249510604256, Acc 0.774. Time 2.544896364212036\n",
            "Training at Epoch 24/50, Batch 35, Cost 1.5997901206369127, Acc 0.797. Time 2.543257236480713\n",
            "Training at Epoch 24/50, Batch 40, Cost 1.586596539201403, Acc 0.795. Time 2.5439131259918213\n",
            "Training at Epoch 24/50, Batch 45, Cost 1.5915589209438918, Acc 0.8. Time 2.530074119567871\n",
            "......\n",
            "Epoch 24/50, Train: Cost 1.5887011389783976, Acc 0.8007463768115942\n",
            "Epoch 24/50, Validation: Cost 1.5910685942696936, Acc 0.7991666666666667\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 25/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 25/50, Batch 5, Cost 1.5884458845879017, Acc 0.793. Time 2.5458154678344727\n",
            "Training at Epoch 25/50, Batch 10, Cost 1.5677311976525647, Acc 0.825. Time 2.5458781719207764\n",
            "Training at Epoch 25/50, Batch 15, Cost 1.5922238779211002, Acc 0.795. Time 2.5437564849853516\n",
            "Training at Epoch 25/50, Batch 20, Cost 1.5803664560187876, Acc 0.808. Time 2.541111707687378\n",
            "Training at Epoch 25/50, Batch 25, Cost 1.6009331615046587, Acc 0.781. Time 2.4291188716888428\n",
            "Training at Epoch 25/50, Batch 30, Cost 1.5845098206659307, Acc 0.817. Time 2.450042963027954\n",
            "Training at Epoch 25/50, Batch 35, Cost 1.5943447118292797, Acc 0.8. Time 2.4527969360351562\n",
            "Training at Epoch 25/50, Batch 40, Cost 1.5931762523442816, Acc 0.797. Time 2.5637245178222656\n",
            "Training at Epoch 25/50, Batch 45, Cost 1.597046985234273, Acc 0.796. Time 2.5646939277648926\n",
            "......\n",
            "Epoch 25/50, Train: Cost 1.5885586434154035, Acc 0.8007463768115942\n",
            "Epoch 25/50, Validation: Cost 1.5883116355365108, Acc 0.8063333333333333\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 26/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 26/50, Batch 5, Cost 1.5909431819472883, Acc 0.809. Time 2.4916865825653076\n",
            "Training at Epoch 26/50, Batch 10, Cost 1.5764811708614452, Acc 0.811. Time 2.555720090866089\n",
            "Training at Epoch 26/50, Batch 15, Cost 1.5622483965999883, Acc 0.829. Time 2.5584394931793213\n",
            "Training at Epoch 26/50, Batch 20, Cost 1.5922230570928186, Acc 0.789. Time 2.5569756031036377\n",
            "Training at Epoch 26/50, Batch 25, Cost 1.5947176781101606, Acc 0.787. Time 2.5489068031311035\n",
            "Training at Epoch 26/50, Batch 30, Cost 1.5844809127553405, Acc 0.797. Time 2.545919418334961\n",
            "Training at Epoch 26/50, Batch 35, Cost 1.5785109383423086, Acc 0.805. Time 2.5525693893432617\n",
            "Training at Epoch 26/50, Batch 40, Cost 1.5766488738894928, Acc 0.799. Time 2.5452120304107666\n",
            "Training at Epoch 26/50, Batch 45, Cost 1.5865556100340468, Acc 0.803. Time 2.545787811279297\n",
            "......\n",
            "Epoch 26/50, Train: Cost 1.5875064144081872, Acc 0.800021739130435\n",
            "Epoch 26/50, Validation: Cost 1.584628806667795, Acc 0.8026666666666668\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 27/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 27/50, Batch 5, Cost 1.5907071566952573, Acc 0.803. Time 2.5451037883758545\n",
            "Training at Epoch 27/50, Batch 10, Cost 1.5836818828254633, Acc 0.807. Time 2.5512168407440186\n",
            "Training at Epoch 27/50, Batch 15, Cost 1.5933103694101065, Acc 0.802. Time 2.5471529960632324\n",
            "Training at Epoch 27/50, Batch 20, Cost 1.5891672919750859, Acc 0.808. Time 2.548332691192627\n",
            "Training at Epoch 27/50, Batch 25, Cost 1.5882483844365392, Acc 0.798. Time 2.421058177947998\n",
            "Training at Epoch 27/50, Batch 30, Cost 1.5797549771999082, Acc 0.817. Time 2.453399896621704\n",
            "Training at Epoch 27/50, Batch 35, Cost 1.586391233226705, Acc 0.792. Time 2.559581756591797\n",
            "Training at Epoch 27/50, Batch 40, Cost 1.5856326454897245, Acc 0.809. Time 2.542358875274658\n",
            "Training at Epoch 27/50, Batch 45, Cost 1.5939973678838306, Acc 0.783. Time 2.5549728870391846\n",
            "......\n",
            "Epoch 27/50, Train: Cost 1.586965299876356, Acc 0.8002246376811594\n",
            "Epoch 27/50, Validation: Cost 1.5888853699157452, Acc 0.8038333333333334\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 28/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 28/50, Batch 5, Cost 1.5794658896319218, Acc 0.806. Time 2.4991092681884766\n",
            "Training at Epoch 28/50, Batch 10, Cost 1.5867950998587415, Acc 0.796. Time 2.5577125549316406\n",
            "Training at Epoch 28/50, Batch 15, Cost 1.5860051724566528, Acc 0.801. Time 2.5500965118408203\n",
            "Training at Epoch 28/50, Batch 20, Cost 1.5806268088892537, Acc 0.816. Time 2.540865659713745\n",
            "Training at Epoch 28/50, Batch 25, Cost 1.588074838090031, Acc 0.809. Time 2.5390985012054443\n",
            "Training at Epoch 28/50, Batch 30, Cost 1.5740402801098392, Acc 0.807. Time 2.5460405349731445\n",
            "Training at Epoch 28/50, Batch 35, Cost 1.588362101036326, Acc 0.791. Time 2.5468642711639404\n",
            "Training at Epoch 28/50, Batch 40, Cost 1.5920339034196376, Acc 0.796. Time 2.5512843132019043\n",
            "Training at Epoch 28/50, Batch 45, Cost 1.5970858282475142, Acc 0.795. Time 2.4245433807373047\n",
            "......\n",
            "Epoch 28/50, Train: Cost 1.5875817297102868, Acc 0.8004637681159422\n",
            "Epoch 28/50, Validation: Cost 1.5865663315608642, Acc 0.802\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 29/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 29/50, Batch 5, Cost 1.5877890120470384, Acc 0.804. Time 2.542901039123535\n",
            "Training at Epoch 29/50, Batch 10, Cost 1.6072324719607436, Acc 0.784. Time 2.5503971576690674\n",
            "Training at Epoch 29/50, Batch 15, Cost 1.582173463095569, Acc 0.808. Time 2.455256462097168\n",
            "Training at Epoch 29/50, Batch 20, Cost 1.5900736700876468, Acc 0.802. Time 2.504831552505493\n",
            "Training at Epoch 29/50, Batch 25, Cost 1.5835013840906687, Acc 0.811. Time 2.5618090629577637\n",
            "Training at Epoch 29/50, Batch 30, Cost 1.569458616750001, Acc 0.816. Time 2.5575153827667236\n",
            "Training at Epoch 29/50, Batch 35, Cost 1.5802541165655093, Acc 0.818. Time 2.5476737022399902\n",
            "Training at Epoch 29/50, Batch 40, Cost 1.5714648752653593, Acc 0.828. Time 2.5438854694366455\n",
            "Training at Epoch 29/50, Batch 45, Cost 1.5740524674578709, Acc 0.81. Time 2.547384023666382\n",
            "......\n",
            "Epoch 29/50, Train: Cost 1.587002831101382, Acc 0.8017101449275361\n",
            "Epoch 29/50, Validation: Cost 1.584713300281731, Acc 0.8013333333333333\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 30/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 30/50, Batch 5, Cost 1.5837020058143578, Acc 0.806. Time 2.561143636703491\n",
            "Training at Epoch 30/50, Batch 10, Cost 1.590420699047747, Acc 0.791. Time 2.5480306148529053\n",
            "Training at Epoch 30/50, Batch 15, Cost 1.5918481287505364, Acc 0.782. Time 2.45648193359375\n",
            "Training at Epoch 30/50, Batch 20, Cost 1.5919558179356477, Acc 0.793. Time 2.546905279159546\n",
            "Training at Epoch 30/50, Batch 25, Cost 1.5870703574079534, Acc 0.788. Time 2.549206018447876\n",
            "Training at Epoch 30/50, Batch 30, Cost 1.567507229357736, Acc 0.837. Time 2.549551010131836\n",
            "Training at Epoch 30/50, Batch 35, Cost 1.5875509744789338, Acc 0.797. Time 2.549609422683716\n",
            "Training at Epoch 30/50, Batch 40, Cost 1.5815186554479819, Acc 0.806. Time 2.549931287765503\n",
            "Training at Epoch 30/50, Batch 45, Cost 1.588559286031635, Acc 0.797. Time 2.4693796634674072\n",
            "......\n",
            "Epoch 30/50, Train: Cost 1.5850040598056407, Acc 0.8005217391304348\n",
            "Epoch 30/50, Validation: Cost 1.5867445679424392, Acc 0.8036666666666669\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 31/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 31/50, Batch 5, Cost 1.5842074002694608, Acc 0.801. Time 2.549394130706787\n",
            "Training at Epoch 31/50, Batch 10, Cost 1.594374018899112, Acc 0.787. Time 2.4743733406066895\n",
            "Training at Epoch 31/50, Batch 15, Cost 1.594597999483433, Acc 0.78. Time 2.444918394088745\n",
            "Training at Epoch 31/50, Batch 20, Cost 1.585032396620868, Acc 0.797. Time 2.4765703678131104\n",
            "Training at Epoch 31/50, Batch 25, Cost 1.5790877191898285, Acc 0.815. Time 2.5547902584075928\n",
            "Training at Epoch 31/50, Batch 30, Cost 1.577063813353897, Acc 0.82. Time 2.558651924133301\n",
            "Training at Epoch 31/50, Batch 35, Cost 1.5851218725151797, Acc 0.8. Time 2.5448007583618164\n",
            "Training at Epoch 31/50, Batch 40, Cost 1.583722462148834, Acc 0.816. Time 2.5309526920318604\n",
            "Training at Epoch 31/50, Batch 45, Cost 1.5804509521189012, Acc 0.809. Time 2.552001714706421\n",
            "......\n",
            "Epoch 31/50, Train: Cost 1.5865133276017187, Acc 0.8013188405797103\n",
            "Epoch 31/50, Validation: Cost 1.5845147311169072, Acc 0.802\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 32/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 32/50, Batch 5, Cost 1.5746652135454828, Acc 0.814. Time 2.5529141426086426\n",
            "Training at Epoch 32/50, Batch 10, Cost 1.5931366888615535, Acc 0.779. Time 2.5405046939849854\n",
            "Training at Epoch 32/50, Batch 15, Cost 1.574207946472985, Acc 0.81. Time 2.5492846965789795\n",
            "Training at Epoch 32/50, Batch 20, Cost 1.5828399090390564, Acc 0.801. Time 2.5016584396362305\n",
            "Training at Epoch 32/50, Batch 25, Cost 1.5833552816451393, Acc 0.81. Time 2.5467441082000732\n",
            "Training at Epoch 32/50, Batch 30, Cost 1.5990298067146478, Acc 0.788. Time 2.5361952781677246\n",
            "Training at Epoch 32/50, Batch 35, Cost 1.5955103216986795, Acc 0.795. Time 2.4613499641418457\n",
            "Training at Epoch 32/50, Batch 40, Cost 1.5963031683827393, Acc 0.795. Time 2.4753260612487793\n",
            "Training at Epoch 32/50, Batch 45, Cost 1.5848190330935465, Acc 0.81. Time 2.569722890853882\n",
            "......\n",
            "Epoch 32/50, Train: Cost 1.5865512031452744, Acc 0.8019420289855074\n",
            "Epoch 32/50, Validation: Cost 1.5861959477827243, Acc 0.7981666666666666\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 33/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 33/50, Batch 5, Cost 1.5946120620683923, Acc 0.78. Time 2.418987989425659\n",
            "Training at Epoch 33/50, Batch 10, Cost 1.5878482938085428, Acc 0.8. Time 2.4611093997955322\n",
            "Training at Epoch 33/50, Batch 15, Cost 1.5821841755838717, Acc 0.809. Time 2.5133020877838135\n",
            "Training at Epoch 33/50, Batch 20, Cost 1.567651506338522, Acc 0.82. Time 2.565016508102417\n",
            "Training at Epoch 33/50, Batch 25, Cost 1.5720290065557838, Acc 0.814. Time 2.5603878498077393\n",
            "Training at Epoch 33/50, Batch 30, Cost 1.5850807288464326, Acc 0.801. Time 2.5480146408081055\n",
            "Training at Epoch 33/50, Batch 35, Cost 1.574796469656839, Acc 0.802. Time 2.546776056289673\n",
            "Training at Epoch 33/50, Batch 40, Cost 1.5700785296831705, Acc 0.824. Time 2.5496788024902344\n",
            "Training at Epoch 33/50, Batch 45, Cost 1.5852917729511677, Acc 0.803. Time 2.550058126449585\n",
            "......\n",
            "Epoch 33/50, Train: Cost 1.5860307202100585, Acc 0.8018623188405797\n",
            "Epoch 33/50, Validation: Cost 1.5863130858718453, Acc 0.799\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 34/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 34/50, Batch 5, Cost 1.588905480363696, Acc 0.802. Time 2.533884048461914\n",
            "Training at Epoch 34/50, Batch 10, Cost 1.5754796341797066, Acc 0.814. Time 2.546097755432129\n",
            "Training at Epoch 34/50, Batch 15, Cost 1.5761478206167299, Acc 0.809. Time 2.534761905670166\n",
            "Training at Epoch 34/50, Batch 20, Cost 1.5852144829923518, Acc 0.807. Time 2.545008420944214\n",
            "Training at Epoch 34/50, Batch 25, Cost 1.5856108546080216, Acc 0.8. Time 2.422909736633301\n",
            "Training at Epoch 34/50, Batch 30, Cost 1.595364296451102, Acc 0.791. Time 2.4790446758270264\n",
            "Training at Epoch 34/50, Batch 35, Cost 1.5721075308795442, Acc 0.816. Time 2.557746171951294\n",
            "Training at Epoch 34/50, Batch 40, Cost 1.5959817395295595, Acc 0.775. Time 2.5506441593170166\n",
            "Training at Epoch 34/50, Batch 45, Cost 1.5922596439602794, Acc 0.791. Time 2.539822816848755\n",
            "......\n",
            "Epoch 34/50, Train: Cost 1.5855579246694902, Acc 0.801536231884058\n",
            "Epoch 34/50, Validation: Cost 1.582744953014122, Acc 0.8071666666666667\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 35/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 35/50, Batch 5, Cost 1.5775507274212834, Acc 0.804. Time 2.555262804031372\n",
            "Training at Epoch 35/50, Batch 10, Cost 1.5825180351635135, Acc 0.8. Time 2.542299747467041\n",
            "Training at Epoch 35/50, Batch 15, Cost 1.5803245784394901, Acc 0.816. Time 2.552403450012207\n",
            "Training at Epoch 35/50, Batch 20, Cost 1.5986780042989452, Acc 0.787. Time 2.5487234592437744\n",
            "Training at Epoch 35/50, Batch 25, Cost 1.5791852549862162, Acc 0.806. Time 2.543426275253296\n",
            "Training at Epoch 35/50, Batch 30, Cost 1.5792015991729393, Acc 0.823. Time 2.5494778156280518\n",
            "Training at Epoch 35/50, Batch 35, Cost 1.5864208933711534, Acc 0.794. Time 2.544813632965088\n",
            "Training at Epoch 35/50, Batch 40, Cost 1.5867005974993862, Acc 0.794. Time 2.5280518531799316\n",
            "Training at Epoch 35/50, Batch 45, Cost 1.5925328124944231, Acc 0.808. Time 2.5411996841430664\n",
            "......\n",
            "Epoch 35/50, Train: Cost 1.5851605063103789, Acc 0.8019999999999999\n",
            "Epoch 35/50, Validation: Cost 1.5834000611396688, Acc 0.8051666666666667\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 36/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 36/50, Batch 5, Cost 1.602421978367459, Acc 0.771. Time 2.542619466781616\n",
            "Training at Epoch 36/50, Batch 10, Cost 1.5861119093002845, Acc 0.814. Time 2.5434682369232178\n",
            "Training at Epoch 36/50, Batch 15, Cost 1.5865939798793725, Acc 0.807. Time 2.5376312732696533\n",
            "Training at Epoch 36/50, Batch 20, Cost 1.585616174151514, Acc 0.808. Time 2.546772003173828\n",
            "Training at Epoch 36/50, Batch 25, Cost 1.589396567063573, Acc 0.795. Time 2.4302215576171875\n",
            "Training at Epoch 36/50, Batch 30, Cost 1.5994246017501423, Acc 0.788. Time 2.4816629886627197\n",
            "Training at Epoch 36/50, Batch 35, Cost 1.5924639742889266, Acc 0.806. Time 2.555950164794922\n",
            "Training at Epoch 36/50, Batch 40, Cost 1.5857825110172081, Acc 0.807. Time 2.5583531856536865\n",
            "Training at Epoch 36/50, Batch 45, Cost 1.5844723064297412, Acc 0.801. Time 2.542980670928955\n",
            "......\n",
            "Epoch 36/50, Train: Cost 1.5855501295804617, Acc 0.8021521739130436\n",
            "Epoch 36/50, Validation: Cost 1.5836116327084202, Acc 0.8031666666666667\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 37/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 37/50, Batch 5, Cost 1.5914048007841897, Acc 0.791. Time 2.539132833480835\n",
            "Training at Epoch 37/50, Batch 10, Cost 1.5866925642723884, Acc 0.802. Time 2.545555830001831\n",
            "Training at Epoch 37/50, Batch 15, Cost 1.5934008763805414, Acc 0.792. Time 2.553560972213745\n",
            "Training at Epoch 37/50, Batch 20, Cost 1.6133443157308394, Acc 0.764. Time 2.5462148189544678\n",
            "Training at Epoch 37/50, Batch 25, Cost 1.583952306340459, Acc 0.81. Time 2.5400664806365967\n",
            "Training at Epoch 37/50, Batch 30, Cost 1.5782863991897536, Acc 0.812. Time 2.5065810680389404\n",
            "Training at Epoch 37/50, Batch 35, Cost 1.5782491487625547, Acc 0.812. Time 2.541872501373291\n",
            "Training at Epoch 37/50, Batch 40, Cost 1.5703583114269395, Acc 0.807. Time 2.5440666675567627\n",
            "Training at Epoch 37/50, Batch 45, Cost 1.581374148523452, Acc 0.814. Time 2.5322399139404297\n",
            "......\n",
            "Epoch 37/50, Train: Cost 1.585323504081556, Acc 0.8028623188405796\n",
            "Epoch 37/50, Validation: Cost 1.5820523565209694, Acc 0.8048333333333334\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 38/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 38/50, Batch 5, Cost 1.5838180427532857, Acc 0.793. Time 2.547057628631592\n",
            "Training at Epoch 38/50, Batch 10, Cost 1.5718958718470588, Acc 0.824. Time 2.5373294353485107\n",
            "Training at Epoch 38/50, Batch 15, Cost 1.5891103292596367, Acc 0.797. Time 2.5472524166107178\n",
            "Training at Epoch 38/50, Batch 20, Cost 1.5802337907265491, Acc 0.805. Time 2.419973134994507\n",
            "Training at Epoch 38/50, Batch 25, Cost 1.566120432131425, Acc 0.833. Time 2.446817398071289\n",
            "Training at Epoch 38/50, Batch 30, Cost 1.577742124547797, Acc 0.815. Time 2.5665950775146484\n",
            "Training at Epoch 38/50, Batch 35, Cost 1.5759627219222232, Acc 0.814. Time 2.551748752593994\n",
            "Training at Epoch 38/50, Batch 40, Cost 1.5873404537767555, Acc 0.803. Time 2.5526933670043945\n",
            "Training at Epoch 38/50, Batch 45, Cost 1.5829262916093698, Acc 0.803. Time 2.5416550636291504\n",
            "......\n",
            "Epoch 38/50, Train: Cost 1.5849536329925125, Acc 0.802768115942029\n",
            "Epoch 38/50, Validation: Cost 1.5854166531718192, Acc 0.8008333333333333\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 39/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 39/50, Batch 5, Cost 1.5891725265247805, Acc 0.799. Time 2.5366952419281006\n",
            "Training at Epoch 39/50, Batch 10, Cost 1.597301549490261, Acc 0.781. Time 2.5494418144226074\n",
            "Training at Epoch 39/50, Batch 15, Cost 1.5902649006074439, Acc 0.787. Time 2.547712564468384\n",
            "Training at Epoch 39/50, Batch 20, Cost 1.5770522364559973, Acc 0.808. Time 2.542531967163086\n",
            "Training at Epoch 39/50, Batch 25, Cost 1.5712250329664756, Acc 0.811. Time 2.533144950866699\n",
            "Training at Epoch 39/50, Batch 30, Cost 1.5849935326092572, Acc 0.794. Time 2.548692226409912\n",
            "Training at Epoch 39/50, Batch 35, Cost 1.563425792428442, Acc 0.82. Time 2.543240547180176\n",
            "Training at Epoch 39/50, Batch 40, Cost 1.5853143300999673, Acc 0.794. Time 2.546066999435425\n",
            "Training at Epoch 39/50, Batch 45, Cost 1.5848613665606464, Acc 0.785. Time 2.430974245071411\n",
            "......\n",
            "Epoch 39/50, Train: Cost 1.5847412391648166, Acc 0.8024782608695652\n",
            "Epoch 39/50, Validation: Cost 1.5809355096998268, Acc 0.8061666666666666\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 40/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 40/50, Batch 5, Cost 1.5873537831683302, Acc 0.8. Time 2.537252187728882\n",
            "Training at Epoch 40/50, Batch 10, Cost 1.588296406984272, Acc 0.798. Time 2.4643332958221436\n",
            "Training at Epoch 40/50, Batch 15, Cost 1.5789883179935482, Acc 0.807. Time 2.4252357482910156\n",
            "Training at Epoch 40/50, Batch 20, Cost 1.5724536996399692, Acc 0.815. Time 2.4495246410369873\n",
            "Training at Epoch 40/50, Batch 25, Cost 1.5809888749840704, Acc 0.798. Time 2.5554091930389404\n",
            "Training at Epoch 40/50, Batch 30, Cost 1.5853699637314336, Acc 0.798. Time 2.5493428707122803\n",
            "Training at Epoch 40/50, Batch 35, Cost 1.5851124076659107, Acc 0.813. Time 2.5433385372161865\n",
            "Training at Epoch 40/50, Batch 40, Cost 1.5975031953896026, Acc 0.787. Time 2.5371811389923096\n",
            "Training at Epoch 40/50, Batch 45, Cost 1.5921050633968616, Acc 0.79. Time 2.5439465045928955\n",
            "......\n",
            "Epoch 40/50, Train: Cost 1.5845470064919163, Acc 0.8024275362318842\n",
            "Epoch 40/50, Validation: Cost 1.5821846521478051, Acc 0.8048333333333333\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 41/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 41/50, Batch 5, Cost 1.5939918977120424, Acc 0.792. Time 2.5438895225524902\n",
            "Training at Epoch 41/50, Batch 10, Cost 1.5983073795322074, Acc 0.78. Time 2.5368258953094482\n",
            "Training at Epoch 41/50, Batch 15, Cost 1.6023465599613969, Acc 0.782. Time 2.5455198287963867\n",
            "Training at Epoch 41/50, Batch 20, Cost 1.5638590795953935, Acc 0.821. Time 2.5426762104034424\n",
            "Training at Epoch 41/50, Batch 25, Cost 1.5788861178850155, Acc 0.817. Time 2.5430984497070312\n",
            "Training at Epoch 41/50, Batch 30, Cost 1.579611663612523, Acc 0.805. Time 2.5328569412231445\n",
            "Training at Epoch 41/50, Batch 35, Cost 1.5749868352668612, Acc 0.815. Time 2.453038454055786\n",
            "Training at Epoch 41/50, Batch 40, Cost 1.5784186387165946, Acc 0.804. Time 2.497980833053589\n",
            "Training at Epoch 41/50, Batch 45, Cost 1.5865837939842267, Acc 0.797. Time 2.55879282951355\n",
            "......\n",
            "Epoch 41/50, Train: Cost 1.5839929465341598, Acc 0.8024782608695651\n",
            "Epoch 41/50, Validation: Cost 1.5824144681807712, Acc 0.8006666666666667\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 42/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 42/50, Batch 5, Cost 1.5878692127841678, Acc 0.788. Time 2.4153332710266113\n",
            "Training at Epoch 42/50, Batch 10, Cost 1.5834212279735107, Acc 0.8. Time 2.4498767852783203\n",
            "Training at Epoch 42/50, Batch 15, Cost 1.5892913642969206, Acc 0.788. Time 2.5022356510162354\n",
            "Training at Epoch 42/50, Batch 20, Cost 1.5790700079385056, Acc 0.815. Time 2.5585391521453857\n",
            "Training at Epoch 42/50, Batch 25, Cost 1.590891321031952, Acc 0.799. Time 2.5452725887298584\n",
            "Training at Epoch 42/50, Batch 30, Cost 1.5855865524692494, Acc 0.803. Time 2.544081211090088\n",
            "Training at Epoch 42/50, Batch 35, Cost 1.5942380632869226, Acc 0.801. Time 2.4980204105377197\n",
            "Training at Epoch 42/50, Batch 40, Cost 1.585198188280589, Acc 0.794. Time 2.536031723022461\n",
            "Training at Epoch 42/50, Batch 45, Cost 1.5777364295155465, Acc 0.818. Time 2.547741413116455\n",
            "......\n",
            "Epoch 42/50, Train: Cost 1.5844993295752667, Acc 0.8031884057971014\n",
            "Epoch 42/50, Validation: Cost 1.5844119751763739, Acc 0.8056666666666668\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 43/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 43/50, Batch 5, Cost 1.5745839534606716, Acc 0.815. Time 2.5427310466766357\n",
            "Training at Epoch 43/50, Batch 10, Cost 1.602623461910247, Acc 0.783. Time 2.5426557064056396\n",
            "Training at Epoch 43/50, Batch 15, Cost 1.5789085700203254, Acc 0.809. Time 2.5327727794647217\n",
            "Training at Epoch 43/50, Batch 20, Cost 1.5752025989145677, Acc 0.821. Time 2.5466537475585938\n",
            "Training at Epoch 43/50, Batch 25, Cost 1.5667836748690962, Acc 0.824. Time 2.539034128189087\n",
            "Training at Epoch 43/50, Batch 30, Cost 1.5791324826968067, Acc 0.798. Time 2.4142699241638184\n",
            "Training at Epoch 43/50, Batch 35, Cost 1.5756418697205905, Acc 0.821. Time 2.4388339519500732\n",
            "Training at Epoch 43/50, Batch 40, Cost 1.5878114676688402, Acc 0.797. Time 2.4954352378845215\n",
            "Training at Epoch 43/50, Batch 45, Cost 1.59343516842887, Acc 0.791. Time 2.5585131645202637\n",
            "......\n",
            "Epoch 43/50, Train: Cost 1.5837151034660242, Acc 0.8032318840579711\n",
            "Epoch 43/50, Validation: Cost 1.5816144446101703, Acc 0.8045\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 44/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 44/50, Batch 5, Cost 1.5987720349451209, Acc 0.783. Time 2.42158842086792\n",
            "Training at Epoch 44/50, Batch 10, Cost 1.57996425243892, Acc 0.799. Time 2.4764597415924072\n",
            "Training at Epoch 44/50, Batch 15, Cost 1.5914718906280068, Acc 0.791. Time 2.5549678802490234\n",
            "Training at Epoch 44/50, Batch 20, Cost 1.5706785116953648, Acc 0.821. Time 2.553114891052246\n",
            "Training at Epoch 44/50, Batch 25, Cost 1.583374755544242, Acc 0.804. Time 2.5362703800201416\n",
            "Training at Epoch 44/50, Batch 30, Cost 1.5853547521487101, Acc 0.818. Time 2.54822039604187\n",
            "Training at Epoch 44/50, Batch 35, Cost 1.5866869044260619, Acc 0.81. Time 2.4967217445373535\n",
            "Training at Epoch 44/50, Batch 40, Cost 1.5878572622483955, Acc 0.789. Time 2.539726734161377\n",
            "Training at Epoch 44/50, Batch 45, Cost 1.5837098903752305, Acc 0.796. Time 2.533782720565796\n",
            "......\n",
            "Epoch 44/50, Train: Cost 1.5834367921673254, Acc 0.8035362318840579\n",
            "Epoch 44/50, Validation: Cost 1.5808689456922373, Acc 0.805\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 45/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 45/50, Batch 5, Cost 1.5854803477510577, Acc 0.803. Time 2.543182373046875\n",
            "Training at Epoch 45/50, Batch 10, Cost 1.5712667168951437, Acc 0.822. Time 2.5417470932006836\n",
            "Training at Epoch 45/50, Batch 15, Cost 1.584795229859907, Acc 0.793. Time 2.5485994815826416\n",
            "Training at Epoch 45/50, Batch 20, Cost 1.5870504768721088, Acc 0.807. Time 2.538275957107544\n",
            "Training at Epoch 45/50, Batch 25, Cost 1.5830300372313053, Acc 0.816. Time 2.430508613586426\n",
            "Training at Epoch 45/50, Batch 30, Cost 1.5803289547368362, Acc 0.809. Time 2.445850372314453\n",
            "Training at Epoch 45/50, Batch 35, Cost 1.5780571318228684, Acc 0.821. Time 2.5477168560028076\n",
            "Training at Epoch 45/50, Batch 40, Cost 1.5765712525261832, Acc 0.809. Time 2.540530204772949\n",
            "Training at Epoch 45/50, Batch 45, Cost 1.5834260234347106, Acc 0.811. Time 2.555797576904297\n",
            "......\n",
            "Epoch 45/50, Train: Cost 1.5837364495088655, Acc 0.8033333333333335\n",
            "Epoch 45/50, Validation: Cost 1.5817741377911814, Acc 0.8033333333333333\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 46/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 46/50, Batch 5, Cost 1.5728610041982498, Acc 0.818. Time 2.4999921321868896\n",
            "Training at Epoch 46/50, Batch 10, Cost 1.5929658580613955, Acc 0.783. Time 2.5465199947357178\n",
            "Training at Epoch 46/50, Batch 15, Cost 1.5770850139232406, Acc 0.804. Time 2.5562806129455566\n",
            "Training at Epoch 46/50, Batch 20, Cost 1.5733048856246246, Acc 0.821. Time 2.540184259414673\n",
            "Training at Epoch 46/50, Batch 25, Cost 1.5838066336599732, Acc 0.796. Time 2.528686761856079\n",
            "Training at Epoch 46/50, Batch 30, Cost 1.586189493943943, Acc 0.804. Time 2.5380895137786865\n",
            "Training at Epoch 46/50, Batch 35, Cost 1.5713611289984968, Acc 0.818. Time 2.418591260910034\n",
            "Training at Epoch 46/50, Batch 40, Cost 1.5879042654775453, Acc 0.806. Time 2.5347046852111816\n",
            "Training at Epoch 46/50, Batch 45, Cost 1.583328545788751, Acc 0.8. Time 2.5343031883239746\n",
            "......\n",
            "Epoch 46/50, Train: Cost 1.5826053960961748, Acc 0.8036086956521737\n",
            "Epoch 46/50, Validation: Cost 1.5805131094101987, Acc 0.8046666666666668\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 47/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 47/50, Batch 5, Cost 1.5699144917870262, Acc 0.824. Time 2.5415427684783936\n",
            "Training at Epoch 47/50, Batch 10, Cost 1.5800502731893067, Acc 0.816. Time 2.541469097137451\n",
            "Training at Epoch 47/50, Batch 15, Cost 1.579261279291694, Acc 0.813. Time 2.5438151359558105\n",
            "Training at Epoch 47/50, Batch 20, Cost 1.5845558409598908, Acc 0.792. Time 2.431095600128174\n",
            "Training at Epoch 47/50, Batch 25, Cost 1.5788475588630921, Acc 0.806. Time 2.4903831481933594\n",
            "Training at Epoch 47/50, Batch 30, Cost 1.5881869692660713, Acc 0.81. Time 2.5503830909729004\n",
            "Training at Epoch 47/50, Batch 35, Cost 1.5796852176403244, Acc 0.814. Time 2.5448429584503174\n",
            "Training at Epoch 47/50, Batch 40, Cost 1.6009682104699494, Acc 0.793. Time 2.5375583171844482\n",
            "Training at Epoch 47/50, Batch 45, Cost 1.5871281505605166, Acc 0.805. Time 2.539900779724121\n",
            "......\n",
            "Epoch 47/50, Train: Cost 1.5826485163579584, Acc 0.8039130434782606\n",
            "Epoch 47/50, Validation: Cost 1.5802310570323232, Acc 0.8061666666666666\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 48/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 48/50, Batch 5, Cost 1.6042959936510675, Acc 0.783. Time 2.5521488189697266\n",
            "Training at Epoch 48/50, Batch 10, Cost 1.5938537351569817, Acc 0.792. Time 2.5475502014160156\n",
            "Training at Epoch 48/50, Batch 15, Cost 1.579528602796512, Acc 0.821. Time 2.5356571674346924\n",
            "Training at Epoch 48/50, Batch 20, Cost 1.5844643986119482, Acc 0.794. Time 2.5448269844055176\n",
            "Training at Epoch 48/50, Batch 25, Cost 1.5730517025009292, Acc 0.801. Time 2.534905195236206\n",
            "Training at Epoch 48/50, Batch 30, Cost 1.5734808548385117, Acc 0.816. Time 2.537782669067383\n",
            "Training at Epoch 48/50, Batch 35, Cost 1.5867059952007896, Acc 0.807. Time 2.459282875061035\n",
            "Training at Epoch 48/50, Batch 40, Cost 1.5631673719765726, Acc 0.82. Time 2.491302490234375\n",
            "Training at Epoch 48/50, Batch 45, Cost 1.583936184028206, Acc 0.794. Time 2.5571820735931396\n",
            "......\n",
            "Epoch 48/50, Train: Cost 1.5827774077181282, Acc 0.8034347826086956\n",
            "Epoch 48/50, Validation: Cost 1.583954093956791, Acc 0.8021666666666666\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 49/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 49/50, Batch 5, Cost 1.5632890684141958, Acc 0.834. Time 2.432234525680542\n",
            "Training at Epoch 49/50, Batch 10, Cost 1.5796471847686158, Acc 0.798. Time 2.4893996715545654\n",
            "Training at Epoch 49/50, Batch 15, Cost 1.5930919455697539, Acc 0.798. Time 2.548053026199341\n",
            "Training at Epoch 49/50, Batch 20, Cost 1.5948989658473929, Acc 0.793. Time 2.5373058319091797\n",
            "Training at Epoch 49/50, Batch 25, Cost 1.5811904145355433, Acc 0.801. Time 2.5452933311462402\n",
            "Training at Epoch 49/50, Batch 30, Cost 1.5915235184495407, Acc 0.796. Time 2.543468713760376\n",
            "Training at Epoch 49/50, Batch 35, Cost 1.580918150412225, Acc 0.806. Time 2.543635606765747\n",
            "Training at Epoch 49/50, Batch 40, Cost 1.6073971499095683, Acc 0.773. Time 2.538820505142212\n",
            "Training at Epoch 49/50, Batch 45, Cost 1.587580945451045, Acc 0.795. Time 2.5478384494781494\n",
            "......\n",
            "Epoch 49/50, Train: Cost 1.5831820789319078, Acc 0.8033695652173911\n",
            "Epoch 49/50, Validation: Cost 1.5799864935115429, Acc 0.8039999999999999\n",
            "=-==-==-==-==-==-==-==-==-==-=\n",
            "Training at Epoch 50/50, Train batches 46, Val batches 3......\n",
            "Training at Epoch 50/50, Batch 5, Cost 1.5839442959452568, Acc 0.803. Time 2.5393948554992676\n",
            "Training at Epoch 50/50, Batch 10, Cost 1.5985861586585652, Acc 0.781. Time 2.539829730987549\n",
            "Training at Epoch 50/50, Batch 15, Cost 1.5710675990662217, Acc 0.814. Time 2.5409255027770996\n",
            "Training at Epoch 50/50, Batch 20, Cost 1.5897318432074716, Acc 0.788. Time 2.545476198196411\n",
            "Training at Epoch 50/50, Batch 25, Cost 1.5646262257691839, Acc 0.805. Time 2.450883388519287\n",
            "Training at Epoch 50/50, Batch 30, Cost 1.585133415132774, Acc 0.798. Time 2.4875295162200928\n",
            "Training at Epoch 50/50, Batch 35, Cost 1.5901170840660102, Acc 0.807. Time 2.5461039543151855\n",
            "Training at Epoch 50/50, Batch 40, Cost 1.5844742201298037, Acc 0.797. Time 2.5544590950012207\n",
            "Training at Epoch 50/50, Batch 45, Cost 1.5775030013707023, Acc 0.808. Time 2.5420496463775635\n",
            "......\n",
            "Epoch 50/50, Train: Cost 1.582751372816278, Acc 0.8038550724637681\n",
            "Epoch 50/50, Validation: Cost 1.5801287597303197, Acc 0.8075000000000001\n",
            "=-==-==-==-==-==-==-==-==-==-=\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-27e2789f097d>:14: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  results_df = pd.concat(\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 50\n",
        "n_reps = 1\n",
        "batch_size = 1000\n",
        "\n",
        "train_sizes = [N_TRAIN]\n",
        "\n",
        "def run_iterations():\n",
        "    results_df = pd.DataFrame(\n",
        "        columns=[\"train_acc\", \"train_cost\", \"val_acc\", \"val_cost\", \"step\", \"n_train\"]\n",
        "    )\n",
        "\n",
        "    for _ in range(n_reps):\n",
        "        results = train_vqc(n_epochs=n_epochs, batchsize=batch_size)\n",
        "        results_df = pd.concat(\n",
        "            [results_df, pd.DataFrame.from_dict(results)], axis=0, ignore_index=True\n",
        "        )\n",
        "\n",
        "    return results_df\n",
        "\n",
        "results_df = run_iterations()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xYAQniN5h8C"
      },
      "outputs": [],
      "source": [
        "testloader = torch.utils.data.DataLoader(\n",
        "        torch.utils.data.Subset(test_dataset, indices_test), batch_size=1000, shuffle=False\n",
        "    )\n",
        "def test_model(testloader, weights, weights_last):\n",
        "    test_cost_batches = []\n",
        "    test_acc_batches = []\n",
        "\n",
        "    # Initialize accumulators for total cost and correct predictions\n",
        "    total_cost = 0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    print(\"Testing the model...\")\n",
        "\n",
        "    # Iterate through the entire test dataset\n",
        "    for x_test, y_test in testloader:\n",
        "        x_test, y_test = jnp.asarray(x_test.numpy()), jnp.asarray(y_test.numpy())\n",
        "\n",
        "        # Get model predictions\n",
        "        test_out = compute_out(weights, weights_last, x_test, y_test)\n",
        "        test_pred = jnp.argmax(test_out, axis=1)\n",
        "\n",
        "        # Compute accuracy\n",
        "        correct_predictions += jnp.sum(jnp.array(test_pred == y_test).astype(int))\n",
        "        total_samples += len(y_test)\n",
        "\n",
        "        # Compute cost\n",
        "        test_cost = jnp.nanmean(optax.softmax_cross_entropy_with_integer_labels(test_out, y_test))\n",
        "        total_cost += test_cost * len(y_test)  # Accumulate total cost\n",
        "\n",
        "    # Compute overall test results\n",
        "    avg_test_cost = total_cost / total_samples\n",
        "    avg_test_acc = correct_predictions / total_samples\n",
        "\n",
        "    print(f\"Final Test: Cost {avg_test_cost}, Acc {avg_test_acc}\")\n",
        "\n",
        "# Example usage:\n",
        "# test_model(testloader, results['weights'], results['weights_last'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import jax.numpy as jnp  # Import JAX's NumPy\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('weights_100.csv')\n",
        "\n",
        "# Check if the DataFrame is not empty\n",
        "if not df.empty:\n",
        "    # Access the last row directly\n",
        "    row = df.iloc[-1]  # Access the last row\n",
        "\n",
        "    # Extract the epoch, weights, and biases\n",
        "    epoch = row['epoch']\n",
        "    weights = jnp.array(ast.literal_eval(row['weights']))  # Convert to JAX array\n",
        "    biases = jnp.array(ast.literal_eval(row['biases']))    # Convert to JAX array\n",
        "\n",
        "    # Call your test model function with the extracted weights and biases\n",
        "    test_model(testloader, weights, biases)\n",
        "\n",
        "    # Print the epoch and first few weights and biases\n",
        "    print(f\"Epoch: {epoch}, Weights: {weights[:5]}, Biases: {biases}\")\n",
        "\n",
        "else:\n",
        "    print(\"The DataFrame is empty.\")\n"
      ],
      "metadata": {
        "id": "KbSKv5ktrLE6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d8e895b-58ba-4d92-a5a1-5780728694e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing the model...\n",
            "Final Test: Cost 1.5874803340120776, Acc 0.794625\n",
            "Epoch: 50, Weights: [0.46925562 0.07458095 0.29607797 0.0762298  0.19027738], Biases: [-0.36918876  0.5168795   0.60393661  0.88456292  0.61219567  0.45577012\n",
            " -0.39919786  0.47452128  0.61667765  0.392655  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_df"
      ],
      "metadata": {
        "id": "N2u4sPeGxQyK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "262b7f49-53a3-41a1-ee41-39ff2743ef63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'results_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-3c899ad3fe5e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'results_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZwqGAsnd4zT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "3dbcb805-d651-4daf-feea-a6b063c83174"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'results_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-c1cfbd09155d>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Assuming `results_df` is already loaded and contains columns:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 'train_acc', 'train_cost', 'val_acc', 'val_cost', 'step', 'n_train'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf_agg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"n_train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"step\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"std\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdf_agg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_agg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Plotting settings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'results_df' is not defined"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming `results_df` is already loaded and contains columns:\n",
        "# 'train_acc', 'train_cost', 'val_acc', 'val_cost', 'step', 'n_train'\n",
        "df_agg = results_df.groupby([\"n_train\", \"step\"]).agg([\"mean\", \"std\"])\n",
        "df_agg = df_agg.reset_index()\n",
        "# Plotting settings\n",
        "sns.set_style('whitegrid')\n",
        "fig, axes = plt.subplots(ncols=2, figsize=(14, 5))\n",
        "\n",
        "# Plot losses (train_cost and val_cost)\n",
        "axes[0].plot(results_df['step'], results_df['train_cost'], 'o-', label='Train Loss', color='blue', alpha=0.8)\n",
        "axes[0].plot(results_df['step'], results_df['val_cost'], 'x--', label='Validation Loss', color='orange', alpha=0.8)\n",
        "axes[0].set_title('Train and Validation Losses', fontsize=14)\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].legend()\n",
        "\n",
        "# Plot accuracies (train_acc and val_acc)\n",
        "axes[1].plot(results_df['step'], results_df['train_acc'], 'o-', label='Train Accuracy', color='blue', alpha=0.8)\n",
        "axes[1].plot(results_df['step'], results_df['val_acc'], 'x--', label='Validation Accuracy', color='orange', alpha=0.8)\n",
        "axes[1].set_title('Train and Validation Accuracies', fontsize=14)\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "axes[1].set_ylim(0, 1)\n",
        "axes[1].legend()\n",
        "\n",
        "# Adjust layout and show plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wr-ZnyKwd5dm"
      },
      "outputs": [],
      "source": [
        "save_folder = \"/content\"\n",
        "results_df.to_csv(os.path.join(save_folder, \"fashion-mnist_HermImgReUpload_results_random.csv\"))\n",
        "df_agg.to_csv(os.path.join(save_folder, \"fashion-mnist_HermImgReUpload_results_agg_random.csv\"))\n",
        "# save the plot to file\n",
        "fig.savefig(os.path.join(save_folder, \"fashion-mnist_HermImgReUpload_results_random.pdf\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import jax.numpy as jnp\n",
        "import torch\n",
        "import optax\n",
        "\n",
        "def load_weights_from_csv(filename):\n",
        "    \"\"\"Load the last row of weights and biases from a CSV file.\"\"\"\n",
        "    df = pd.read_csv(filename)\n",
        "    if not df.empty:\n",
        "        row = df.iloc[-1]\n",
        "        weights = jnp.array(ast.literal_eval(row['weights']))\n",
        "        biases = jnp.array(ast.literal_eval(row['biases']))\n",
        "        return weights, biases\n",
        "    else:\n",
        "        raise ValueError(\"The CSV file is empty.\")\n",
        "\n",
        "def ensemble_test_model_average(testloader, weights1, weights_last1, weights2, weights_last2):\n",
        "    total_cost = 0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    print(\"Testing the simple average ensemble model...\")\n",
        "\n",
        "    for x_test, y_test in testloader:\n",
        "        x_test, y_test = jnp.asarray(x_test.numpy()), jnp.asarray(y_test.numpy())\n",
        "\n",
        "        # Compute outputs for both models\n",
        "        out1 = compute_out(weights1, weights_last1, x_test, y_test)\n",
        "        out2 = compute_out(weights2, weights_last2, x_test, y_test)\n",
        "\n",
        "        # Average the logits\n",
        "        ensemble_out = (out1 + out2) / 2\n",
        "        ensemble_pred = jnp.argmax(ensemble_out, axis=1)\n",
        "\n",
        "        correct_predictions += jnp.sum(jnp.array(ensemble_pred == y_test).astype(int))\n",
        "        total_samples += len(y_test)\n",
        "\n",
        "        test_cost = jnp.nanmean(optax.softmax_cross_entropy_with_integer_labels(ensemble_out, y_test))\n",
        "        total_cost += test_cost * len(y_test)\n",
        "\n",
        "    avg_test_cost = total_cost / total_samples\n",
        "    avg_test_acc = correct_predictions / total_samples\n",
        "    print(f\"Simple Average Ensemble: Cost {avg_test_cost}, Acc {avg_test_acc}\")\n",
        "\n",
        "\n",
        "\n",
        "weights1, weights_last1 = load_weights_from_csv('weights.csv')\n",
        "weights2, weights_last2 = load_weights_from_csv('weights_beta.csv')\n",
        "\n",
        "ensemble_test_model_average(testloader, weights1, weights_last1, weights2, weights_last2)\n"
      ],
      "metadata": {
        "id": "9h4MStM5tQMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import jax.numpy as jnp\n",
        "import torch\n",
        "import optax\n",
        "\n",
        "def load_weights_from_csv(filename):\n",
        "    \"\"\"Load the last row of weights and biases from a CSV file.\"\"\"\n",
        "    df = pd.read_csv(filename)\n",
        "    if not df.empty:\n",
        "        row = df.iloc[-1]\n",
        "        weights = jnp.array(ast.literal_eval(row['weights']))\n",
        "        biases = jnp.array(ast.literal_eval(row['biases']))\n",
        "        return weights, biases\n",
        "    else:\n",
        "        raise ValueError(\"The CSV file is empty.\")\n",
        "\n",
        "def ensemble_test_model_weighted(testloader, weights1, weights_last1, weights2, weights_last2, alpha=0.2):\n",
        "    total_cost = 0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    print(\"Testing the weighted average ensemble model...\")\n",
        "\n",
        "    for x_test, y_test in testloader:\n",
        "        x_test, y_test = jnp.asarray(x_test.numpy()), jnp.asarray(y_test.numpy())\n",
        "\n",
        "        # Compute outputs for both models\n",
        "        out1 = compute_out(weights1, weights_last1, x_test, y_test)\n",
        "        out2 = compute_out(weights2, weights_last2, x_test, y_test)\n",
        "\n",
        "        # Weighted average of the logits\n",
        "        ensemble_out = alpha * out1 + (1 - alpha) * out2\n",
        "        ensemble_pred = jnp.argmax(ensemble_out, axis=1)\n",
        "\n",
        "        correct_predictions += jnp.sum(jnp.array(ensemble_pred == y_test).astype(int))\n",
        "        total_samples += len(y_test)\n",
        "\n",
        "        test_cost = jnp.nanmean(optax.softmax_cross_entropy_with_integer_labels(ensemble_out, y_test))\n",
        "        total_cost += test_cost * len(y_test)\n",
        "\n",
        "    avg_test_cost = total_cost / total_samples\n",
        "    avg_test_acc = correct_predictions / total_samples\n",
        "    print(f\"Weighted Average Ensemble: Cost {avg_test_cost}, Acc {avg_test_acc}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "weights1, weights_last1 = load_weights_from_csv('/content/weights.csv')\n",
        "weights2, weights_last2 = load_weights_from_csv('/content/weights_100.csv')\n",
        "\n",
        "ensemble_test_model_weighted(testloader, weights1, weights_last1, weights2, weights_last2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pO9Bhs-xX9L",
        "outputId": "bbcc1933-b9de-42bd-eae3-7c9b6552e12f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing the weighted average ensemble model...\n",
            "Weighted Average Ensemble: Cost 1.5941642231761024, Acc 0.792875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def ensemble_test_model_weighted(testloader, weights1, weights_last1, weights2, weights_last2, alpha=0.2):\n",
        "    total_cost = 0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    print(\"Testing the weighted average ensemble model...\")\n",
        "\n",
        "    for x_test, y_test in testloader:\n",
        "        x_test, y_test = jnp.asarray(x_test.numpy()), jnp.asarray(y_test.numpy())\n",
        "\n",
        "        # Compute outputs for both models\n",
        "        out1 = compute_out(weights1, weights_last1, x_test, y_test)\n",
        "        out2 = compute_out(weights2, weights_last2, x_test, y_test)\n",
        "\n",
        "        # Weighted average of the logits\n",
        "        ensemble_out = (1- alpha) * out1 + ( alpha) * out2\n",
        "        ensemble_pred = jnp.argmax(ensemble_out, axis=1)\n",
        "\n",
        "        correct_predictions += jnp.sum(jnp.array(ensemble_pred == y_test).astype(int))\n",
        "        total_samples += len(y_test)\n",
        "\n",
        "        test_cost = jnp.nanmean(optax.softmax_cross_entropy_with_integer_labels(ensemble_out, y_test))\n",
        "        total_cost += test_cost * len(y_test)\n",
        "\n",
        "    avg_test_cost = total_cost / total_samples\n",
        "    avg_test_acc = correct_predictions / total_samples\n",
        "    print(f\"Weighted Average Ensemble: Cost {avg_test_cost}, Acc {avg_test_acc}\")\n",
        "\n",
        "    # Return the results to use outside the function\n",
        "    return avg_test_cost, avg_test_acc\n",
        "\n",
        "# Define the range of alpha values to test\n",
        "alpha_values = np.linspace(0, 1, 11)  # e.g., [0.0, 0.1, 0.2, ..., 1.0]\n",
        "results = []\n",
        "\n",
        "# Loop over each alpha value and store results\n",
        "for alpha in alpha_values:\n",
        "    avg_test_cost, avg_test_acc = ensemble_test_model_weighted(testloader, weights1, weights_last1, weights2, weights_last2, alpha)\n",
        "    results.append({'Alpha': alpha, 'Average_Cost': avg_test_cost, 'Average_Accuracy': avg_test_acc})\n",
        "\n",
        "# Create a DataFrame to store the results\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Display the table of results\n",
        "print(\"Alpha = 0 = random\")\n",
        "print(\"Beta = 0 = random\")\n",
        "print(results_df)\n",
        "\n",
        "# Plotting the results\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Plot accuracy vs alpha\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(results_df['Alpha'], results_df['Average_Accuracy'], marker='o', color='b')\n",
        "plt.xlabel('Alpha')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy vs Alpha')\n",
        "\n",
        "# Plot cost vs alpha\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(results_df['Alpha'], results_df['Average_Cost'], marker='o', color='r')\n",
        "plt.xlabel('Alpha')\n",
        "plt.ylabel('Cost')\n",
        "plt.title('Cost vs Alpha')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        },
        "id": "kIzM8_aPaL_o",
        "outputId": "52e18875-f79c-4dea-b157-0dfbc4f360dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing the weighted average ensemble model...\n",
            "Weighted Average Ensemble: Cost 1.6218039005820422, Acc 0.77325\n",
            "Testing the weighted average ensemble model...\n",
            "Weighted Average Ensemble: Cost 1.618270663950763, Acc 0.775375\n",
            "Testing the weighted average ensemble model...\n",
            "Weighted Average Ensemble: Cost 1.6147596333939436, Acc 0.778\n",
            "Testing the weighted average ensemble model...\n",
            "Weighted Average Ensemble: Cost 1.6112708882190137, Acc 0.7815\n",
            "Testing the weighted average ensemble model...\n",
            "Weighted Average Ensemble: Cost 1.6078045078272698, Acc 0.783875\n",
            "Testing the weighted average ensemble model...\n",
            "Weighted Average Ensemble: Cost 1.604360571675229, Acc 0.786875\n",
            "Testing the weighted average ensemble model...\n",
            "Weighted Average Ensemble: Cost 1.6009391592337248, Acc 0.788375\n",
            "Testing the weighted average ensemble model...\n",
            "Weighted Average Ensemble: Cost 1.597540349944756, Acc 0.790875\n",
            "Testing the weighted average ensemble model...\n",
            "Weighted Average Ensemble: Cost 1.5941642231761024, Acc 0.792875\n",
            "Testing the weighted average ensemble model...\n",
            "Weighted Average Ensemble: Cost 1.5908108581737397, Acc 0.79425\n",
            "Testing the weighted average ensemble model...\n",
            "Weighted Average Ensemble: Cost 1.5874803340120776, Acc 0.794625\n",
            "Alpha = 0 = random\n",
            "Beta = 0 = random\n",
            "    Alpha        Average_Cost Average_Accuracy\n",
            "0     0.0  1.6218039005820422          0.77325\n",
            "1     0.1   1.618270663950763         0.775375\n",
            "2     0.2  1.6147596333939436            0.778\n",
            "3     0.3  1.6112708882190137           0.7815\n",
            "4     0.4  1.6078045078272698         0.783875\n",
            "5     0.5   1.604360571675229         0.786875\n",
            "6     0.6  1.6009391592337248         0.788375\n",
            "7     0.7   1.597540349944756         0.790875\n",
            "8     0.8  1.5941642231761024         0.792875\n",
            "9     0.9  1.5908108581737397          0.79425\n",
            "10    1.0  1.5874803340120776         0.794625\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import jax.numpy as jnp\n",
        "import torch\n",
        "import optax\n",
        "\n",
        "def load_weights_from_csv(filename):\n",
        "    \"\"\"Load the last row of weights and biases from a CSV file.\"\"\"\n",
        "    df = pd.read_csv(filename)\n",
        "    if not df.empty:\n",
        "        row = df.iloc[-1]\n",
        "        weights = jnp.array(ast.literal_eval(row['weights']))\n",
        "        biases = jnp.array(ast.literal_eval(row['biases']))\n",
        "        return weights, biases\n",
        "    else:\n",
        "        raise ValueError(\"The CSV file is empty.\")\n",
        "\n",
        "def ensemble_test_model_majority_voting(testloader, weights1, weights_last1, weights2, weights_last2):\n",
        "    total_cost = 0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    print(\"Testing the majority voting ensemble model...\")\n",
        "\n",
        "    for x_test, y_test in testloader:\n",
        "        x_test, y_test = jnp.asarray(x_test.numpy()), jnp.asarray(y_test.numpy())\n",
        "\n",
        "        # Get predictions from both models\n",
        "        pred1 = jnp.argmax(compute_out(weights1, weights_last1, x_test, y_test), axis=1)\n",
        "        pred2 = jnp.argmax(compute_out(weights2, weights_last2, x_test, y_test), axis=1)\n",
        "\n",
        "        # Majority vote\n",
        "        ensemble_pred = jnp.array([pred1[i] if pred1[i] == pred2[i] else pred1[i] for i in range(len(pred1))])\n",
        "\n",
        "        correct_predictions += jnp.sum(jnp.array(ensemble_pred == y_test).astype(int))\n",
        "        total_samples += len(y_test)\n",
        "\n",
        "    avg_test_acc = correct_predictions / total_samples\n",
        "    print(f\"Majority Voting Ensemble: Accuracy {avg_test_acc}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "weights1, weights_last1 = load_weights_from_csv('weights.csv')\n",
        "weights2, weights_last2 = load_weights_from_csv('weights_beta.csv')\n",
        "\n",
        "ensemble_test_model_majority_voting(testloader, weights1, weights_last1, weights2, weights_last2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "4Z9z6uA6xjJy",
        "outputId": "23af112a-f1c1-4ad0-ba80-b947e27c50be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'weights.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-3c042649c614>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mweights1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_last1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_weights_from_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weights.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0mweights2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_last2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_weights_from_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weights_beta.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-3c042649c614>\u001b[0m in \u001b[0;36mload_weights_from_csv\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_weights_from_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m\"\"\"Load the last row of weights and biases from a CSV file.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'weights.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def load_weights_from_csv(filename):\n",
        "    \"\"\"Load the last row of weights and biases from a CSV file.\"\"\"\n",
        "    df = pd.read_csv(filename)\n",
        "    if not df.empty:\n",
        "        row = df.iloc[-1]\n",
        "        weights = jnp.array(ast.literal_eval(row['weights']))\n",
        "        biases = jnp.array(ast.literal_eval(row['biases']))\n",
        "        return weights, biases\n",
        "    else:\n",
        "        raise ValueError(\"The CSV file is empty.\")\n",
        "\n",
        "def ensemble_test_model_stacking(testloader, weights1, weights_last1, weights2, weights_last2):\n",
        "    predictions, actuals = [], []\n",
        "    print(\"Testing the stacking ensemble model...\")\n",
        "\n",
        "    for x_test, y_test in testloader:\n",
        "        x_test, y_test = jnp.asarray(x_test.numpy()), jnp.asarray(y_test.numpy())\n",
        "\n",
        "        out1 = compute_out(weights1, weights_last1, x_test, y_test)\n",
        "        out2 = compute_out(weights2, weights_last2, x_test, y_test)\n",
        "\n",
        "        predictions.extend(jnp.stack([jnp.argmax(out1, axis=1), jnp.argmax(out2, axis=1)], axis=1))\n",
        "        actuals.extend(y_test)\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    predictions = jnp.array(predictions)\n",
        "    actuals = jnp.array(actuals)\n",
        "\n",
        "    # Train meta-model\n",
        "    meta_model = LogisticRegression()\n",
        "    meta_model.fit(predictions, actuals)\n",
        "\n",
        "    # Meta model predictions\n",
        "    ensemble_pred = meta_model.predict(predictions)\n",
        "    ensemble_acc = accuracy_score(actuals, ensemble_pred)\n",
        "    print(f\"Stacking Ensemble: Accuracy {ensemble_acc}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "weights1, weights_last1 = load_weights_from_csv('weights.csv')\n",
        "weights2, weights_last2 = load_weights_from_csv('weights_beta.csv')\n",
        "\n",
        "ensemble_test_model_stacking(testloader, weights1, weights_last1, weights2, weights_last2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dSFEiBIyIGe",
        "outputId": "092543df-6bf1-463c-bd10-d19f99ba4784"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing the stacking ensemble model...\n",
            "Stacking Ensemble: Accuracy 0.736875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}